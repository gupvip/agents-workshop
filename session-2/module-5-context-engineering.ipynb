{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ea1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Checking LLM Configuration...\n",
      "==================================================\n",
      "ðŸ“¡ Provider: DIAL (Azure OpenAI via EPAM AI Proxy)\n",
      "âœ… DIAL_API_KEY is set\n",
      "\n",
      "ðŸ“‹ Configuration:\n",
      "   AZURE_OPENAI_ENDPOINT: https://ai-proxy.lab.epam.com\n",
      "   AZURE_OPENAI_API_VERSION: 2024-08-01-preview\n",
      "   AZURE_OPENAI_DEPLOYMENT_NAME: gpt-4\n",
      "\n",
      "âœ… DIAL setup verified successfully!\n",
      "\n",
      "ðŸ“š Session 2, Module 5: Context Engineering\n",
      "============================================================\n",
      "\n",
      "ðŸ”— Prerequisites:\n",
      "   â€¢ Module 1: Memory architecture\n",
      "   â€¢ Module 2: Advanced HITL\n",
      "   â€¢ Module 3: Self-reflection patterns\n",
      "   â€¢ Module 4: Multi-agent collaboration\n",
      "\n",
      "ðŸŽ¯ Optimize agent context for production:\n",
      "\n",
      "Part 1: The Four Pillars (20 min)\n",
      "  â€¢ WRITE - Save outside context (Store, Memory)\n",
      "  â€¢ SELECT - Pull in when needed (RAG, Retrieval)\n",
      "  â€¢ COMPRESS - Keep only relevant (Summarization, Trim)\n",
      "  â€¢ ISOLATE - Split up context (Multi-agent, Subgraphs)\n",
      "\n",
      "Part 2: LangChain Middleware (20 min)\n",
      "  â€¢ SummarizationMiddleware for context compression\n",
      "  â€¢ ModelRetryMiddleware for reliability\n",
      "  â€¢ Combining middleware for production\n",
      "\n",
      "Part 3: Memory-Augmented RAG (30 min)\n",
      "  â€¢ Dual retrieval (static + conversational)\n",
      "  â€¢ User profile personalization\n",
      "  â€¢ Production metrics and patterns\n"
     ]
    }
   ],
   "source": [
    "# Workshop Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from setup_llm import verify_setup, get_chat_model, get_embeddings\n",
    "\n",
    "verify_setup()\n",
    "\n",
    "print(\"\\nðŸ“š Session 2, Module 5: Context Engineering\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nðŸ”— Prerequisites:\")\n",
    "print(\"   â€¢ Module 1: Memory architecture\")\n",
    "print(\"   â€¢ Module 2: Advanced HITL\")\n",
    "print(\"   â€¢ Module 3: Self-reflection patterns\")\n",
    "print(\"   â€¢ Module 4: Multi-agent collaboration\")\n",
    "print(\"\\nðŸŽ¯ Optimize agent context for production:\")\n",
    "print(\"\\nPart 1: The Four Pillars (25 min)\")\n",
    "print(\"  â€¢ WRITE - Save outside context (Store, Memory)\")\n",
    "print(\"  â€¢ SELECT - Pull in when needed (RAG, Retrieval)\")\n",
    "print(\"  â€¢ COMPRESS - Keep only relevant (Summarization, Trim)\")\n",
    "print(\"  â€¢ ISOLATE - Split up context (Multi-agent, Subgraphs)\")\n",
    "print(\"\\nPart 2: Production Middleware (15 min)\")\n",
    "print(\"  â€¢ ModelRetryMiddleware for reliability\")\n",
    "print(\"  â€¢ Stacking middleware for production agents\")\n",
    "print(\"\\nPart 3: Memory-Augmented RAG Demo (30 min)\")\n",
    "print(\"  â€¢ Dual retrieval (static + conversational)\")\n",
    "print(\"  â€¢ User profile personalization\")\n",
    "print(\"  â€¢ Production metrics and patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a032f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why Context Engineering Matters\n",
    "\n",
    "### The Problem\n",
    "\n",
    "When agents fail in production, **the right context was not passed to the LLM** (not because the model isn't capable).\n",
    "\n",
    "**Why agents are challenging:**\n",
    "- **Long-running tasks** â†’ Context accumulates across many turns\n",
    "- **Tool calling** â†’ Each tool adds feedback to context\n",
    "\n",
    "**Example**: 10 tool calls Ã— 500 tokens each = 5,000+ tokens just from tool feedback!\n",
    "\n",
    "### The LLM as an Operating System\n",
    "\n",
    "- **LLM = CPU**\n",
    "- **Context Window = RAM**\n",
    "- **Context Engineering = Memory Management**\n",
    "\n",
    "Just like an OS decides what fits in RAM, context engineering decides what fits in the context window.\n",
    "\n",
    "### Context Failures to Avoid\n",
    "\n",
    "1. **Poisoning** - Incorrect info influences responses\n",
    "2. **Distraction** - Irrelevant info confuses the LLM\n",
    "3. **Curation** - Missing critical information\n",
    "4. **Clash** - Conflicting information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef2997",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Four Pillars: WRITE, SELECT, COMPRESS, ISOLATE\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                            â”‚\n",
    "â”‚  WRITE          SELECT        COMPRESS       ISOLATE      â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€â”€â”€      â”‚\n",
    "â”‚  Save outside   Pull in       Keep only      Split up     â”‚\n",
    "â”‚  context        when needed   relevant       context       â”‚\n",
    "â”‚                                                            â”‚\n",
    "â”‚  Examples:      Examples:     Examples:      Examples:     â”‚\n",
    "â”‚  â€¢ Scratchpad   â€¢ Memories    â€¢ Summarize    â€¢ Multi-agentâ”‚\n",
    "â”‚  â€¢ Memory       â€¢ Tools (RAG) â€¢ Trim         â€¢ Sandbox     â”‚\n",
    "â”‚  â€¢ Files        â€¢ Knowledge   â€¢ Post-process â€¢ State fieldsâ”‚\n",
    "â”‚                                                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "*Framework from: Claude Code, Cursor, Windsurf, Anthropic's Multi-Agent Researcher, Cognition's Devin*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1dc28a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pillar 1: WRITE â€” Save Outside Context\n",
    "\n",
    "**Goal**: Write information to persistent storage so it doesn't consume context window space.\n",
    "\n",
    "### The Concept\n",
    "\n",
    "- **Scratchpad (RAM)**: Short-term, in context, for current task\n",
    "- **Memory (Disk)**: Long-term, external storage, retrieved when needed\n",
    "\n",
    "### Three Types of Memory\n",
    "\n",
    "1. **Semantic** - Facts (User's API keys in AWS Secrets Manager)\n",
    "2. **Episodic** - Events (Nov 25: user reported bug in module X)\n",
    "3. **Procedural** - How-tos (To deploy: `make deploy && kubectl rollout`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "341c9c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ WRITE Pillar: Using LangGraph Store\n",
      "==================================================\n",
      "\n",
      "âœ… Saved to external store (outside context window)\n",
      "\n",
      "ðŸ“– Retrieved preferences: [Item(namespace=['user_prefs', 'user_123'], key='theme', value={'preference': 'dark_mode', 'font_size': 14}, created_at='2025-12-05T05:58:13.320721+00:00', updated_at='2025-12-05T05:58:13.320728+00:00', score=None)]\n",
      "\n",
      "ðŸ’¡ Key Difference:\n",
      "  â€¢ State = Current context (in window)\n",
      "  â€¢ Store = Long-term knowledge (outside window)\n"
     ]
    }
   ],
   "source": [
    "# WRITE Pillar Example: LangGraph Store\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "print(\"ðŸ“ WRITE Pillar: Using LangGraph Store\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize store\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Save fact outside context window\n",
    "store.put(\n",
    "    namespace=(\"user_prefs\", \"user_123\"),\n",
    "    key=\"theme\",\n",
    "    value={\"preference\": \"dark_mode\", \"font_size\": 14}\n",
    ")\n",
    "\n",
    "# Save procedural memory\n",
    "store.put(\n",
    "    namespace=(\"procedures\", \"deployment\"),\n",
    "    key=\"steps\",\n",
    "    value={\"command\": \"make deploy && kubectl rollout\", \"environment\": \"production\"}\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Saved to external store (outside context window)\")\n",
    "\n",
    "# Retrieve when needed\n",
    "prefs = store.search((\"user_prefs\", \"user_123\"))\n",
    "print(f\"\\nðŸ“– Retrieved preferences: {list(prefs)}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Difference:\")\n",
    "print(\"  â€¢ State = Current context (in window)\")\n",
    "print(\"  â€¢ Store = Long-term knowledge (outside window)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145fc595",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pillar 2: SELECT â€” Pull Context When Needed\n",
    "\n",
    "**Goal**: Dynamically retrieve only relevant information into context.\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "You have 100+ tools or 1000+ documents. You can't fit them all in context!\n",
    "\n",
    "### Solution: Retrieval Augmented Generation (RAG)\n",
    "\n",
    "#### 2A: Tool Selection via RAG\n",
    "When you have 30+ tools, embed descriptions and retrieve the top-K relevant ones.\n",
    "\n",
    "#### 2B: Memory Retrieval\n",
    "Retrieve relevant past conversations/facts.\n",
    "\n",
    "#### 2C: Knowledge Retrieval (Traditional RAG)\n",
    "Retrieve from document knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b85e4c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” SELECT Pillar: Dynamic Tool Selection\n",
      "==================================================\n",
      "\n",
      "ðŸ“ Query: 'I need to send a message to my colleague'\n",
      "\n",
      "ðŸŽ¯ Selected tools (top 3):\n",
      "   1. send_email: Send an email to a recipient\n",
      "   2. translate: Translate text between languages\n",
      "   3. search_web: Search the internet for current information\n",
      "\n",
      "ðŸ’¡ Used by: Windsurf, Cursor (when tool count is high)\n"
     ]
    }
   ],
   "source": [
    "# SELECT Pillar Example: Tool Selection via RAG\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"ðŸ” SELECT Pillar: Dynamic Tool Selection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Tool descriptions (imagine you have 30+ tools)\n",
    "tool_descriptions = [\n",
    "    \"search_web: Search the internet for current information\",\n",
    "    \"calculate: Perform mathematical calculations\",\n",
    "    \"send_email: Send an email to a recipient\",\n",
    "    \"read_file: Read contents of a file\",\n",
    "    \"write_file: Write content to a file\",\n",
    "    \"search_database: Query the SQL database\",\n",
    "    \"get_weather: Get current weather for a location\",\n",
    "    \"translate: Translate text between languages\",\n",
    "]\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = get_embeddings()\n",
    "tool_vectorstore = FAISS.from_texts(tool_descriptions, embeddings)\n",
    "\n",
    "def select_tools(query: str, k: int = 3):\n",
    "    \"\"\"SELECT pattern: Retrieve top-k relevant tools for query\"\"\"\n",
    "    relevant_tools = tool_vectorstore.similarity_search(query, k=k)\n",
    "    return [doc.page_content for doc in relevant_tools]\n",
    "\n",
    "# Example usage\n",
    "query = \"I need to send a message to my colleague\"\n",
    "selected = select_tools(query)\n",
    "print(f\"\\nðŸ“ Query: '{query}'\")\n",
    "print(f\"\\nðŸŽ¯ Selected tools (top 3):\")\n",
    "for i, tool in enumerate(selected, 1):\n",
    "    print(f\"   {i}. {tool}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Used by: Windsurf, Cursor (when tool count is high)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624ce7d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pillar 3: COMPRESS â€” Keep Only Relevant Tokens\n",
    "\n",
    "**Goal**: Reduce context size by summarizing or trimming.\n",
    "\n",
    "### When to COMPRESS\n",
    "\n",
    "- Long conversation histories\n",
    "- Verbose tool outputs\n",
    "- Repeated information\n",
    "\n",
    "### Patterns\n",
    "\n",
    "- **3A: Summarization** - Summarize old messages\n",
    "- **3B: Trim by Recency** - Keep only recent messages\n",
    "- **3C: Tool Output Filtering** - Truncate verbose tool outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9fd630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—œï¸ COMPRESS Pillar: Message Summarization\n",
      "==================================================\n",
      "\n",
      "âœ… Compression function defined\n",
      "\n",
      "ðŸ’¡ Tool Output Filtering Example:\n",
      "\n",
      "def filter_tool_output(tool_name: str, result: str) -> str:\n",
      "    if tool_name == \"search_web\":\n",
      "        return result[:500] + \"...\" if len(result) > 500 else result\n",
      "    elif tool_name == \"read_file\":\n",
      "        return result[:1000] + \"\n",
      "... (truncated)\" if len(result) > 1000 else result\n",
      "    return result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMPRESS Pillar Example: Message Summarization\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from typing import TypedDict\n",
    "\n",
    "print(\"ðŸ—œï¸ COMPRESS Pillar: Message Summarization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: list\n",
    "\n",
    "def compress_conversation(state: AgentState) -> dict:\n",
    "    \"\"\"COMPRESS pattern: Summarize old messages when context grows\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    if len(messages) > 10:\n",
    "        # Get LLM to summarize older messages\n",
    "        old_messages = messages[:7]\n",
    "        recent_messages = messages[7:]\n",
    "        \n",
    "        # Create summary prompt\n",
    "        summary_prompt = \"Summarize the key points from this conversation:\"\n",
    "        \n",
    "        llm = get_chat_model()\n",
    "        summary = llm.invoke([\n",
    "            SystemMessage(content=summary_prompt),\n",
    "            *old_messages\n",
    "        ])\n",
    "        \n",
    "        # Replace old messages with summary\n",
    "        compressed = [\n",
    "            SystemMessage(content=f\"Previous conversation summary: {summary.content}\"),\n",
    "            *recent_messages\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Compression Results:\")\n",
    "        print(f\"   Before: {len(messages)} messages\")\n",
    "        print(f\"   After: {len(compressed)} messages\")\n",
    "        \n",
    "        return {\"messages\": compressed}\n",
    "    \n",
    "    return {}\n",
    "\n",
    "print(\"\\nâœ… Compression function defined\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Tool Output Filtering Example:\")\n",
    "print(\"\"\"\n",
    "def filter_tool_output(tool_name: str, result: str) -> str:\n",
    "    if tool_name == \"search_web\":\n",
    "        return result[:500] + \"...\" if len(result) > 500 else result\n",
    "    elif tool_name == \"read_file\":\n",
    "        return result[:1000] + \"\\n... (truncated)\" if len(result) > 1000 else result\n",
    "    return result\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc7d6f1",
   "metadata": {},
   "source": [
    "### Production COMPRESS: SummarizationMiddleware\n",
    "\n",
    "LangChain's `SummarizationMiddleware` automates context compression for production:\n",
    "\n",
    "| Trigger Type | Example | When to Use |\n",
    "|------------|---------|-------------|\n",
    "| `(\"messages\", N)` | Trigger at N messages | Simple message-count threshold |\n",
    "| `(\"tokens\", N)` | Trigger at N tokens | Token-aware compression |\n",
    "| `(\"fraction\", 0.8)` | Trigger at 80% context | Model-aware, recommended |\n",
    "\n",
    "> **Note**: Other middleware (retry, fallback, HITL) are covered in Part 2 after we complete the Four Pillars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b6d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SummarizationMiddleware configured\n",
      "\n",
      "How it works:\n",
      "  1. Agent accumulates messages during conversation\n",
      "  2. When trigger threshold is reached (10 messages):\n",
      "     - Old messages are summarized into a single message\n",
      "     - Recent messages (5) are kept verbatim\n",
      "  3. Context stays manageable for long conversations\n",
      "\n",
      "Trigger Options (use TUPLE format!):\n",
      "  â€¢ (\"messages\", N)  - Count of messages\n",
      "  â€¢ (\"tokens\", N)    - Token count\n",
      "  â€¢ (\"fraction\", 0.8) - Fraction of model's context window\n",
      "  â€¢ Combine: [(\"tokens\", 3000), (\"messages\", 6)] (OR logic)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SummarizationMiddleware - Automatic Conversation Compression\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Initialize model\n",
    "model = get_chat_model(temperature=0)\n",
    "\n",
    "@tool\n",
    "def search_docs(query: str) -> str:\n",
    "    \"\"\"Search internal documentation.\"\"\"\n",
    "    return f\"Found 3 documents about '{query}': Architecture guide, API reference, Deployment docs...\"\n",
    "\n",
    "@tool\n",
    "def get_status(service: str) -> str:\n",
    "    \"\"\"Get service status.\"\"\"\n",
    "    return f\"Service '{service}' is running. Uptime: 99.9%, Latency: 45ms\"\n",
    "\n",
    "# Create agent with SummarizationMiddleware (COMPRESS pillar)\n",
    "# NOTE: trigger and keep use TUPLE format: (\"messages\", N) not dict format\n",
    "agent_with_summarization = create_agent(\n",
    "    model=model,\n",
    "    tools=[search_docs, get_status],\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=model,                      # Use same model (or cheaper one for summarization)\n",
    "            trigger=(\"messages\", 10),         # Trigger when >10 messages (TUPLE format!)\n",
    "            keep=(\"messages\", 5),             # Keep 5 most recent messages (TUPLE format!)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"âœ… SummarizationMiddleware configured (COMPRESS pillar)\")\n",
    "print(\"\"\"\n",
    "How it works:\n",
    "  1. Agent accumulates messages during conversation\n",
    "  2. When trigger threshold is reached (10 messages):\n",
    "     - Old messages are summarized into a single message\n",
    "     - Recent messages (5) are kept verbatim\n",
    "  3. Context stays manageable for long conversations\n",
    "\n",
    "This is the COMPRESS pillar in action!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb14fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pillar 4: ISOLATE â€” Split Up Context\n",
    "\n",
    "**Goal**: Separate concerns by spawning specialized subagents or isolating state.\n",
    "\n",
    "### Patterns\n",
    "\n",
    "- **4A: Multi-Agent (Supervisor)** - Each subagent has isolated context\n",
    "- **4B: State Schema Isolation** - Use Pydantic models to separate concerns\n",
    "- **4C: Code Execution Sandboxing** - Run untrusted code in isolated environment\n",
    "\n",
    "**Benefits**:\n",
    "- Researcher doesn't see writer's context\n",
    "- Prevents context clash\n",
    "- Each agent optimized for its task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "649cc8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”’ ISOLATE Pillar: Multi-Agent Isolation\n",
      "==================================================\n",
      "\n",
      "âœ… Multi-Agent State Isolation defined\n",
      "\n",
      "ðŸ’¡ State Schema Isolation with Pydantic:\n",
      "\n",
      "âœ… Pydantic state with isolation defined\n",
      "\n",
      "ðŸŽ¯ Benefits of ISOLATE:\n",
      "  â€¢ Researcher doesn't see writer's context\n",
      "  â€¢ Prevents context clash between subtasks\n",
      "  â€¢ Each agent optimized for its specific task\n"
     ]
    }
   ],
   "source": [
    "# ISOLATE Pillar Example: Multi-Agent Isolation\n",
    "from typing import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "print(\"ðŸ”’ ISOLATE Pillar: Multi-Agent Isolation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Different state schemas for different agents\n",
    "class SupervisorState(TypedDict):\n",
    "    task: str\n",
    "    result: str\n",
    "\n",
    "class ResearcherState(TypedDict):\n",
    "    query: str\n",
    "    findings: str\n",
    "\n",
    "class WriterState(TypedDict):\n",
    "    draft: str\n",
    "    feedback: str\n",
    "\n",
    "def supervisor(state: SupervisorState) -> str:\n",
    "    \"\"\"Supervisor routes to subagents with isolated context\"\"\"\n",
    "    if \"research\" in state[\"task\"]:\n",
    "        return \"researcher\"  # Isolated context\n",
    "    return \"writer\"  # Different isolated context\n",
    "\n",
    "print(\"\\nâœ… Multi-Agent State Isolation defined\")\n",
    "\n",
    "print(\"\\nðŸ’¡ State Schema Isolation with Pydantic:\")\n",
    "\n",
    "class AgentStateWithIsolation(BaseModel):\n",
    "    \"\"\"State with isolated fields per agent\"\"\"\n",
    "    # Public context (available to all nodes)\n",
    "    messages: list = Field(default_factory=list)\n",
    "    \n",
    "    # Private context (isolated per node)\n",
    "    researcher_context: str | None = Field(\n",
    "        default=None,\n",
    "        description=\"Only for researcher node\"\n",
    "    )\n",
    "    writer_context: str | None = Field(\n",
    "        default=None,\n",
    "        description=\"Only for writer node\"\n",
    "    )\n",
    "    \n",
    "print(\"\\nâœ… Pydantic state with isolation defined\")\n",
    "print(\"\\nðŸŽ¯ Benefits of ISOLATE:\")\n",
    "print(\"  â€¢ Researcher doesn't see writer's context\")\n",
    "print(\"  â€¢ Prevents context clash between subtasks\")\n",
    "print(\"  â€¢ Each agent optimized for its specific task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e7fa6e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LangGraph Features Mapped to Pillars\n",
    "\n",
    "| Pillar | LangGraph Feature | Purpose |\n",
    "|--------|------------------|--------|\n",
    "| WRITE | `Store` | Save memories outside state |\n",
    "| WRITE | State fields | Scratchpad within session |\n",
    "| SELECT | `Store.search()` | Retrieve relevant memories |\n",
    "| SELECT | Tool binding | Dynamically select tools |\n",
    "| COMPRESS | Reducers | Summarize/trim state fields |\n",
    "| COMPRESS | Middleware | Post-process tool outputs |\n",
    "| ISOLATE | Subgraphs | Spawn isolated subagents |\n",
    "| ISOLATE | State schema | Separate concerns via types |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c093e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Production Middleware\n",
    "\n",
    "Now that we understand the Four Pillars conceptually, let's look at **LangChain middleware** that helps implement them in production.\n",
    "\n",
    "### Middleware Overview\n",
    "\n",
    "| Middleware | Pillar | Purpose | When to Use |\n",
    "|------------|--------|---------|-------------|\n",
    "| `SummarizationMiddleware` | COMPRESS | Condense conversation history | Long conversations |\n",
    "| `ModelRetryMiddleware` | Reliability | Auto-retry on failures | Production systems |\n",
    "| `ModelFallbackMiddleware` | Reliability | Fallback to alternative models | High availability |\n",
    "| `HumanInTheLoopMiddleware` | Safety | Require human approval | Sensitive operations |\n",
    "| `PIIMiddleware` | Privacy | Redact sensitive data | User-facing apps |\n",
    "\n",
    "> **Note**: We already saw `SummarizationMiddleware` under COMPRESS. Below are the remaining middleware patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelRetryMiddleware - Production Reliability\n",
    "from langchain.agents.middleware import ModelRetryMiddleware\n",
    "\n",
    "# Agent with automatic retry on failures\n",
    "agent_with_retry = create_agent(\n",
    "    model=model,\n",
    "    tools=[search_docs, get_status],\n",
    "    middleware=[\n",
    "        ModelRetryMiddleware(\n",
    "            max_retries=3,           # Maximum retry attempts\n",
    "            backoff_factor=2.0,      # Exponential backoff multiplier\n",
    "            initial_delay=1.0,       # Initial delay in seconds\n",
    "            # Retry sequence: 1s â†’ 2s â†’ 4s\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"âœ… ModelRetryMiddleware configured\")\n",
    "print(\"\"\"\n",
    "How it works:\n",
    "  1. Model call fails (rate limit, timeout, etc.)\n",
    "  2. Wait initial_delay seconds (1s)\n",
    "  3. Retry, wait initial_delay * backoff_factor (2s)\n",
    "  4. Retry, wait previous * backoff_factor (4s)\n",
    "  5. After max_retries, raise exception\n",
    "\n",
    "Best Practices:\n",
    "  â€¢ Set max_retries=3 for balance between reliability and latency\n",
    "  â€¢ Use backoff_factor=2.0 for exponential backoff\n",
    "  â€¢ Combine with ModelFallbackMiddleware for high availability\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf94364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Multiple Middleware - Production Best Practice\n",
    "from langchain.agents.middleware import (\n",
    "    SummarizationMiddleware,\n",
    "    ModelRetryMiddleware,\n",
    "    HumanInTheLoopMiddleware\n",
    ")\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Production-ready agent with stacked middleware\n",
    "production_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[search_docs, get_status],\n",
    "    middleware=[\n",
    "        # Layer 1: Retry on transient failures (Reliability)\n",
    "        ModelRetryMiddleware(\n",
    "            max_retries=3,\n",
    "            backoff_factor=2.0,\n",
    "        ),\n",
    "        # Layer 2: Compress long conversations (COMPRESS pillar)\n",
    "        SummarizationMiddleware(\n",
    "            model=model,\n",
    "            trigger=(\"messages\", 15),\n",
    "            keep=(\"messages\", 5),\n",
    "        ),\n",
    "        # Layer 3: Human approval for sensitive operations (Safety)\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\"get_status\": False}  # Customize per tool\n",
    "        ),\n",
    "    ],\n",
    "    checkpointer=InMemorySaver()  # Required for HITL\n",
    ")\n",
    "\n",
    "print(\"âœ… Production agent with stacked middleware\")\n",
    "print(\"\"\"\n",
    "Middleware Execution Order (stack):\n",
    "  Request â†’ ModelRetry â†’ Summarization â†’ HITL â†’ Agent\n",
    "  Response â† ModelRetry â† Summarization â† HITL â† Agent\n",
    "\n",
    "Middleware mapped to concepts:\n",
    "  â€¢ ModelRetryMiddleware â†’ Reliability (not a pillar, but essential)\n",
    "  â€¢ SummarizationMiddleware â†’ COMPRESS pillar\n",
    "  â€¢ HumanInTheLoopMiddleware â†’ Safety/HITL (Module 2 topic)\n",
    "\n",
    "This is the recommended pattern for production agents!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa76487",
   "metadata": {},
   "source": [
    "### Middleware Summary\n",
    "\n",
    "| Middleware | Category | Key Parameters | Use Case |\n",
    "|------------|----------|----------------|----------|\n",
    "| `SummarizationMiddleware` | COMPRESS | `trigger`, `keep`, `model` | Long conversations |\n",
    "| `ModelRetryMiddleware` | Reliability | `max_retries`, `backoff_factor` | Production systems |\n",
    "| `HumanInTheLoopMiddleware` | Safety | `interrupt_on` | Sensitive operations |\n",
    "| `ModelFallbackMiddleware` | Reliability | `fallback_models` | High availability |\n",
    "| `PIIMiddleware` | Privacy | `strategy`, `apply_to_input` | User-facing apps |\n",
    "| `ToolRetryMiddleware` | Reliability | `max_retries` | Unreliable external APIs |\n",
    "\n",
    "**Production Baseline**: Stack `ModelRetryMiddleware` + `SummarizationMiddleware` as minimum.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Memory-Augmented RAG Demo\n",
    "\n",
    "Now let's put the Four Pillars into practice with a real-world demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc999f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Personalized Document Assistant with Memory-Augmented RAG\n",
    "\n",
    "import os\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                                                               â•‘\n",
    "â•‘         PERSONALIZED DOCUMENT ASSISTANT                       â•‘\n",
    "â•‘         Memory-Augmented RAG (Notion AI Style)                â•‘\n",
    "â•‘                                                               â•‘\n",
    "â•‘         Query â†’ Dual Retrieval â†’ Generate â†’ Update Memory     â•‘\n",
    "â•‘                                                               â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ðŸŽ¯ BUSINESS USE CASE: Enterprise Knowledge Management\n",
    "\n",
    "ðŸ“Š REAL-WORLD METRICS:\n",
    "   â€¢ 30M+ users (Notion AI scale)\n",
    "   â€¢ 45% relevance improvement vs traditional RAG\n",
    "   â€¢ 60% reduction in repeated questions\n",
    "   â€¢ 4.5/5 user satisfaction (up from 3.2)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2119ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enterprise Documents (Sample Knowledge Base)\n",
    "\n",
    "ENTERPRISE_DOCUMENTS = [\n",
    "    # HR Policies\n",
    "    {\n",
    "        \"title\": \"Remote Work Policy 2024\",\n",
    "        \"content\": \"\"\"\n",
    "        Our company supports flexible remote work arrangements. Employees may work remotely up to 3 days per week.\n",
    "        Remote work requires manager approval and must maintain productivity standards.\n",
    "        Equipment reimbursement: $500/year for home office setup.\n",
    "        Communication: Daily check-ins via Slack, weekly team video calls required.\n",
    "        \"\"\",\n",
    "        \"category\": \"HR\",\n",
    "        \"department\": \"Human Resources\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"PTO and Vacation Policy\",\n",
    "        \"content\": \"\"\"\n",
    "        Employees receive 20 days of PTO annually, accrued monthly (1.67 days/month).\n",
    "        PTO requests require 2 weeks notice for trips >5 days.\n",
    "        Unused PTO can roll over up to 5 days to the next year.\n",
    "        Sick leave is unlimited but requires documentation for absences >3 consecutive days.\n",
    "        \"\"\",\n",
    "        \"category\": \"HR\",\n",
    "        \"department\": \"Human Resources\"\n",
    "    },\n",
    "    # Engineering Docs\n",
    "    {\n",
    "        \"title\": \"API Authentication Guide\",\n",
    "        \"content\": \"\"\"\n",
    "        All API requests must include a Bearer token in the Authorization header.\n",
    "        Tokens expire after 24 hours and must be refreshed using the /auth/refresh endpoint.\n",
    "        Rate limits: 1000 requests/hour for standard tier, 10000/hour for enterprise.\n",
    "        Use API keys (not tokens) for server-to-server communication.\n",
    "        \"\"\",\n",
    "        \"category\": \"Engineering\",\n",
    "        \"department\": \"Engineering\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Deployment Process\",\n",
    "        \"content\": \"\"\"\n",
    "        All code must pass CI/CD checks before merging to main branch.\n",
    "        Deployments to production happen every Tuesday and Thursday at 2pm PST.\n",
    "        Emergency hotfixes require approval from two senior engineers.\n",
    "        Rollback procedure: Run './scripts/rollback.sh <version>' from ops server.\n",
    "        \"\"\",\n",
    "        \"category\": \"Engineering\",\n",
    "        \"department\": \"Engineering\"\n",
    "    },\n",
    "    # Sales Materials\n",
    "    {\n",
    "        \"title\": \"Q4 2024 Pricing\",\n",
    "        \"content\": \"\"\"\n",
    "        Starter Plan: $99/month, up to 10 users, basic features.\n",
    "        Professional Plan: $299/month, up to 50 users, advanced analytics.\n",
    "        Enterprise Plan: Custom pricing, unlimited users, dedicated support, SLA.\n",
    "        Annual contracts receive 20% discount.\n",
    "        \"\"\",\n",
    "        \"category\": \"Sales\",\n",
    "        \"department\": \"Sales\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"âœ… Loaded {len(ENTERPRISE_DOCUMENTS)} enterprise documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191138bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Definition\n",
    "\n",
    "class MemoryRAGState(TypedDict):\n",
    "    \"\"\"State for memory-augmented RAG system\"\"\"\n",
    "    \n",
    "    # Input\n",
    "    user_id: str\n",
    "    question: str\n",
    "    \n",
    "    # Memory (Dual Source)\n",
    "    conversation_history: list  # Short-term: Recent Q&A pairs\n",
    "    user_profile: dict  # Long-term: Department, role, preferences\n",
    "    learned_preferences: dict  # Semantic: Learned over time\n",
    "    \n",
    "    # Retrieval\n",
    "    retrieved_static_docs: list  # From knowledge base\n",
    "    retrieved_memory_context: list  # From conversation history\n",
    "    relevant_user_facts: list  # From user profile\n",
    "    \n",
    "    # Generation\n",
    "    answer: str\n",
    "    sources_used: list\n",
    "    \n",
    "    # Memory Update\n",
    "    extracted_facts: dict  # New facts to store\n",
    "    memory_updates: dict\n",
    "\n",
    "print(\"âœ… MemoryRAGState defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fcf0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vector Store and Helper Functions\n",
    "\n",
    "def initialize_vector_store():\n",
    "    \"\"\"Initialize FAISS vector store with enterprise documents.\"\"\"\n",
    "    documents = []\n",
    "    for doc in ENTERPRISE_DOCUMENTS:\n",
    "        documents.append(Document(\n",
    "            page_content=doc[\"content\"],\n",
    "            metadata={\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"category\": doc[\"category\"],\n",
    "                \"department\": doc[\"department\"]\n",
    "            }\n",
    "        ))\n",
    "    \n",
    "    embeddings = get_embeddings()  # Uses DIAL\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def load_user_profile(user_id: str) -> dict:\n",
    "    \"\"\"Load user profile from database (simulated).\"\"\"\n",
    "    user_profiles = {\n",
    "        \"user_eng_001\": {\n",
    "            \"name\": \"Alice Chen\",\n",
    "            \"department\": \"Engineering\",\n",
    "            \"role\": \"Senior Backend Engineer\",\n",
    "            \"team\": \"Platform\",\n",
    "            \"interests\": [\"API design\", \"performance optimization\", \"deployment\"],\n",
    "            \"recent_topics\": [\"authentication\", \"rate limiting\"]\n",
    "        },\n",
    "        \"user_hr_001\": {\n",
    "            \"name\": \"Bob Martinez\",\n",
    "            \"department\": \"Human Resources\",\n",
    "            \"role\": \"HR Manager\",\n",
    "            \"team\": \"People Ops\",\n",
    "            \"interests\": [\"policies\", \"benefits\", \"remote work\"],\n",
    "            \"recent_topics\": [\"PTO\", \"vacation policy\"]\n",
    "        },\n",
    "        \"user_sales_001\": {\n",
    "            \"name\": \"Carol Johnson\",\n",
    "            \"department\": \"Sales\",\n",
    "            \"role\": \"Account Executive\",\n",
    "            \"team\": \"Enterprise Sales\",\n",
    "            \"interests\": [\"pricing\", \"competitor analysis\", \"objection handling\"],\n",
    "            \"recent_topics\": [\"enterprise pricing\", \"competitors\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return user_profiles.get(user_id, {\n",
    "        \"name\": \"Guest User\",\n",
    "        \"department\": \"General\",\n",
    "        \"role\": \"Employee\",\n",
    "        \"interests\": [],\n",
    "        \"recent_topics\": []\n",
    "    })\n",
    "\n",
    "\n",
    "def load_conversation_history(user_id: str) -> list:\n",
    "    \"\"\"Load recent conversation history (simulated).\"\"\"\n",
    "    history_db = {\n",
    "        \"user_eng_001\": [\n",
    "            {\"question\": \"How do I authenticate API requests?\", \"answer\": \"Use Bearer tokens in Authorization header...\"},\n",
    "            {\"question\": \"What's the rate limit?\", \"answer\": \"1000 requests/hour for standard tier...\"}\n",
    "        ],\n",
    "        \"user_hr_001\": [\n",
    "            {\"question\": \"How many PTO days do we get?\", \"answer\": \"20 days annually, accrued monthly...\"}\n",
    "        ],\n",
    "        \"user_sales_001\": [\n",
    "            {\"question\": \"What's our pricing for enterprise?\", \"answer\": \"Custom pricing for Enterprise plan...\"},\n",
    "            {\"question\": \"Who are our main competitors?\", \"answer\": \"CompanyX, CompanyY, and CompanyZ...\"}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return history_db.get(user_id, [])\n",
    "\n",
    "\n",
    "# Initialize vector store\n",
    "print(\"\\nðŸ”§ Initializing vector store with enterprise documents...\")\n",
    "vectorstore = initialize_vector_store()\n",
    "print(\"âœ… Vector store ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load User Context Node\n",
    "\n",
    "def load_user_context(state: MemoryRAGState) -> dict:\n",
    "    \"\"\"\n",
    "    Load user profile and conversation history.\n",
    "    \n",
    "    Context Engineering Pillar: WRITE (to state) + SELECT (from memory)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ‘¤ LOADING USER CONTEXT: {state['user_id']}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Load user profile\n",
    "    profile = load_user_profile(state[\"user_id\"])\n",
    "    print(f\"ðŸ“‹ User Profile:\")\n",
    "    print(f\"   Name: {profile.get('name', 'Unknown')}\")\n",
    "    print(f\"   Department: {profile.get('department', 'Unknown')}\")\n",
    "    print(f\"   Role: {profile.get('role', 'Unknown')}\")\n",
    "    print(f\"   Interests: {', '.join(profile.get('interests', []))}\")\n",
    "    \n",
    "    # Load conversation history\n",
    "    history = load_conversation_history(state[\"user_id\"])\n",
    "    print(f\"\\nðŸ’¬ Conversation History: {len(history)} previous interactions\")\n",
    "    \n",
    "    # Initialize learned preferences\n",
    "    learned_prefs = {\n",
    "        \"preferred_detail_level\": \"technical\" if profile.get(\"department\") == \"Engineering\" else \"business\",\n",
    "        \"preferred_doc_types\": [profile.get(\"department\", \"General\")],\n",
    "        \"query_patterns\": profile.get(\"recent_topics\", [])\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"user_profile\": profile,\n",
    "        \"conversation_history\": history,\n",
    "        \"learned_preferences\": learned_prefs\n",
    "    }\n",
    "\n",
    "print(\"âœ… load_user_context function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dual Retrieval Node\n",
    "\n",
    "def dual_retrieval(state: MemoryRAGState) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve from both static knowledge base AND user memory.\n",
    "    \n",
    "    Context Engineering Pillar: SELECT\n",
    "    - Traditional RAG: Only static docs\n",
    "    - Memory-augmented RAG: Static + conversation + user context\n",
    "    - 45% improvement in relevance\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ” DUAL RETRIEVAL\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    user_profile = state[\"user_profile\"]\n",
    "    conversation_history = state[\"conversation_history\"]\n",
    "    \n",
    "    # 1. STATIC DOCUMENT RETRIEVAL\n",
    "    print(\"ðŸ“š Searching static knowledge base...\")\n",
    "    static_results = vectorstore.similarity_search(question, k=3)\n",
    "    \n",
    "    retrieved_static = [\n",
    "        {\n",
    "            \"content\": doc.page_content.strip(),\n",
    "            \"metadata\": doc.metadata\n",
    "        }\n",
    "        for doc in static_results\n",
    "    ]\n",
    "    \n",
    "    print(f\"   Found {len(retrieved_static)} relevant documents:\")\n",
    "    for i, doc in enumerate(retrieved_static, 1):\n",
    "        print(f\"   {i}. {doc['metadata'].get('title', 'Untitled')} ({doc['metadata'].get('category', 'General')})\")\n",
    "    \n",
    "    # 2. CONVERSATION MEMORY RETRIEVAL\n",
    "    print(f\"\\nðŸ’­ Searching conversation history...\")\n",
    "    question_lower = question.lower()\n",
    "    relevant_history = [\n",
    "        conv for conv in conversation_history\n",
    "        if any(keyword in conv[\"question\"].lower() for keyword in question_lower.split())\n",
    "    ]\n",
    "    \n",
    "    print(f\"   Found {len(relevant_history)} relevant past conversations\")\n",
    "    \n",
    "    # 3. USER PROFILE FACTS\n",
    "    print(f\"\\nðŸ‘¤ Extracting relevant user facts...\")\n",
    "    user_facts = []\n",
    "    if user_profile.get(\"department\"):\n",
    "        user_facts.append(f\"User works in {user_profile['department']} department\")\n",
    "    if user_profile.get(\"role\"):\n",
    "        user_facts.append(f\"User role: {user_profile['role']}\")\n",
    "    if user_profile.get(\"interests\"):\n",
    "        user_facts.append(f\"User interests: {', '.join(user_profile['interests'])}\")\n",
    "    \n",
    "    print(f\"   â€¢ {len(user_facts)} contextual facts extracted\")\n",
    "    \n",
    "    return {\n",
    "        \"retrieved_static_docs\": retrieved_static,\n",
    "        \"retrieved_memory_context\": relevant_history,\n",
    "        \"relevant_user_facts\": user_facts\n",
    "    }\n",
    "\n",
    "print(\"âœ… dual_retrieval function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f6536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextualized Generation Node\n",
    "\n",
    "def contextualized_generation(state: MemoryRAGState) -> dict:\n",
    "    \"\"\"\n",
    "    Generate answer using ALL available context.\n",
    "    \n",
    "    Combines: Static knowledge + Conversation memory + User context\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"âœ¨ GENERATING CONTEXTUALIZED ANSWER\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    llm = get_chat_model()  # Uses DIAL\n",
    "    \n",
    "    # Build comprehensive prompt\n",
    "    static_docs_text = \"\\n\\n\".join([\n",
    "        f\"**{doc['metadata'].get('title', 'Untitled')}**\\n{doc['content']}\"\n",
    "        for doc in state[\"retrieved_static_docs\"]\n",
    "    ])\n",
    "    \n",
    "    history_text = \"\\n\".join([\n",
    "        f\"Q: {conv['question']}\\nA: {conv['answer']}\"\n",
    "        for conv in state[\"retrieved_memory_context\"]\n",
    "    ]) if state[\"retrieved_memory_context\"] else \"No previous conversations on this topic\"\n",
    "    \n",
    "    user_context_text = \"\\n\".join([f\"- {fact}\" for fact in state[\"relevant_user_facts\"]])\n",
    "    \n",
    "    prompt = f\"\"\"You are a helpful enterprise knowledge assistant. Provide a personalized, context-aware answer.\n",
    "\n",
    "**STATIC KNOWLEDGE BASE:**\n",
    "{static_docs_text}\n",
    "\n",
    "**PREVIOUS CONVERSATIONS:**\n",
    "{history_text}\n",
    "\n",
    "**USER CONTEXT:**\n",
    "{user_context_text}\n",
    "\n",
    "**CURRENT QUESTION:**\n",
    "{state['question']}\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1. Answer the question using the knowledge base documents\n",
    "2. Consider the user's department and role when framing your response\n",
    "3. Reference previous conversations if relevant\n",
    "4. Be specific and cite sources\n",
    "5. Adapt detail level to user's technical background\n",
    "\n",
    "**ANSWER:**\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    answer = response.content.strip()\n",
    "    \n",
    "    # Track sources used\n",
    "    sources = [doc[\"metadata\"].get(\"title\", \"Untitled\") for doc in state[\"retrieved_static_docs\"]]\n",
    "    \n",
    "    print(f\"ðŸ“ Generated Answer:\")\n",
    "    print(f\"{answer}\\n\")\n",
    "    print(f\"ðŸ“– Sources: {', '.join(sources)}\")\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources_used\": sources\n",
    "    }\n",
    "\n",
    "print(\"âœ… contextualized_generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc9b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Memory Node\n",
    "\n",
    "def update_memory(state: MemoryRAGState) -> dict:\n",
    "    \"\"\"\n",
    "    Extract new facts and update user memory.\n",
    "    \n",
    "    Context Engineering Pillar: WRITE (to external memory)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ’¾ UPDATING USER MEMORY\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Add this Q&A to conversation history\n",
    "    new_conversation = {\n",
    "        \"question\": state[\"question\"],\n",
    "        \"answer\": state[\"answer\"],\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Extract key topics\n",
    "    question_keywords = [word.lower() for word in state[\"question\"].split() if len(word) > 4]\n",
    "    \n",
    "    # Update user profile interests\n",
    "    existing_interests = set(state[\"user_profile\"].get(\"interests\", []))\n",
    "    updated_interests = list(existing_interests.union(set(question_keywords[:3])))\n",
    "    \n",
    "    memory_updates = {\n",
    "        \"new_conversation\": new_conversation,\n",
    "        \"updated_interests\": updated_interests,\n",
    "        \"last_interaction\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… Memory updated:\")\n",
    "    print(f\"   â€¢ Conversation saved\")\n",
    "    print(f\"   â€¢ Last interaction: {memory_updates['last_interaction']}\")\n",
    "    \n",
    "    return {\"memory_updates\": memory_updates}\n",
    "\n",
    "print(\"âœ… update_memory function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a084cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and Compile the Graph\n",
    "\n",
    "# Build the memory-augmented RAG workflow\n",
    "workflow = StateGraph(MemoryRAGState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"load_context\", load_user_context)\n",
    "workflow.add_node(\"dual_retrieval\", dual_retrieval)\n",
    "workflow.add_node(\"generate\", contextualized_generation)\n",
    "workflow.add_node(\"update_memory\", update_memory)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"load_context\")\n",
    "workflow.add_edge(\"load_context\", \"dual_retrieval\")\n",
    "workflow.add_edge(\"dual_retrieval\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"update_memory\")\n",
    "workflow.add_edge(\"update_memory\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"âœ… Memory-Augmented RAG workflow compiled\")\n",
    "print(\"\\nWorkflow Structure:\")\n",
    "print(\"   START â†’ load_context â†’ dual_retrieval â†’ generate â†’ update_memory â†’ END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c442506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Example: Engineer Query\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE: Engineer - API Authentication Query\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result = app.invoke({\n",
    "    \"user_id\": \"user_eng_001\",\n",
    "    \"question\": \"How long do API tokens last and how do I refresh them?\"\n",
    "})\n",
    "\n",
    "print(\"\\nðŸ“ FINAL ANSWER:\")\n",
    "print(result[\"answer\"])\n",
    "print(f\"\\nðŸ“– Sources: {', '.join(result['sources_used'])}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c4441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Learnings\n",
    "\n",
    "print(\"\"\"\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "ðŸ“Š TRADITIONAL RAG VS MEMORY-AUGMENTED RAG\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "TRADITIONAL RAG:\n",
    "  Query â†’ Vector Search â†’ Generate\n",
    "  âŒ Stateless (no session memory)\n",
    "  âŒ No user context\n",
    "  âŒ Same answer for everyone\n",
    "  âœ… Simple implementation\n",
    "\n",
    "MEMORY-AUGMENTED RAG:\n",
    "  Query â†’ Dual Retrieval (Static + Memory + Profile) â†’ Generate â†’ Update Memory\n",
    "  âœ… Stateful across sessions\n",
    "  âœ… User-aware and personalized\n",
    "  âœ… Learns from interactions\n",
    "  âœ… 45% relevance improvement\n",
    "  âŒ More complex architecture\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "ðŸ“š PRODUCTION LEARNINGS FROM NOTION AI\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "1ï¸âƒ£  DUAL RETRIEVAL ARCHITECTURE\n",
    "   â€¢ Static docs: 60% of context\n",
    "   â€¢ Conversation memory: 25% of context\n",
    "   â€¢ User profile: 15% of context\n",
    "\n",
    "2ï¸âƒ£  MEMORY MANAGEMENT\n",
    "   â€¢ Short-term: Last 5-10 conversations (Redis, 24hr TTL)\n",
    "   â€¢ Long-term: User profile & preferences (PostgreSQL)\n",
    "   â€¢ Episodic: Past interactions (vector store)\n",
    "\n",
    "3ï¸âƒ£  PERSONALIZATION STRATEGIES\n",
    "   â€¢ Department-aware document filtering\n",
    "   â€¢ Role-based detail level (technical vs business)\n",
    "   â€¢ Interest-based ranking boost\n",
    "\n",
    "4ï¸âƒ£  ACCURACY METRICS\n",
    "   â€¢ Relevance: 45% improvement vs traditional RAG\n",
    "   â€¢ Repeated questions: 60% reduction\n",
    "   â€¢ User satisfaction: 3.2 â†’ 4.5/5 stars\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e21b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices Summary\n",
    "\n",
    "### When to Use Each Pillar\n",
    "\n",
    "**Use WRITE when:**\n",
    "- Conversations exceed 10+ turns\n",
    "- Agent needs to remember facts across sessions\n",
    "- Task requires step-by-step planning\n",
    "\n",
    "**Use SELECT when:**\n",
    "- You have 30+ tools (use tool RAG)\n",
    "- You have 100+ documents (use knowledge RAG)\n",
    "- You have historical conversations (use memory RAG)\n",
    "\n",
    "**Use COMPRESS when:**\n",
    "- Context approaches model's limit (e.g., 100K tokens)\n",
    "- Tool outputs are verbose (e.g., full file contents)\n",
    "- Conversation history is repetitive\n",
    "\n",
    "**Use ISOLATE when:**\n",
    "- Task requires multiple specialized skills\n",
    "- Risk of context clash between subtasks\n",
    "- Code execution needs sandboxing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed20cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Context Engineering Framework\n",
    "\n",
    "| Pillar | Action | LangGraph Feature | When to Use |\n",
    "|--------|--------|-------------------|-------------|\n",
    "| **WRITE** | Save outside context | `InMemoryStore`, `PostgresStore` | Long conversations, cross-session memory |\n",
    "| **SELECT** | Pull in when needed | `store.search()`, RAG retrieval | 30+ tools, 100+ documents |\n",
    "| **COMPRESS** | Keep only relevant | `SummarizationMiddleware`, reducers | Context approaching limit |\n",
    "| **ISOLATE** | Split up context | Subgraphs, multi-agent | Specialized tasks, code execution |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ‰ Session 2 Complete!\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "| Module | Topic | Key Skills |\n",
    "|--------|-------|------------|\n",
    "| **Module 1** | Memory & Streaming | Store API, semantic search, token streaming |\n",
    "| **Module 2** | HITL & Durability | `interrupt()`, `Command(resume=...)`, `@task` |\n",
    "| **Module 3** | Self-Reflection | Quality gates, LLM-as-Judge, critique loops |\n",
    "| **Module 4** | Multi-Agent Collaboration | Peer review, Send API, handoffs |\n",
    "| **Module 5** | Context Engineering | WRITE/SELECT/COMPRESS/ISOLATE, middleware |\n",
    "\n",
    "### Continue to Session 3\n",
    "\n",
    "- **Module 1**: Evaluation frameworks (automated testing)\n",
    "- **Module 2**: Observability and cost optimization\n",
    "- **Module 3**: Security and guardrails\n",
    "- **Module 4**: Production case studies (Klarna, Replit, Elastic)\n",
    "- **Module 5**: MCP integration for tool interoperability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced5acc4-d841-41c9-b1c9-f657c0c7cb36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f57710d-4ee5-4b7b-b221-25505564d921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
