{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3f85d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking LLM Configuration...\n",
      "==================================================\n",
      "ğŸ“¡ Provider: DIAL (Azure OpenAI via EPAM AI Proxy)\n",
      "âœ… DIAL_API_KEY is set\n",
      "\n",
      "ğŸ“‹ Configuration:\n",
      "   AZURE_OPENAI_ENDPOINT: https://ai-proxy.lab.epam.com\n",
      "   AZURE_OPENAI_API_VERSION: 2024-08-01-preview\n",
      "   AZURE_OPENAI_DEPLOYMENT_NAME: gpt-4\n",
      "\n",
      "âœ… DIAL setup verified successfully!\n",
      "\n",
      "ğŸ“š Session 2, Module 3: Self-Reflection & Quality Patterns\n",
      "============================================================\n",
      "\n",
      "This module covers:\n",
      "\n",
      "Part 1: The Self-Reflection Pattern (15 min)\n",
      "  â€¢ Why single-pass generation fails\n",
      "  â€¢ Generate â†’ Critique â†’ Revise loop\n",
      "  â€¢ Research backing and ROI\n",
      "\n",
      "Part 2: LLM-as-Judge Evaluation (15 min)\n",
      "  â€¢ Structured critique with Pydantic\n",
      "  â€¢ Multi-dimensional scoring rubrics\n",
      "  â€¢ Actionable feedback generation\n",
      "\n",
      "Part 3: Quality Gates & Cost Control (10 min)\n",
      "  â€¢ Threshold-based decisions\n",
      "  â€¢ Iteration limits for cost control\n",
      "  â€¢ Early stopping strategies\n",
      "\n",
      "Part 4: Simple Demo - Report Generator (10 min)\n",
      "  â€¢ End-to-end self-reflection workflow\n",
      "  â€¢ Tracking quality improvements\n",
      "\n",
      "ğŸ¯ Build agents that improve their own work!\n",
      "\n",
      "âš ï¸  Prerequisites: Module 1 (Memory), Module 2 (HITL)\n"
     ]
    }
   ],
   "source": [
    "# Workshop Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from setup_llm import verify_setup, get_chat_model\n",
    "\n",
    "verify_setup()\n",
    "\n",
    "print(\"\\nğŸ“š Session 2, Module 3: Self-Reflection & Quality Patterns\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nThis module covers:\")\n",
    "print(\"\\nPart 1: The Self-Reflection Pattern (15 min)\")\n",
    "print(\"  â€¢ Why single-pass generation fails\")\n",
    "print(\"  â€¢ Generate â†’ Critique â†’ Revise loop\")\n",
    "print(\"  â€¢ Research backing and ROI\")\n",
    "print(\"\\nPart 2: LLM-as-Judge Evaluation (15 min)\")\n",
    "print(\"  â€¢ Structured critique with Pydantic\")\n",
    "print(\"  â€¢ Multi-dimensional scoring rubrics\")\n",
    "print(\"  â€¢ Actionable feedback generation\")\n",
    "print(\"\\nPart 3: Quality Gates & Cost Control (10 min)\")\n",
    "print(\"  â€¢ Threshold-based decisions\")\n",
    "print(\"  â€¢ Iteration limits for cost control\")\n",
    "print(\"  â€¢ Early stopping strategies\")\n",
    "print(\"\\nPart 4: Simple Demo - Report Generator (10 min)\")\n",
    "print(\"  â€¢ End-to-end self-reflection workflow\")\n",
    "print(\"  â€¢ Tracking quality improvements\")\n",
    "print(\"\\nğŸ¯ Build agents that improve their own work!\")\n",
    "print(\"\\nâš ï¸  Prerequisites: Module 1 (Memory), Module 2 (HITL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca9854",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Self-Reflection Pattern\n",
    "\n",
    "### Why Single-Pass Generation Fails\n",
    "\n",
    "**The Problem:**\n",
    "- LLMs often make errors on first attempt\n",
    "- No opportunity for self-correction\n",
    "- Quality varies unpredictably\n",
    "- Missed edge cases and nuances\n",
    "\n",
    "**Self-Reflection adds an iterative improvement loop:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚            SELF-REFLECTION LOOP                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                     â”‚\n",
    "â”‚    Input/Task                                       â”‚\n",
    "â”‚        â”‚                                            â”‚\n",
    "â”‚        â–¼                                            â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚\n",
    "â”‚   â”‚ Generate â”‚ â† Create initial output              â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                      â”‚\n",
    "â”‚        â”‚                                            â”‚\n",
    "â”‚        â–¼                                            â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚\n",
    "â”‚   â”‚ Critique â”‚ â† LLM-as-Judge evaluates             â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                      â”‚\n",
    "â”‚        â”‚                                            â”‚\n",
    "â”‚        â–¼                                            â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚\n",
    "â”‚   â”‚ Quality Gate â”‚ â† Score >= threshold?            â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚\n",
    "â”‚          â”‚                                          â”‚\n",
    "â”‚    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                    â”‚\n",
    "â”‚    â–¼           â–¼                                    â”‚\n",
    "â”‚  [Pass]     [Fail]                                  â”‚\n",
    "â”‚    â”‚           â”‚                                    â”‚\n",
    "â”‚    â”‚      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                               â”‚\n",
    "â”‚    â”‚      â”‚ Revise  â”‚ â† Apply feedback              â”‚\n",
    "â”‚    â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                               â”‚\n",
    "â”‚    â”‚           â””â”€â”€â”€â”€â”€â”€â†’ Back to Critique            â”‚\n",
    "â”‚    â–¼                    (max N iterations)          â”‚\n",
    "â”‚  Output                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Research Backing\n",
    "\n",
    "| Study | Finding |\n",
    "|-------|---------|\n",
    "| **Reflexion (Shinn et al. 2023)** | 91% on HumanEval (+31% vs GPT-4 baseline) |\n",
    "| **Self-Refine (Madaan et al. 2023)** | 20-40% improvement across tasks |\n",
    "| **Constitutional AI (Anthropic)** | 90% reduction in harmful outputs |\n",
    "| **LinkedIn Text-to-SQL** | 55% â†’ 81% accuracy with reflection |\n",
    "\n",
    "### Key Components\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| **Generator** | Creates content (code, text, SQL, etc.) |\n",
    "| **Critic (LLM-as-Judge)** | Evaluates quality with structured rubric |\n",
    "| **Quality Gate** | Numeric threshold to decide pass/fail |\n",
    "| **Reviser** | Applies critique feedback to improve output |\n",
    "| **Iteration Limiter** | Prevents runaway costs (typically max 3) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46713b6b",
   "metadata": {},
   "source": [
    "### When to Use Self-Reflection\n",
    "\n",
    "âœ… **Use for:**\n",
    "- Code generation (SQL, Python, etc.)\n",
    "- Content writing (essays, reports, documentation)\n",
    "- Complex reasoning tasks\n",
    "- High-stakes outputs (legal, financial, medical)\n",
    "\n",
    "âŒ **Skip for:**\n",
    "- Simple lookups or classifications\n",
    "- Real-time requirements (<100ms)\n",
    "- Low-stakes casual chat\n",
    "- Budget-constrained scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba07e7f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: LLM-as-Judge Evaluation\n",
    "\n",
    "### The LLM-as-Judge Pattern\n",
    "\n",
    "Instead of human reviewers, we use an LLM to evaluate outputs using **structured criteria**.\n",
    "\n",
    "**Key Benefits:**\n",
    "- Consistent evaluation (same rubric every time)\n",
    "- Scalable (evaluate thousands of outputs)\n",
    "- Fast (seconds vs hours for human review)\n",
    "- Actionable (structured feedback for revision)\n",
    "\n",
    "### Structured Output for Evaluation\n",
    "\n",
    "Using Pydantic models ensures consistent, parseable evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74e13fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CritiqueResult model defined\n",
      "\n",
      "Structure:\n",
      "  â€¢ 4 scoring dimensions (1-10 each)\n",
      "  â€¢ Strengths (what's working)\n",
      "  â€¢ Weaknesses (what needs work)\n",
      "  â€¢ Revision suggestions (actionable next steps)\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class CritiqueResult(BaseModel):\n",
    "    \"\"\"Structured critique from LLM-as-Judge.\"\"\"\n",
    "    \n",
    "    # Multi-dimensional scoring (1-10 each)\n",
    "    clarity_score: int = Field(\n",
    "        description=\"Clarity and readability (1-10)\",\n",
    "        ge=1, le=10\n",
    "    )\n",
    "    completeness_score: int = Field(\n",
    "        description=\"Coverage of requirements (1-10)\",\n",
    "        ge=1, le=10\n",
    "    )\n",
    "    accuracy_score: int = Field(\n",
    "        description=\"Factual accuracy (1-10)\",\n",
    "        ge=1, le=10\n",
    "    )\n",
    "    overall_score: int = Field(\n",
    "        description=\"Overall quality (1-10)\",\n",
    "        ge=1, le=10\n",
    "    )\n",
    "    \n",
    "    # Actionable feedback\n",
    "    strengths: List[str] = Field(\n",
    "        description=\"2-3 specific strengths\",\n",
    "        min_length=1, max_length=3\n",
    "    )\n",
    "    weaknesses: List[str] = Field(\n",
    "        description=\"2-3 specific areas for improvement\",\n",
    "        min_length=1, max_length=3\n",
    "    )\n",
    "    revision_suggestions: List[str] = Field(\n",
    "        description=\"Actionable suggestions for next revision\",\n",
    "        min_length=1, max_length=3\n",
    "    )\n",
    "\n",
    "print(\"âœ… CritiqueResult model defined\")\n",
    "print(\"\\nStructure:\")\n",
    "print(\"  â€¢ 4 scoring dimensions (1-10 each)\")\n",
    "print(\"  â€¢ Strengths (what's working)\")\n",
    "print(\"  â€¢ Weaknesses (what needs work)\")\n",
    "print(\"  â€¢ Revision suggestions (actionable next steps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef997ca",
   "metadata": {},
   "source": [
    "### Using with_structured_output for Evaluation\n",
    "\n",
    "The `with_structured_output` method ensures consistent, parseable critiques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a1ba2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š LLM-as-Judge Evaluation:\n",
      "\n",
      "   Clarity:      6/10\n",
      "   Completeness: 4/10\n",
      "   Accuracy:     5/10\n",
      "   Overall:      5/10\n",
      "\n",
      "   ğŸ¯ Quality Score: 50.0%\n",
      "\n",
      "âœ¨ Strengths:\n",
      "   â€¢ The explanation is concise and easy to understand.\n",
      "   â€¢ It introduces the concept of AI agents and their utility.\n",
      "\n",
      "âš ï¸  Weaknesses:\n",
      "   â€¢ The description lacks technical depth and specificity.\n",
      "   â€¢ The role of LLMs in AI agents is oversimplified.\n",
      "\n",
      "ğŸ“ Revision Suggestions:\n",
      "   â€¢ Expand on the definition of AI agents, including their components and functionality.\n",
      "   â€¢ Provide examples of tasks AI agents can perform and how LLMs contribute to their capabilities.\n",
      "   â€¢ Clarify the relationship between AI agents and LLMs, emphasizing their integration and limitations.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a critic using structured output\n",
    "model = get_chat_model()\n",
    "critic = model.with_structured_output(CritiqueResult)\n",
    "\n",
    "# Example content to evaluate\n",
    "sample_content = \"\"\"\n",
    "AI agents are programs that can do tasks. They use LLMs to think.\n",
    "Agents can be helpful for many things like answering questions.\n",
    "\"\"\"\n",
    "\n",
    "# Get structured critique\n",
    "critique = critic.invoke([\n",
    "    HumanMessage(content=f\"\"\"Evaluate this brief explanation of AI agents:\n",
    "\n",
    "{sample_content}\n",
    "\n",
    "Rate it for a technical blog audience. Be constructive but honest.\"\"\")\n",
    "])\n",
    "\n",
    "print(\"ğŸ“Š LLM-as-Judge Evaluation:\")\n",
    "print(f\"\\n   Clarity:      {critique.clarity_score}/10\")\n",
    "print(f\"   Completeness: {critique.completeness_score}/10\")\n",
    "print(f\"   Accuracy:     {critique.accuracy_score}/10\")\n",
    "print(f\"   Overall:      {critique.overall_score}/10\")\n",
    "\n",
    "# Calculate normalized score (0-1)\n",
    "quality_score = (critique.clarity_score + critique.completeness_score + \n",
    "                 critique.accuracy_score + critique.overall_score) / 40\n",
    "print(f\"\\n   ğŸ¯ Quality Score: {quality_score:.1%}\")\n",
    "\n",
    "print(f\"\\nâœ¨ Strengths:\")\n",
    "for s in critique.strengths:\n",
    "    print(f\"   â€¢ {s}\")\n",
    "\n",
    "print(f\"\\nâš ï¸  Weaknesses:\")\n",
    "for w in critique.weaknesses:\n",
    "    print(f\"   â€¢ {w}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Revision Suggestions:\")\n",
    "for r in critique.revision_suggestions:\n",
    "    print(f\"   â€¢ {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083e31d6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Simple Demo - Self-Improving Report Generator\n",
    "\n",
    "Let's build a complete self-reflection workflow for generating technical summaries.\n",
    "\n",
    "### What We'll Build\n",
    "\n",
    "1. **Generator Node**: Creates initial summary\n",
    "2. **Critic Node**: LLM-as-Judge evaluation\n",
    "3. **Quality Gate**: Pass/fail routing\n",
    "4. **Reviser**: Improves based on feedback (loops back to Generator)\n",
    "5. **Finalizer**: Outputs result with improvement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32b38078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ReflectionState defined\n",
      "\n",
      "Key fields:\n",
      "  â€¢ topic, requirements: Input\n",
      "  â€¢ content: Generated output (updated each revision)\n",
      "  â€¢ critique, quality_score: Evaluation results\n",
      "  â€¢ iteration, max_iterations, threshold: Control flow\n",
      "  â€¢ score_history: Track improvement over iterations\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Literal, List\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# STATE DEFINITION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class ReflectionState(TypedDict):\n",
    "    \"\"\"State for self-reflection workflow.\"\"\"\n",
    "    \n",
    "    # Input\n",
    "    topic: str\n",
    "    requirements: str\n",
    "    \n",
    "    # Generated content\n",
    "    content: str\n",
    "    \n",
    "    # Critique\n",
    "    critique: dict\n",
    "    quality_score: float  # 0.0 to 1.0\n",
    "    \n",
    "    # Control\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    threshold: float\n",
    "    \n",
    "    # History (for tracking improvement)\n",
    "    score_history: List[float]\n",
    "\n",
    "print(\"âœ… ReflectionState defined\")\n",
    "print(\"\\nKey fields:\")\n",
    "print(\"  â€¢ topic, requirements: Input\")\n",
    "print(\"  â€¢ content: Generated output (updated each revision)\")\n",
    "print(\"  â€¢ critique, quality_score: Evaluation results\")\n",
    "print(\"  â€¢ iteration, max_iterations, threshold: Control flow\")\n",
    "print(\"  â€¢ score_history: Track improvement over iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4206d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… generate node defined\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# NODE FUNCTIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def generate(state: ReflectionState) -> dict:\n",
    "    \"\"\"Generate or revise content based on feedback.\"\"\"\n",
    "    \n",
    "    model = get_chat_model()\n",
    "    iteration = state.get(\"iteration\", 0) + 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“ GENERATE - Iteration {iteration}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if state.get(\"critique\"):  # Revision mode\n",
    "        critique = state[\"critique\"]\n",
    "        prompt = f\"\"\"Revise this content based on feedback:\n",
    "\n",
    "TOPIC: {state['topic']}\n",
    "REQUIREMENTS: {state['requirements']}\n",
    "\n",
    "CURRENT CONTENT:\n",
    "{state['content']}\n",
    "\n",
    "FEEDBACK TO ADDRESS:\n",
    "Weaknesses: {critique.get('weaknesses', [])}\n",
    "Suggestions: {critique.get('revision_suggestions', [])}\n",
    "\n",
    "Write an IMPROVED version addressing all feedback.\"\"\"\n",
    "        print(\"   Mode: REVISION (applying feedback)\")\n",
    "    else:  # Initial generation\n",
    "        prompt = f\"\"\"Write a clear, concise summary on this topic:\n",
    "\n",
    "TOPIC: {state['topic']}\n",
    "REQUIREMENTS: {state['requirements']}\n",
    "\n",
    "Provide a well-structured, informative summary.\"\"\"\n",
    "        print(\"   Mode: INITIAL GENERATION\")\n",
    "    \n",
    "    response = model.invoke([HumanMessage(content=prompt)])\n",
    "    content = response.content.strip()\n",
    "    \n",
    "    print(f\"   Generated {len(content)} characters\")\n",
    "    \n",
    "    return {\n",
    "        \"content\": content,\n",
    "        \"iteration\": iteration\n",
    "    }\n",
    "\n",
    "print(\"âœ… generate node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8a0f941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… critique node defined\n"
     ]
    }
   ],
   "source": [
    "def critique(state: ReflectionState) -> dict:\n",
    "    \"\"\"LLM-as-Judge evaluation with structured output.\"\"\"\n",
    "    \n",
    "    model = get_chat_model()\n",
    "    critic = model.with_structured_output(CritiqueResult)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ” CRITIQUE - LLM-as-Judge\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = critic.invoke([\n",
    "        HumanMessage(content=f\"\"\"Evaluate this content rigorously:\n",
    "\n",
    "TOPIC: {state['topic']}\n",
    "REQUIREMENTS: {state['requirements']}\n",
    "\n",
    "CONTENT TO EVALUATE:\n",
    "{state['content']}\n",
    "\n",
    "Provide honest, constructive feedback with specific examples.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # Calculate normalized quality score\n",
    "    scores = [result.clarity_score, result.completeness_score,\n",
    "              result.accuracy_score, result.overall_score]\n",
    "    quality_score = sum(scores) / 40  # Normalize to 0-1\n",
    "    \n",
    "    critique_dict = {\n",
    "        \"clarity\": result.clarity_score,\n",
    "        \"completeness\": result.completeness_score,\n",
    "        \"accuracy\": result.accuracy_score,\n",
    "        \"overall\": result.overall_score,\n",
    "        \"strengths\": result.strengths,\n",
    "        \"weaknesses\": result.weaknesses,\n",
    "        \"revision_suggestions\": result.revision_suggestions\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n   ğŸ“Š Scores: Clarity={result.clarity_score}, Completeness={result.completeness_score}\")\n",
    "    print(f\"              Accuracy={result.accuracy_score}, Overall={result.overall_score}\")\n",
    "    print(f\"   ğŸ¯ Quality Score: {quality_score:.1%} (threshold: {state['threshold']:.1%})\")\n",
    "    \n",
    "    # Track score history\n",
    "    history = state.get(\"score_history\", [])\n",
    "    history.append(quality_score)\n",
    "    \n",
    "    return {\n",
    "        \"critique\": critique_dict,\n",
    "        \"quality_score\": quality_score,\n",
    "        \"score_history\": history\n",
    "    }\n",
    "\n",
    "print(\"âœ… critique node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd3b8fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… should_continue routing defined\n"
     ]
    }
   ],
   "source": [
    "def should_continue(state: ReflectionState) -> Literal[\"revise\", \"finalize\"]:\n",
    "    \"\"\"Quality gate: decide whether to continue or finalize.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸš¦ QUALITY GATE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   Score: {state['quality_score']:.1%} | Threshold: {state['threshold']:.1%}\")\n",
    "    print(f\"   Iteration: {state['iteration']}/{state['max_iterations']}\")\n",
    "    \n",
    "    # Check quality threshold\n",
    "    if state[\"quality_score\"] >= state[\"threshold\"]:\n",
    "        print(f\"   âœ… PASS: Quality threshold met!\")\n",
    "        return \"finalize\"\n",
    "    \n",
    "    # Check iteration limit\n",
    "    if state[\"iteration\"] >= state[\"max_iterations\"]:\n",
    "        print(f\"   âš ï¸  MAX ITERATIONS: Cost control triggered\")\n",
    "        return \"finalize\"\n",
    "    \n",
    "    print(f\"   ğŸ”„ CONTINUE: Sending for revision\")\n",
    "    return \"revise\"\n",
    "\n",
    "print(\"âœ… should_continue routing defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "593662a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… finalize node defined\n"
     ]
    }
   ],
   "source": [
    "def finalize(state: ReflectionState) -> dict:\n",
    "    \"\"\"Output final content with improvement metrics.\"\"\"\n",
    "    \n",
    "    history = state.get(\"score_history\", [])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"âœ… FINALIZE - Self-Reflection Complete\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    if len(history) >= 2:\n",
    "        initial = history[0]\n",
    "        final = history[-1]\n",
    "        improvement = final - initial\n",
    "        print(f\"\\n   ğŸ“ˆ IMPROVEMENT METRICS:\")\n",
    "        print(f\"      Initial: {initial:.1%}\")\n",
    "        print(f\"      Final:   {final:.1%}\")\n",
    "        print(f\"      Gain:    {improvement:+.1%}\")\n",
    "    \n",
    "    print(f\"\\n   ğŸ“Š Quality Progression:\")\n",
    "    for i, score in enumerate(history, 1):\n",
    "        bar = \"â–ˆ\" * int(score * 20)\n",
    "        print(f\"      Iteration {i}: {bar} {score:.1%}\")\n",
    "    \n",
    "    print(f\"\\n   ğŸ“ Final Content ({len(state['content'])} chars):\")\n",
    "    print(f\"   {'-'*50}\")\n",
    "    preview = state['content'][:400] + \"...\" if len(state['content']) > 400 else state['content']\n",
    "    print(f\"   {preview}\")\n",
    "    \n",
    "    return {}\n",
    "\n",
    "print(\"âœ… finalize node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8819fdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Self-Reflection Graph Compiled!\n",
      "\n",
      "Graph Structure:\n",
      "\n",
      "    START\n",
      "      â”‚\n",
      "      â–¼\n",
      "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "  â”‚ generate â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                   â”‚\n",
      "       â”‚                         â”‚\n",
      "       â–¼                         â”‚\n",
      "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
      "  â”‚ critique â”‚                   â”‚\n",
      "  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                   â”‚\n",
      "       â”‚                         â”‚\n",
      "       â–¼                         â”‚\n",
      "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
      "  â”‚  quality  â”‚                  â”‚\n",
      "  â”‚   gate    â”‚                  â”‚\n",
      "  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                  â”‚\n",
      "        â”‚                        â”‚\n",
      "   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”                   â”‚\n",
      "   â–¼         â–¼                   â”‚\n",
      " [pass]   [fail]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "   â”‚       (revise)\n",
      "   â–¼\n",
      "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "  â”‚ finalize â”‚\n",
      "  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
      "       â”‚\n",
      "       â–¼\n",
      "      END\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# BUILD THE GRAPH\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "builder = StateGraph(ReflectionState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"generate\", generate)\n",
    "builder.add_node(\"critique\", critique)\n",
    "builder.add_node(\"finalize\", finalize)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"generate\")\n",
    "builder.add_edge(\"generate\", \"critique\")\n",
    "\n",
    "# Quality gate: conditional routing after critique\n",
    "builder.add_conditional_edges(\n",
    "    \"critique\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"revise\": \"generate\",   # Loop back for improvement\n",
    "        \"finalize\": \"finalize\"  # Exit to finalization\n",
    "    }\n",
    ")\n",
    "\n",
    "builder.add_edge(\"finalize\", END)\n",
    "\n",
    "# Compile\n",
    "reflection_graph = builder.compile()\n",
    "\n",
    "print(\"âœ… Self-Reflection Graph Compiled!\")\n",
    "print(\"\"\"\n",
    "Graph Structure:\n",
    "\n",
    "    START\n",
    "      â”‚\n",
    "      â–¼\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ generate â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "       â”‚                         â”‚\n",
    "       â–¼                         â”‚\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "  â”‚ critique â”‚                   â”‚\n",
    "  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "       â”‚                         â”‚\n",
    "       â–¼                         â”‚\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "  â”‚  quality  â”‚                  â”‚\n",
    "  â”‚   gate    â”‚                  â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "        â”‚                        â”‚\n",
    "   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”                   â”‚\n",
    "   â–¼         â–¼                   â”‚\n",
    " [pass]   [fail]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   â”‚       (revise)\n",
    "   â–¼\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ finalize â”‚\n",
    "  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â–¼\n",
    "      END\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "776808c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Running Self-Reflection Report Generator\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“ GENERATE - Iteration 1\n",
      "============================================================\n",
      "   Mode: INITIAL GENERATION\n",
      "   Generated 1429 characters\n",
      "\n",
      "============================================================\n",
      "ğŸ” CRITIQUE - LLM-as-Judge\n",
      "============================================================\n",
      "\n",
      "   ğŸ“Š Scores: Clarity=9, Completeness=8\n",
      "              Accuracy=9, Overall=9\n",
      "   ğŸ¯ Quality Score: 87.5% (threshold: 75.0%)\n",
      "\n",
      "============================================================\n",
      "ğŸš¦ QUALITY GATE\n",
      "============================================================\n",
      "   Score: 87.5% | Threshold: 75.0%\n",
      "   Iteration: 1/3\n",
      "   âœ… PASS: Quality threshold met!\n",
      "\n",
      "============================================================\n",
      "âœ… FINALIZE - Self-Reflection Complete\n",
      "============================================================\n",
      "\n",
      "   ğŸ“Š Quality Progression:\n",
      "      Iteration 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 87.5%\n",
      "\n",
      "   ğŸ“ Final Content (1429 chars):\n",
      "   --------------------------------------------------\n",
      "   **Benefits of LangGraph for Production AI Agents**\n",
      "\n",
      "LangGraph is a powerful framework designed to enhance the development and deployment of production-grade AI agents. By addressing key challenges in scalability, reliability, and adaptability, LangGraph offers several distinct advantages for engineering teams building AI-driven systems.\n",
      "\n",
      "First, LangGraph excels in **state management**, enabling AI...\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ SELF-REFLECTION COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# RUN THE SELF-REFLECTION WORKFLOW\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"ğŸš€ Running Self-Reflection Report Generator\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = reflection_graph.invoke({\n",
    "    \"topic\": \"Benefits of LangGraph for Production AI Agents\",\n",
    "    \"requirements\": \"\"\"\n",
    "- 150-200 words\n",
    "- Include 3 specific benefits\n",
    "- Mention state management and human-in-the-loop\n",
    "- Professional tone for engineering blog\n",
    "\"\"\",\n",
    "    \"content\": \"\",\n",
    "    \"critique\": {},\n",
    "    \"quality_score\": 0.0,\n",
    "    \"iteration\": 0,\n",
    "    \"max_iterations\": 3,\n",
    "    \"threshold\": 0.75,  # 75% quality required\n",
    "    \"score_history\": []\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ¯ SELF-REFLECTION COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be72bfd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Self-Reflection Pattern\n",
    "\n",
    "| Component | Purpose | Implementation |\n",
    "|-----------|---------|----------------|\n",
    "| **Generator** | Creates initial output | Single node, switches between generate/revise |\n",
    "| **Critic (LLM-as-Judge)** | Evaluates quality | `with_structured_output(CritiqueResult)` |\n",
    "| **Quality Gate** | Pass/fail decision | Conditional edge with threshold check |\n",
    "| **Iteration Limiter** | Cost control | `max_iterations` parameter |\n",
    "\n",
    "### Production Tips\n",
    "\n",
    "1. **Start Conservative**: Begin with threshold=0.7, max_iterations=3\n",
    "2. **Track Improvement**: Log score_history to validate the pattern helps\n",
    "3. **Cost Awareness**: Each iteration costs ~$0.008 (GPT-4o)\n",
    "4. **Diminishing Returns**: Most gains happen in first 2 iterations\n",
    "\n",
    "### When to Use\n",
    "\n",
    "| Use Case | Recommended? | Reason |\n",
    "|----------|--------------|--------|\n",
    "| Content generation | âœ… Yes | High ROI, visible quality improvement |\n",
    "| Code generation | âœ… Yes | Catches bugs, improves structure |\n",
    "| Simple Q&A | âŒ No | Overkill, adds latency |\n",
    "| Real-time chat | âŒ No | Too slow for conversational UX |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ac0ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "### Module 4: Multi-Agent Collaboration\n",
    "\n",
    "Build on self-reflection by adding **multiple specialized agents**:\n",
    "- Hierarchical agents (supervisor â†’ workers)\n",
    "- Peer review patterns (proposer â†” reviewer)\n",
    "- Consensus building across agents\n",
    "- Send API for dynamic parallelism\n",
    "\n",
    "### Connection to Project\n",
    "\n",
    "The **Incident PostMortem Generator** project combines:\n",
    "- Self-reflection for report quality (this module!)\n",
    "- Multiple agents: Log Analyzer, Root Cause Agent, Report Writer\n",
    "- Human review checkpoints\n",
    "- Production streaming output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c7929-f9c0-4df2-8809-938c2c21491d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
