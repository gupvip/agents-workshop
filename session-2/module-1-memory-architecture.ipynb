{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f858974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Checking LLM Configuration...\n",
      "==================================================\n",
      "ðŸ“¡ Provider: DIAL (Azure OpenAI via EPAM AI Proxy)\n",
      "âœ… DIAL_API_KEY is set\n",
      "\n",
      "ðŸ“‹ Configuration:\n",
      "   AZURE_OPENAI_ENDPOINT: https://ai-proxy.lab.epam.com\n",
      "   AZURE_OPENAI_API_VERSION: 2024-02-01\n",
      "   AZURE_OPENAI_DEPLOYMENT_NAME: gpt-4\n",
      "\n",
      "âœ… DIAL setup verified successfully!\n",
      "âœ… DIAL configuration loaded successfully\n",
      "\n",
      "ðŸ“š Session 2, Module 1: Memory Architecture Deep Dive\n",
      "============================================================\n",
      "\n",
      "ðŸ”— Prerequisites:\n",
      "   â€¢ Module 0: Opening (Session recap)\n",
      "   â€¢ Session 1 M5: MemorySaver basics, thread IDs\n",
      "\n",
      "ðŸŽ¯ What you'll learn:\n",
      "   Part 1: Production Checkpointers\n",
      "   Part 2: Store API & Long-Term Memory\n",
      "   Part 3: Semantic Memory with Embeddings\n",
      "   Part 4: Real-Time Streaming\n",
      "   HANDS-ON: Memory-augmented assistant\n"
     ]
    }
   ],
   "source": [
    "# Workshop Setup - Run this cell first!\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from setup_llm import verify_setup, get_chat_model, get_deployment_name\n",
    "\n",
    "if verify_setup():\n",
    "    print(\"âœ… DIAL configuration loaded successfully\")\n",
    "else:\n",
    "    print(\"âŒ DIAL configuration not found. Please set DIAL_API_KEY in your .env file\")\n",
    "\n",
    "print(\"\\nðŸ“š Session 2, Module 1: Memory Architecture Deep Dive\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nðŸ”— Prerequisites:\")\n",
    "print(\"   â€¢ Module 0: Opening (Session recap)\")\n",
    "print(\"   â€¢ Session 1 M5: MemorySaver basics, thread IDs\")\n",
    "print(\"\\nðŸŽ¯ What you'll learn:\")\n",
    "print(\"   Part 1: Production Checkpointers\")\n",
    "print(\"   Part 2: Store API & Long-Term Memory\")\n",
    "print(\"   Part 3: Semantic Memory with Embeddings\")\n",
    "print(\"   Part 4: Real-Time Streaming\")\n",
    "print(\"   HANDS-ON: Memory-augmented assistant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000544a",
   "metadata": {},
   "source": [
    "## Module Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. **Implement production memory** using LangGraph's Store API\n",
    "2. **Design namespaced storage** for multi-tenant applications\n",
    "3. **Build semantic search** for memory retrieval using embeddings\n",
    "4. **Stream LLM outputs** in real-time for better UX\n",
    "5. **Use multiple streaming modes** for different use cases\n",
    "\n",
    "---\n",
    "\n",
    "## Memory Architecture: From Conceptual to Hands-On\n",
    "\n",
    "### Recap: What You Learned in Session 1\n",
    "\n",
    "In **Session 1 Module 5**, you learned:\n",
    "- `MemorySaver` for in-memory checkpointing\n",
    "- `thread_id` for conversation persistence\n",
    "- Basic state snapshots and recovery\n",
    "\n",
    "**The limitation**: MemorySaver only handles **short-term, thread-scoped** memory. \n",
    "\n",
    "### What's Missing for Production?\n",
    "\n",
    "Real production agents need:\n",
    "\n",
    "| Memory Type | Session 1 | This Module |\n",
    "|------------|-----------|-------------|\n",
    "| **Short-term** (conversation) | âœ… MemorySaver | âœ… MemorySaver |\n",
    "| **Long-term** (cross-session) | âŒ Not covered | âœ… Store API |\n",
    "| **Semantic search** (by meaning) | âŒ Not covered | âœ… Embeddings |\n",
    "| **Multi-tenant** (user isolation) | âŒ Not covered | âœ… Namespaces |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc24b7",
   "metadata": {},
   "source": [
    "## Part 1: Long-Term Memory with Store API\n",
    "\n",
    "### The Store Concept\n",
    "\n",
    "LangGraph's **Store** is a key-value database for long-term memories:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      STORE                                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Namespace: (\"user_123\", \"preferences\")                      â”‚\n",
    "â”‚  â”œâ”€â”€ Key: \"communication_style\" â†’ {\"tone\": \"formal\"}        â”‚\n",
    "â”‚  â””â”€â”€ Key: \"topics_of_interest\" â†’ {\"topics\": [\"AI\", \"ML\"]}   â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  Namespace: (\"user_123\", \"memories\")                         â”‚\n",
    "â”‚  â”œâ”€â”€ Key: \"mem_001\" â†’ {\"text\": \"User is a data scientist\"}  â”‚\n",
    "â”‚  â””â”€â”€ Key: \"mem_002\" â†’ {\"text\": \"User prefers Python\"}       â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  Namespace: (\"user_456\", \"preferences\")                      â”‚\n",
    "â”‚  â””â”€â”€ Key: \"communication_style\" â†’ {\"tone\": \"casual\"}        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key Operations**:\n",
    "- `store.put(namespace, key, value)` - Save a memory\n",
    "- `store.get(namespace, key)` - Retrieve by exact key\n",
    "- `store.search(namespace, query=...)` - Semantic search (with embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a55931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Basic Store populated\n",
      "\n",
      "ðŸ“ Stored in namespace ('user_123', 'preferences'):\n",
      "   - communication_style\n",
      "   - topics_of_interest\n",
      "\n",
      "ðŸ“ Stored in namespace ('user_123', 'memories'):\n",
      "   - mem_001\n",
      "   - mem_002\n"
     ]
    }
   ],
   "source": [
    "# Demo 1: Basic Store Operations\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Create a basic store (no embeddings yet)\n",
    "basic_store = InMemoryStore()\n",
    "\n",
    "# Define namespaces for different users and contexts\n",
    "user_id = \"user_123\"\n",
    "preferences_namespace = (user_id, \"preferences\")\n",
    "memories_namespace = (user_id, \"memories\")\n",
    "\n",
    "# Store user preferences\n",
    "basic_store.put(\n",
    "    preferences_namespace,\n",
    "    \"communication_style\",\n",
    "    {\n",
    "        \"tone\": \"professional\",\n",
    "        \"verbosity\": \"concise\",\n",
    "        \"language\": \"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "basic_store.put(\n",
    "    preferences_namespace,\n",
    "    \"topics_of_interest\",\n",
    "    {\n",
    "        \"topics\": [\"Agentic AI\", \"LangGraph\", \"Python\"],\n",
    "        \"expertise_level\": \"advanced\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Store user memories (facts learned during conversations)\n",
    "basic_store.put(\n",
    "    memories_namespace,\n",
    "    \"mem_001\",\n",
    "    {\n",
    "        \"text\": \"User is a solutions architect at a Fortune 500 company\",\n",
    "        \"timestamp\": \"2024-12-01\"\n",
    "    }\n",
    ")\n",
    "\n",
    "basic_store.put(\n",
    "    memories_namespace,\n",
    "    \"mem_002\", \n",
    "    {\n",
    "        \"text\": \"User is building a customer support automation system\",\n",
    "        \"timestamp\": \"2024-12-02\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"âœ… Basic Store populated\")\n",
    "print(f\"\\nðŸ“ Stored in namespace {preferences_namespace}:\")\n",
    "print(f\"   - communication_style\")\n",
    "print(f\"   - topics_of_interest\")\n",
    "print(f\"\\nðŸ“ Stored in namespace {memories_namespace}:\")\n",
    "print(f\"   - mem_001\")\n",
    "print(f\"   - mem_002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ae1d48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEMO: Retrieving Memories by Key\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Communication Style:\n",
      "   {'tone': 'professional', 'verbosity': 'concise', 'language': 'English'}\n",
      "\n",
      "ðŸ“ Memory (mem_001):\n",
      "   {'text': 'User is a solutions architect at a Fortune 500 company', 'timestamp': '2024-12-01'}\n",
      "\n",
      "ðŸ“‹ All items in memories namespace:\n",
      "   Key: mem_001\n",
      "   Value: {'text': 'User is a solutions architect at a Fortune 500 company', 'timestamp': '2024-12-01'}\n",
      "\n",
      "   Key: mem_002\n",
      "   Value: {'text': 'User is building a customer support automation system', 'timestamp': '2024-12-02'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve memories by exact key\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMO: Retrieving Memories by Key\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get specific preference\n",
    "style_item = basic_store.get(preferences_namespace, \"communication_style\")\n",
    "print(f\"\\nðŸ“ Communication Style:\")\n",
    "print(f\"   {style_item.value}\")\n",
    "\n",
    "# Get specific memory\n",
    "memory_item = basic_store.get(memories_namespace, \"mem_001\")\n",
    "print(f\"\\nðŸ“ Memory (mem_001):\")\n",
    "print(f\"   {memory_item.value}\")\n",
    "\n",
    "# List all items in a namespace using search (without embeddings)\n",
    "print(f\"\\nðŸ“‹ All items in memories namespace:\")\n",
    "all_memories = basic_store.search(memories_namespace)\n",
    "for item in all_memories:\n",
    "    print(f\"   Key: {item.key}\")\n",
    "    print(f\"   Value: {item.value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358086e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Semantic Search with Embeddings\n",
    "\n",
    "### Why Semantic Search?\n",
    "\n",
    "**Problem with exact-key retrieval**: \n",
    "- You need to know the exact key to retrieve a memory\n",
    "- No way to find \"relevant\" memories based on meaning\n",
    "\n",
    "**Solution**: Configure the Store with embeddings for semantic search\n",
    "\n",
    "```\n",
    "User asks: \"What projects am I working on?\"\n",
    "â†“\n",
    "Store.search(query=\"projects work building\")\n",
    "â†“\n",
    "Returns: \"User is building a customer support automation system\"\n",
    "```\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. When you `put()` a memory, it's automatically embedded\n",
    "2. When you `search()` with a query, the query is embedded\n",
    "3. Results are ranked by **cosine similarity** to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0c83278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Semantic Store created with embedding support\n"
     ]
    }
   ],
   "source": [
    "# Demo 2: Semantic Search with Embeddings\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "# Initialize embeddings (using Azure OpenAI via DIAL)\n",
    "# Note: In production, use text-embedding-3-small or ada-002\n",
    "def get_embeddings():\n",
    "    \"\"\"Get embeddings model - using a simple mock for demo\"\"\"\n",
    "    # For actual production, uncomment below:\n",
    "    # return AzureOpenAIEmbeddings(\n",
    "    #     azure_endpoint=os.getenv(\"DIAL_API_BASE\"),\n",
    "    #     api_key=os.getenv(\"DIAL_API_KEY\"),\n",
    "    #     model=\"text-embedding-ada-002\"\n",
    "    # )\n",
    "    \n",
    "    # Simple mock embeddings for demo (deterministic based on text length)\n",
    "    def mock_embed(texts: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Generate simple deterministic embeddings based on text features\"\"\"\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            text_lower = text.lower()\n",
    "            # Create a simple feature vector based on keywords\n",
    "            vec = [\n",
    "                1.0 if \"architect\" in text_lower else 0.0,\n",
    "                1.0 if \"python\" in text_lower else 0.0,\n",
    "                1.0 if \"ai\" in text_lower or \"automation\" in text_lower else 0.0,\n",
    "                1.0 if \"support\" in text_lower or \"customer\" in text_lower else 0.0,\n",
    "                1.0 if \"project\" in text_lower or \"building\" in text_lower else 0.0,\n",
    "                1.0 if \"data\" in text_lower else 0.0,\n",
    "                1.0 if \"enterprise\" in text_lower or \"company\" in text_lower else 0.0,\n",
    "                len(text) / 100.0,  # Normalized length\n",
    "            ]\n",
    "            embeddings.append(vec)\n",
    "        return embeddings\n",
    "    \n",
    "    return mock_embed\n",
    "\n",
    "# Create store WITH embeddings enabled\n",
    "embeddings_fn = get_embeddings()\n",
    "semantic_store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embeddings_fn,  # Embedding function\n",
    "        \"dims\": 8,               # Dimension of embeddings\n",
    "        \"fields\": [\"text\", \"$\"]  # Fields to embed (text field + all root fields)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"âœ… Semantic Store created with embedding support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a093d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added 8 memories to semantic store\n",
      "\n",
      "ðŸ“ Sample memories:\n",
      "   - User is a solutions architect at a Fortune 500 company...\n",
      "   - User is building a customer support automation system with A...\n",
      "   - User prefers Python for backend development...\n"
     ]
    }
   ],
   "source": [
    "# Populate the semantic store with user memories\n",
    "user_id = \"user_123\"\n",
    "memories_ns = (user_id, \"memories\")\n",
    "\n",
    "# Add diverse memories about the user\n",
    "memories = [\n",
    "    {\"id\": \"mem_001\", \"text\": \"User is a solutions architect at a Fortune 500 company\", \"category\": \"role\"},\n",
    "    {\"id\": \"mem_002\", \"text\": \"User is building a customer support automation system with AI agents\", \"category\": \"project\"},\n",
    "    {\"id\": \"mem_003\", \"text\": \"User prefers Python for backend development\", \"category\": \"preference\"},\n",
    "    {\"id\": \"mem_004\", \"text\": \"User has 10 years of enterprise software experience\", \"category\": \"background\"},\n",
    "    {\"id\": \"mem_005\", \"text\": \"User is evaluating LangGraph vs CrewAI for their project\", \"category\": \"project\"},\n",
    "    {\"id\": \"mem_006\", \"text\": \"User's company handles 50,000 support tickets daily\", \"category\": \"context\"},\n",
    "    {\"id\": \"mem_007\", \"text\": \"User is interested in agentic AI and multi-agent systems\", \"category\": \"interest\"},\n",
    "    {\"id\": \"mem_008\", \"text\": \"User needs real-time streaming for customer-facing chatbot\", \"category\": \"requirement\"},\n",
    "]\n",
    "\n",
    "for mem in memories:\n",
    "    semantic_store.put(memories_ns, mem[\"id\"], {\"text\": mem[\"text\"], \"category\": mem[\"category\"]})\n",
    "\n",
    "print(f\"âœ… Added {len(memories)} memories to semantic store\")\n",
    "print(\"\\nðŸ“ Sample memories:\")\n",
    "for mem in memories[:3]:\n",
    "    print(f\"   - {mem['text'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5edf95ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEMO: Semantic Search in Action\n",
      "============================================================\n",
      "\n",
      "ðŸ” Query: 'What projects is the user working on?'\n",
      "\n",
      "ðŸ“‹ Top 3 relevant memories:\n",
      "\n",
      "   1. User is evaluating LangGraph vs CrewAI for their project\n",
      "      Category: project\n",
      "      Key: mem_005\n",
      "\n",
      "   2. User is building a customer support automation system with AI agents\n",
      "      Category: project\n",
      "      Key: mem_002\n",
      "\n",
      "   3. User is interested in agentic AI and multi-agent systems\n",
      "      Category: interest\n",
      "      Key: mem_007\n"
     ]
    }
   ],
   "source": [
    "# Demo: Semantic Search\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMO: Semantic Search in Action\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Search for memories related to \"projects\"\n",
    "print(\"\\nðŸ” Query: 'What projects is the user working on?'\")\n",
    "results = semantic_store.search(\n",
    "    memories_ns,\n",
    "    query=\"projects building working on automation\",\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“‹ Top 3 relevant memories:\")\n",
    "for i, item in enumerate(results, 1):\n",
    "    print(f\"\\n   {i}. {item.value['text']}\")\n",
    "    print(f\"      Category: {item.value['category']}\")\n",
    "    print(f\"      Key: {item.key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4726443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "More Semantic Search Examples\n",
      "============================================================\n",
      "\n",
      "ðŸ” Query: 'What is the user's technical background?'\n",
      "   â†’ User is a solutions architect at a Fortune 500 company...\n",
      "   â†’ User prefers Python for backend development...\n",
      "\n",
      "ðŸ” Query: 'What are the user's requirements?'\n",
      "   â†’ User needs real-time streaming for customer-facing chatbot...\n",
      "   â†’ User is building a customer support automation system with AI agents...\n",
      "\n",
      "ðŸ” Query: 'What AI tools is the user considering?'\n",
      "   â†’ User is interested in agentic AI and multi-agent systems...\n",
      "   â†’ User is evaluating LangGraph vs CrewAI for their project...\n"
     ]
    }
   ],
   "source": [
    "# More semantic search examples\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"More Semantic Search Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "queries = [\n",
    "    (\"What is the user's technical background?\", \"architect enterprise Python experience\"),\n",
    "    (\"What are the user's requirements?\", \"real-time streaming customer requirement\"),\n",
    "    (\"What AI tools is the user considering?\", \"AI agents LangGraph evaluation\"),\n",
    "]\n",
    "\n",
    "for display_query, search_query in queries:\n",
    "    print(f\"\\nðŸ” Query: '{display_query}'\")\n",
    "    results = semantic_store.search(memories_ns, query=search_query, limit=2)\n",
    "    for item in results:\n",
    "        print(f\"   â†’ {item.value['text'][:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f91da98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DEMO: Combining Semantic Search with Filters\n",
      "============================================================\n",
      "\n",
      "ðŸ” Query: 'AI' + filter by category='project'\n",
      "\n",
      "ðŸ“‹ Filtered results (projects only):\n",
      "   â†’ [project] User is evaluating LangGraph vs CrewAI for their project\n",
      "   â†’ [project] User is building a customer support automation system with AI agents\n"
     ]
    }
   ],
   "source": [
    "# Demo: Filtering with search\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEMO: Combining Semantic Search with Filters\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Search with category filter\n",
    "print(\"\\nðŸ” Query: 'AI' + filter by category='project'\")\n",
    "results = semantic_store.search(\n",
    "    memories_ns,\n",
    "    query=\"AI automation building\",\n",
    "    filter={\"category\": \"project\"},  # Only return project-related memories\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“‹ Filtered results (projects only):\")\n",
    "for item in results:\n",
    "    print(f\"   â†’ [{item.value['category']}] {item.value['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5901b3e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Integrating Store with LangGraph Agents\n",
    "\n",
    "### The Pattern: Store + Checkpointer\n",
    "\n",
    "In production, you use **both**:\n",
    "- **Checkpointer** (MemorySaver/PostgresSaver): Thread-scoped state snapshots\n",
    "- **Store** (InMemoryStore/PostgresStore): Cross-thread long-term memory\n",
    "\n",
    "```python\n",
    "graph = builder.compile(\n",
    "    checkpointer=memory_saver,  # Thread state\n",
    "    store=long_term_store       # Cross-thread memories\n",
    ")\n",
    "```\n",
    "\n",
    "### Accessing Store in Nodes\n",
    "\n",
    "Nodes can access the store via `store: BaseStore` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38aff9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… State and LLM initialized\n"
     ]
    }
   ],
   "source": [
    "# Demo 3: Agent with Long-Term Memory\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.store.base import BaseStore\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "import operator\n",
    "\n",
    "llm = get_chat_model()\n",
    "\n",
    "# Define state\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, operator.add]\n",
    "    user_id: str\n",
    "\n",
    "print(\"âœ… State and LLM initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c82e0ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Memory-augmented agent defined\n"
     ]
    }
   ],
   "source": [
    "# Create the memory-augmented agent node\n",
    "def memory_augmented_agent(\n",
    "    state: AgentState, \n",
    "    config: RunnableConfig, \n",
    "    *, \n",
    "    store: BaseStore\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Agent that uses long-term memory to personalize responses.\n",
    "    \n",
    "    Key: store is injected automatically when graph is compiled with store=\n",
    "    \"\"\"\n",
    "    user_id = state[\"user_id\"]\n",
    "    memories_ns = (user_id, \"memories\")\n",
    "    \n",
    "    # Get user's last message\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    \n",
    "    # Search for relevant memories\n",
    "    relevant_memories = store.search(\n",
    "        memories_ns,\n",
    "        query=last_message,\n",
    "        limit=3\n",
    "    )\n",
    "    \n",
    "    # Format memories for the prompt\n",
    "    memory_context = \"\"\n",
    "    if relevant_memories:\n",
    "        memory_lines = [f\"- {item.value.get('text', str(item.value))}\" for item in relevant_memories]\n",
    "        memory_context = f\"\\n\\nRelevant memories about this user:\\n\" + \"\\n\".join(memory_lines)\n",
    "    \n",
    "    # Build prompt with memory context\n",
    "    system_prompt = f\"\"\"You are a helpful assistant with access to long-term memory about users.\n",
    "{memory_context}\n",
    "\n",
    "Use this context to personalize your responses. If the memories are relevant, reference them naturally.\"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + [\n",
    "        {\"role\": \"user\" if isinstance(m, HumanMessage) else \"assistant\", \"content\": m.content}\n",
    "        for m in state[\"messages\"]\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    print(f\"\\nðŸ“š Retrieved {len(relevant_memories)} relevant memories\")\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=response.content)]}\n",
    "\n",
    "print(\"âœ… Memory-augmented agent defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d38f59f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agent compiled with checkpointer + store\n"
     ]
    }
   ],
   "source": [
    "# Build the graph with both checkpointer AND store\n",
    "def create_memory_agent():\n",
    "    graph = StateGraph(AgentState)\n",
    "    graph.add_node(\"agent\", memory_augmented_agent)\n",
    "    graph.add_edge(START, \"agent\")\n",
    "    graph.add_edge(\"agent\", END)\n",
    "    return graph\n",
    "\n",
    "# Create stores\n",
    "checkpointer = MemorySaver()  # Thread-scoped state\n",
    "\n",
    "# Reuse our semantic store (already populated with memories)\n",
    "long_term_store = semantic_store\n",
    "\n",
    "# Compile with BOTH\n",
    "graph = create_memory_agent()\n",
    "agent = graph.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    store=long_term_store\n",
    ")\n",
    "\n",
    "print(\"âœ… Agent compiled with checkpointer + store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "058cd5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEMO: Memory-Augmented Agent\n",
      "============================================================\n",
      "\n",
      "ðŸ’¬ User: 'What technology should I use for my project?'\n",
      "\n",
      "ðŸ“š Retrieved 3 relevant memories\n",
      "\n",
      "ðŸ¤– Agent: Since you're building a customer support automation system with AI agents and need real-time streaming for your customer-facing chatbot, the choice of technology is crucial. Based on your evaluation of LangGraph vs CrewAI, here are some tailored recommendations:\n",
      "\n",
      "1. **LangGraph**: If your project requires complex conversational flows and the ability to integrate multiple AI models seamlessly, LangGraph might be a better fit. It excels in managing intricate workflows and can be customized for advanced customer support scenarios.\n",
      "\n",
      "2. **CrewAI**: If your focus is on real-time streaming and responsiveness, CrewAI could be advantageous. It is designed for high-speed interactions and might be better suited for handling live customer queries efficiently.\n",
      "\n",
      "3. **Streaming Technology**: For real-time streaming, consider integrating WebSockets or server-sent events (SSE) into your chatbot infrastructure. These technologies ensure low-latency communication, which is essential for a smooth customer experience.\n",
      "\n",
      "4. **AI Model**: Depending on the complexity of your customer queries, you might want to use OpenAI's GPT models or similar large language models. These can be fine-tuned to handle specific support scenarios effectively.\n",
      "\n",
      "Ultimately, the choice between LangGraph and CrewAI depends on whether your priority is advanced conversational workflows or real-time responsiveness. If you'd like, I can help you dive deeper into the pros and cons of each platform for your specific needs.\n"
     ]
    }
   ],
   "source": [
    "# Run the memory-augmented agent\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMO: Memory-Augmented Agent\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"session_001\",  # Thread for checkpointer\n",
    "    }\n",
    "}\n",
    "\n",
    "# First query - should retrieve project-related memories\n",
    "print(\"\\nðŸ’¬ User: 'What technology should I use for my project?'\")\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What technology should I use for my project?\")],\n",
    "        \"user_id\": \"user_123\"  # Same user we stored memories for\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ¤– Agent: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2da183ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¬ User: 'Tell me more about my experience level'\n",
      "\n",
      "ðŸ“š Retrieved 3 relevant memories\n",
      "\n",
      "ðŸ¤– Agent: From what I know, you have 10 years of experience in enterprise software, which gives you a strong foundation in building scalable, robust systems. This level of expertise likely means you're familiar with designing and implementing complex architectures, integrating various technologies, and managing large-scale projects. \n",
      "\n",
      "Given your background, you're probably comfortable evaluating trade-offs between different tools and technologies, and you likely have a good sense of how to align technical decisions with business goals. This experience will be invaluable as you work on your customer-facing chatbot with real-time streaming and explore agentic AI or multi-agent systems. If you'd like, I can tailor my suggestions to align with your expertise in enterprise-grade solutions.\n"
     ]
    }
   ],
   "source": [
    "# Second query in same thread - should still have context\n",
    "print(\"\\nðŸ’¬ User: 'Tell me more about my experience level'\")\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Tell me more about my experience level\")],\n",
    "        \"user_id\": \"user_123\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ¤– Agent: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573b9bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Token-Level Streaming for UX\n",
    "\n",
    "### Why Streaming Matters\n",
    "\n",
    "Without streaming:\n",
    "```\n",
    "User: \"Explain LangGraph\"\n",
    "[3 seconds of waiting...]\n",
    "Agent: \"LangGraph is a framework for...\"\n",
    "```\n",
    "\n",
    "With streaming:\n",
    "```\n",
    "User: \"Explain LangGraph\"\n",
    "Agent: \"L|a|n|g|G|r|a|p|h| |i|s| |a| |f|r|a|m|e|w|o|r|k|...\"\n",
    "```\n",
    "\n",
    "### LangGraph Streaming Modes\n",
    "\n",
    "| Mode | What it streams | Use case |\n",
    "|------|-----------------|----------|\n",
    "| `values` | Full state after each step | Debugging |\n",
    "| `updates` | State deltas after each step | Progress tracking |\n",
    "| `messages` | LLM tokens + metadata | Chat UX |\n",
    "| `custom` | Your custom data | Progress indicators |\n",
    "| `debug` | Everything | Deep debugging |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e98b451c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Streaming demo graph created\n"
     ]
    }
   ],
   "source": [
    "# Demo 4: Token-Level Streaming\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import operator\n",
    "\n",
    "# Simple agent for streaming demo\n",
    "class StreamState(TypedDict):\n",
    "    messages: Annotated[list, operator.add]\n",
    "\n",
    "def streaming_agent(state: StreamState) -> dict:\n",
    "    \"\"\"Agent node - LLM call will be streamed automatically\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # The LLM invoke call is automatically streamed in \"messages\" mode\n",
    "    response = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Give detailed, thoughtful responses.\"},\n",
    "        *[{\"role\": \"user\" if isinstance(m, HumanMessage) else \"assistant\", \"content\": m.content} for m in messages]\n",
    "    ])\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=response.content)]}\n",
    "\n",
    "# Build simple graph\n",
    "stream_graph = StateGraph(StreamState)\n",
    "stream_graph.add_node(\"agent\", streaming_agent)\n",
    "stream_graph.add_edge(START, \"agent\")\n",
    "stream_graph.add_edge(\"agent\", END)\n",
    "stream_app = stream_graph.compile()\n",
    "\n",
    "print(\"âœ… Streaming demo graph created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f65098f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEMO: Token-Level Streaming (stream_mode='messages')\n",
      "============================================================\n",
      "\n",
      "ðŸ’¬ User: 'What are the benefits of agentic AI?'\n",
      "\n",
      "ðŸ¤– Agent: Agentic AI, which operates with a degree of autonomy to perform tasks or make decisions, offers several benefits:\n",
      "\n",
      "1. **Increased Efficiency**: Automates complex, repetitive, or time-consuming tasks, freeing up human resources for higher-level work.\n",
      "2. **Scalability**: Handles large-scale operations or data processing beyond human capacity, enabling businesses to grow and adapt quickly.\n",
      "3. **24/7 Operation**: Functions continuously without fatigue, ensuring consistent performance and availability.\n",
      "4. **Improved Decision-Making**: Analyzes vast amounts of data to provide insights, make predictions, or optimize outcomes in real-time.\n",
      "5. **Personalization**: Tailors services or products to individual needs, enhancing user experiences (e.g., in customer service or healthcare).\n",
      "6. **Problem-Solving**: Tackles complex challenges in areas like logistics, climate modeling, or scientific research with minimal human intervention.\n",
      "7. **Cost Savings**: Reduces labor costs and operational inefficiencies over time.\n",
      "\n",
      "However, these benefits must be balanced with ethical considerations, safety measures, and proper oversight to ensure responsible use.Agentic AI, which operates with a degree of autonomy to perform tasks or make decisions, offers several benefits:\n",
      "\n",
      "1. **Increased Efficiency**: Automates complex, repetitive, or time-consuming tasks, freeing up human resources for higher-level work.\n",
      "2. **Scalability**: Handles large-scale operations or data processing beyond human capacity, enabling businesses to grow and adapt quickly.\n",
      "3. **24/7 Operation**: Functions continuously without fatigue, ensuring consistent performance and availability.\n",
      "4. **Improved Decision-Making**: Analyzes vast amounts of data to provide insights, make predictions, or optimize outcomes in real-time.\n",
      "5. **Personalization**: Tailors services or products to individual needs, enhancing user experiences (e.g., in customer service or healthcare).\n",
      "6. **Problem-Solving**: Tackles complex challenges in areas like logistics, climate modeling, or scientific research with minimal human intervention.\n",
      "7. **Cost Savings**: Reduces labor costs and operational inefficiencies over time.\n",
      "\n",
      "However, these benefits must be balanced with ethical considerations, safety measures, and proper oversight to ensure responsible use.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Streaming Mode: \"messages\" - Token-by-token LLM output\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMO: Token-Level Streaming (stream_mode='messages')\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nðŸ’¬ User: 'What are the benefits of agentic AI?'\\n\")\n",
    "print(\"ðŸ¤– Agent: \", end=\"\", flush=True)\n",
    "\n",
    "for message_chunk, metadata in stream_app.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What are the benefits of agentic AI? Keep it brief.\")]},\n",
    "    stream_mode=\"messages\"\n",
    "):\n",
    "    # message_chunk contains the token, metadata has node info\n",
    "    if message_chunk.content:\n",
    "        print(message_chunk.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\")  # Final newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8cece0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEMO: Step-Level Streaming (stream_mode='updates')\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Watching state updates as graph executes:\n",
      "\n",
      "ðŸ“ Node 'agent' completed\n",
      "   State update keys: ['messages']\n",
      "   Last message: Hello! ðŸ˜Š How can I assist you today?...\n"
     ]
    }
   ],
   "source": [
    "# Streaming Mode: \"updates\" - State deltas after each step\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMO: Step-Level Streaming (stream_mode='updates')\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nðŸ“Š Watching state updates as graph executes:\\n\")\n",
    "\n",
    "for update in stream_app.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Hello!\")]},\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    for node_name, state_update in update.items():\n",
    "        print(f\"ðŸ“ Node '{node_name}' completed\")\n",
    "        print(f\"   State update keys: {list(state_update.keys())}\")\n",
    "        if \"messages\" in state_update:\n",
    "            last_msg = state_update[\"messages\"][-1]\n",
    "            print(f\"   Last message: {last_msg.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659395cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Custom Streaming with get_stream_writer\n",
    "\n",
    "### Use Case: Progress Updates from Tools\n",
    "\n",
    "When your agent performs long-running operations (API calls, file processing), you can stream custom progress updates:\n",
    "\n",
    "```python\n",
    "from langgraph.config import get_stream_writer\n",
    "\n",
    "@tool\n",
    "def analyze_documents(files: list[str]) -> str:\n",
    "    writer = get_stream_writer()\n",
    "    \n",
    "    writer(\"Starting document analysis...\")\n",
    "    for i, file in enumerate(files):\n",
    "        writer(f\"Processing {file} ({i+1}/{len(files)})\")\n",
    "        # ... actual processing ...\n",
    "    writer(\"Analysis complete!\")\n",
    "    \n",
    "    return \"Results: ...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5872a9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Custom streaming demo ready\n"
     ]
    }
   ],
   "source": [
    "# Demo 5: Custom Streaming with Progress Updates\n",
    "from langgraph.config import get_stream_writer\n",
    "import time\n",
    "\n",
    "class AnalysisState(TypedDict):\n",
    "    query: str\n",
    "    result: str\n",
    "\n",
    "def analysis_node(state: AnalysisState) -> dict:\n",
    "    \"\"\"Node that streams custom progress updates\"\"\"\n",
    "    writer = get_stream_writer()\n",
    "    \n",
    "    # Stream progress updates\n",
    "    writer({\"type\": \"progress\", \"message\": \"Starting analysis...\", \"percent\": 0})\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    writer({\"type\": \"progress\", \"message\": \"Gathering data...\", \"percent\": 25})\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    writer({\"type\": \"progress\", \"message\": \"Processing...\", \"percent\": 50})\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    writer({\"type\": \"progress\", \"message\": \"Generating insights...\", \"percent\": 75})\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    # Final result\n",
    "    result = f\"Analysis complete for: {state['query']}\"\n",
    "    \n",
    "    writer({\"type\": \"progress\", \"message\": \"Done!\", \"percent\": 100})\n",
    "    \n",
    "    return {\"result\": result}\n",
    "\n",
    "# Build graph\n",
    "analysis_graph = StateGraph(AnalysisState)\n",
    "analysis_graph.add_node(\"analyze\", analysis_node)\n",
    "analysis_graph.add_edge(START, \"analyze\")\n",
    "analysis_graph.add_edge(\"analyze\", END)\n",
    "analysis_app = analysis_graph.compile()\n",
    "\n",
    "print(\"âœ… Custom streaming demo ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d6e7ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEMO: Custom Progress Streaming\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Streaming progress updates:\n",
      "\n",
      "   [  0%] Starting analysis...\n",
      "   [ 25%] Gathering data...\n",
      "   [ 50%] Processing...\n",
      "   [ 75%] Generating insights...\n",
      "   [100%] Done!\n",
      "\n",
      "âœ… Final result: Analysis complete for: enterprise AI adoption trends\n"
     ]
    }
   ],
   "source": [
    "# Run with custom streaming\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMO: Custom Progress Streaming\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nðŸ“Š Streaming progress updates:\\n\")\n",
    "\n",
    "for stream_mode, chunk in analysis_app.stream(\n",
    "    {\"query\": \"enterprise AI adoption trends\"},\n",
    "    stream_mode=[\"updates\", \"custom\"]\n",
    "):\n",
    "    if stream_mode == \"custom\":\n",
    "        if isinstance(chunk, dict) and chunk.get(\"type\") == \"progress\":\n",
    "            print(f\"   [{chunk['percent']:3d}%] {chunk['message']}\")\n",
    "    elif stream_mode == \"updates\":\n",
    "        if \"result\" in chunk.get(\"analyze\", {}):\n",
    "            print(f\"\\nâœ… Final result: {chunk['analyze']['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c70789",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Production Considerations\n",
    "\n",
    "### Memory Architecture Best Practices\n",
    "\n",
    "| Aspect | Development | Production |\n",
    "|--------|-------------|------------|\n",
    "| **Checkpointer** | `MemorySaver` | `PostgresSaver`, `SqliteSaver` |\n",
    "| **Store** | `InMemoryStore` | `PostgresStore` |\n",
    "| **Embeddings** | Mock/local | `text-embedding-3-small` |\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "**Short-term Memory (Checkpointer)**:\n",
    "- Current conversation context\n",
    "- Undo/redo (time-travel)\n",
    "- Fault recovery mid-conversation\n",
    "\n",
    "**Long-term Memory (Store)**:\n",
    "- User preferences\n",
    "- Facts learned across sessions\n",
    "- Shared knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc470733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REFERENCE: Production Checkpointer Setup\n",
      "============================================================\n",
      "\n",
      "# PostgresSaver for production persistence\n",
      "from langgraph.checkpoint.postgres import PostgresSaver\n",
      "import psycopg\n",
      "\n",
      "# Connection string\n",
      "DB_URI = \"postgresql://user:pass@host:5432/langgraph\"\n",
      "\n",
      "# Sync version\n",
      "with psycopg.Connection.connect(DB_URI) as conn:\n",
      "    checkpointer = PostgresSaver(conn)\n",
      "    checkpointer.setup()  # Creates necessary tables\n",
      "\n",
      "# Async version (for async graphs)\n",
      "from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n",
      "import psycopg_pool\n",
      "\n",
      "pool = psycopg_pool.AsyncConnectionPool(conninfo=DB_URI)\n",
      "checkpointer = AsyncPostgresSaver(pool)\n",
      "await checkpointer.setup()\n",
      "\n",
      "# Compile graph with production checkpointer\n",
      "graph = builder.compile(checkpointer=checkpointer)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Production Example: PostgresSaver setup (code reference only)\n",
    "print(\"=\" * 60)\n",
    "print(\"REFERENCE: Production Checkpointer Setup\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "production_code = '''\n",
    "# PostgresSaver for production persistence\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "import psycopg\n",
    "\n",
    "# Connection string\n",
    "DB_URI = \"postgresql://user:pass@host:5432/langgraph\"\n",
    "\n",
    "# Sync version\n",
    "with psycopg.Connection.connect(DB_URI) as conn:\n",
    "    checkpointer = PostgresSaver(conn)\n",
    "    checkpointer.setup()  # Creates necessary tables\n",
    "\n",
    "# Async version (for async graphs)\n",
    "from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n",
    "import psycopg_pool\n",
    "\n",
    "pool = psycopg_pool.AsyncConnectionPool(conninfo=DB_URI)\n",
    "checkpointer = AsyncPostgresSaver(pool)\n",
    "await checkpointer.setup()\n",
    "\n",
    "# Compile graph with production checkpointer\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "'''\n",
    "\n",
    "print(production_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844e50c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Memory Architecture\n",
    "âœ… **Store API** provides production-grade long-term memory\n",
    "- `InMemoryStore` for development, `PostgresStore` for production\n",
    "- Namespaced storage for multi-tenant isolation\n",
    "- `put()`, `get()`, `search()` operations\n",
    "\n",
    "âœ… **Semantic Search** enables intelligent memory retrieval\n",
    "- Configure with embedding function and dimensions\n",
    "- `search(namespace, query=..., filter=..., limit=...)` finds relevant memories\n",
    "- Combine with filters for hybrid search\n",
    "\n",
    "âœ… **Checkpointer + Store** work together\n",
    "- Checkpointer: Thread-scoped state snapshots\n",
    "- Store: Cross-thread long-term memories\n",
    "- Both passed to `compile(checkpointer=..., store=...)`\n",
    "\n",
    "### Streaming\n",
    "âœ… **Multiple streaming modes** for different needs\n",
    "- `messages`: Token-by-token LLM output for chat UX\n",
    "- `updates`: State deltas for progress tracking\n",
    "- `custom`: Your own progress/status updates\n",
    "- Combine modes with `stream_mode=[\"messages\", \"custom\"]`\n",
    "\n",
    "âœ… **Custom streaming** with `get_stream_writer()`\n",
    "- Stream progress from any node\n",
    "- Great for long-running operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d2bfe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **Module 2: Advanced HITL & Durability** where you'll learn:\n",
    "- Dynamic `interrupt()` vs static `interrupt_before`\n",
    "- `Command(resume=...)` for structured human input\n",
    "- `@task` decorator for durable execution\n",
    "- `RetryPolicy` for fault tolerance\n",
    "- HANDS-ON: Expense approval workflow\n",
    "\n",
    "Open: `module-2-advanced-hitl.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c0c9d-2e78-429e-9f0a-f15c0e24ea9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
