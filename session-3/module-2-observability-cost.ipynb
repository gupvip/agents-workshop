{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991e3a3b",
   "metadata": {},
   "source": [
    "# Session 3, Module 2: Observability & Cost Optimization\n",
    "\n",
    "---\n",
    "\n",
    "## Module Overview\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                                    â”‚\n",
    "â”‚  ğŸ“š Session 3, Module 2: Observability & Cost Optimization         â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚  ğŸ¯ Learning Objectives:                                           â”‚\n",
    "â”‚     â€¢ Understand the observability landscape (LangSmithvs Langfuse)â”‚\n",
    "â”‚     â€¢ Set up Langfuse for production tracing                       â”‚\n",
    "â”‚     â€¢ Integrate observability into incident-postmortem project     â”‚\n",
    "â”‚     â€¢ Learn cost optimization strategies (theory)                  â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b65fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking LLM Configuration...\n",
      "==================================================\n",
      "ğŸ“¡ Provider: DIAL (Azure OpenAI via EPAM AI Proxy)\n",
      "âœ… DIAL_API_KEY is set\n",
      "\n",
      "ğŸ“‹ Configuration:\n",
      "   AZURE_OPENAI_ENDPOINT: https://ai-proxy.lab.epam.com\n",
      "   AZURE_OPENAI_API_VERSION: 2024-08-01-preview\n",
      "   AZURE_OPENAI_DEPLOYMENT_NAME: gpt-4\n",
      "\n",
      "âœ… DIAL setup verified successfully!\n",
      "\n",
      "ğŸ“š Session 3, Module 2: Observability & Cost Optimization\n",
      "============================================================\n",
      "\n",
      "ğŸ”— Prerequisites:\n",
      "   â€¢ Module 1 complete (Evaluation Frameworks)\n",
      "   â€¢ OpenAI API key configured\n",
      "\n",
      "ğŸ¯ What you'll learn:\n",
      "   â€¢ When to use LangSmith vs Langfuse\n",
      "   â€¢ How to set up Langfuse for production tracing\n",
      "   â€¢ Cost optimization strategies for LLM applications\n"
     ]
    }
   ],
   "source": [
    "# Workshop Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from setup_llm import verify_setup, get_chat_model\n",
    "\n",
    "verify_setup()\n",
    "\n",
    "print(\"\\nğŸ“š Session 3, Module 2: Observability & Cost Optimization\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nğŸ”— Prerequisites:\")\n",
    "print(\"   â€¢ Module 1 complete (Evaluation Frameworks)\")\n",
    "print(\"   â€¢ OpenAI API key configured\")\n",
    "print(\"\\nğŸ¯ What you'll learn:\")\n",
    "print(\"   â€¢ When to use LangSmith vs Langfuse\")\n",
    "print(\"   â€¢ How to set up Langfuse for production tracing\")\n",
    "print(\"   â€¢ Cost optimization strategies for LLM applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9daf9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Why Observability Matters\n",
    "\n",
    "### The Visibility Problem\n",
    "\n",
    "LLM applications are inherently **black boxes**. Without observability:\n",
    "\n",
    "| Problem | Impact |\n",
    "|---------|--------|\n",
    "| **No Debugging** | \"It worked yesterday, why did it fail today?\" |\n",
    "| **No Cost Insight** | $10K bill surprise at month end |\n",
    "| **No Performance Data** | \"Is my agent getting slower?\" |\n",
    "| **No Quality Tracking** | \"Are responses getting worse over time?\" |\n",
    "\n",
    "### What We Need to Observe\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    LLM OBSERVABILITY LAYERS                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚ TRACE (End-to-end request)                                      â”‚    â”‚\n",
    "â”‚  â”‚                                                                 â”‚    â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚\n",
    "â”‚  â”‚  â”‚ SPAN: Agent                                               â”‚  â”‚    â”‚\n",
    "â”‚  â”‚  â”‚                                                           â”‚  â”‚    â”‚\n",
    "â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚  â”‚    â”‚\n",
    "â”‚  â”‚  â”‚  â”‚ GENERATION â”‚  â”‚ GENERATION â”‚  â”‚ GENERATION â”‚           â”‚  â”‚    â”‚\n",
    "â”‚  â”‚  â”‚  â”‚ LLM Call 1 â”‚  â”‚ LLM Call 2 â”‚  â”‚ LLM Call 3 â”‚           â”‚  â”‚    â”‚\n",
    "â”‚  â”‚  â”‚  â”‚            â”‚  â”‚            â”‚  â”‚            â”‚           â”‚  â”‚    â”‚\n",
    "â”‚  â”‚  â”‚  â”‚ â€¢ Prompt   â”‚  â”‚ â€¢ Prompt   â”‚  â”‚ â€¢ Prompt   â”‚           â”‚  â”‚    â”‚\n",
    "â”‚  â”‚  â”‚  â”‚ â€¢ Response â”‚  â”‚ â€¢ Response â”‚  â”‚ â€¢ Response â”‚           â”‚  â”‚    â”‚\n",
    "â”‚  â”‚  â”‚  â”‚ â€¢ Tokens   â”‚  â”‚ â€¢ Tokens   â”‚  â”‚ â€¢ Tokens   â”‚           â”‚  â”‚    â”‚\n",
    "â”‚  â”‚  â”‚  â”‚ â€¢ Latency  â”‚  â”‚ â€¢ Latency  â”‚  â”‚ â€¢ Latency  â”‚           â”‚  â”‚    â”‚\n",
    "â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  â”‚    â”‚\n",
    "â”‚  â”‚  â”‚                                                           â”‚  â”‚    â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚\n",
    "â”‚  â”‚                                                                 â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  Key Metrics at Each Level:                                             â”‚\n",
    "â”‚  â€¢ Trace: Total latency, total cost, success/failure                    â”‚\n",
    "â”‚  â€¢ Span: Agent step duration, tool calls, state changes                 â”‚\n",
    "â”‚  â€¢ Generation: Token count, model used, prompt/response content         â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd62b014",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: LangSmith vs Langfuse Comparison\n",
    "\n",
    "**Lets take a look at available options:**\n",
    "\n",
    "| Tool | Provider | Deployment | Best For | Open Source | Cost Tracking | Self-Hosted |\n",
    "|------|----------|------------|----------|-------------|---------------|-------------|\n",
    "| **LangSmith** | LangChain | Cloud only | LangChain/LangGraph users | âŒ | âš ï¸ Limited | âŒ |\n",
    "| **Langfuse** | Community | Cloud + Self-hosted | Framework-agnostic, privacy | âœ… | âœ… Built-in | âœ… |\n",
    "| **Arize Phoenix** | Arize AI | Cloud + Self-hosted | ML teams, RAG analysis | âœ… | âœ… | âœ… |\n",
    "| **Helicone** | Helicone | Cloud + Self-hosted | Cost optimization | âœ… | âœ… Excellent | âœ… |\n",
    "| **OpenLLMetry** | Traceloop | Self-hosted | OpenTelemetry native, any backend | âœ… | âœ… | âœ… |\n",
    "| **Lunary** | Lunary | Cloud + Self-hosted | OSS LangSmith alternative | âœ… | âœ… | âœ… |\n",
    "| **OpenLIT** | OpenLIT | Self-hosted | OTel + GPU monitoring | âœ… | âœ… | âœ… |\n",
    "\n",
    "### The Two Major Players\n",
    "\n",
    "| Aspect | **LangSmith** | **Langfuse** |\n",
    "|--------|---------------|--------------|\n",
    "| **Provider** | LangChain (commercial) | Open-source community |\n",
    "| **Best For** | LangChain/LangGraph native | Any LLM framework, privacy-focused |\n",
    "| **Deployment** | Cloud-only | Cloud OR self-hosted |\n",
    "| **Pricing** | Free tier â†’ paid | Free OSS, paid cloud |\n",
    "| **Integration** | Automatic w/ LangChain | Manual setup (easy) |\n",
    "| **Unique Features** | Prompt hub, native datasets | Cost tracking, public analytics |\n",
    "\n",
    "### When to Choose Which?\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    DECISION TREE                                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  Are you using LangChain/LangGraph?                                     â”‚\n",
    "â”‚      â”‚                                                                  â”‚\n",
    "â”‚      â”œâ”€ YES â†’ Do you need self-hosting or cost tracking?                â”‚\n",
    "â”‚      â”‚            â”‚                                                     â”‚\n",
    "â”‚      â”‚            â”œâ”€ YES â†’ Langfuse âœ“                                   â”‚\n",
    "â”‚      â”‚            â”‚                                                     â”‚\n",
    "â”‚      â”‚            â””â”€ NO â†’ LangSmith âœ“ (tightest integration)            â”‚\n",
    "â”‚      â”‚                                                                  â”‚\n",
    "â”‚      â””â”€ NO â†’ Do you need open-source/self-hosted?                       â”‚\n",
    "â”‚                   â”‚                                                     â”‚\n",
    "â”‚                   â”œâ”€ YES â†’ Langfuse âœ“                                   â”‚\n",
    "â”‚                   â”‚                                                     â”‚\n",
    "â”‚                   â””â”€ NO â†’ Either works, Langfuse has broader support    â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Feature Comparison Table\n",
    "\n",
    "| Feature | LangSmith | Langfuse |\n",
    "|---------|-----------|----------|\n",
    "| **Tracing** | âœ… Automatic (LangChain) | âœ… Callback handler |\n",
    "| **Cost Tracking** | âŒ Limited | âœ… Built-in |\n",
    "| **Self-Hosted** | âŒ No | âœ… Docker/K8s |\n",
    "| **Prompt Management** | âœ… Prompt Hub | âœ… Prompt versioning |\n",
    "| **Evaluation** | âœ… Native | âœ… External (DeepEval, etc.) |\n",
    "| **User Feedback** | âœ… Thumbs up/down | âœ… Custom scores |\n",
    "| **Session Tracking** | âœ… Thread-based | âœ… Session IDs |\n",
    "| **Multi-model** | âœ… Any (via LangChain) | âœ… Any model |\n",
    "| **SOC2/HIPAA** | âœ… Enterprise | âœ… Self-host for compliance |\n",
    "\n",
    "### Our Choice: Langfuse\n",
    "\n",
    "For this workshop, we'll use **Langfuse** because:\n",
    "\n",
    "1. **Open-source** - No vendor lock-in, can self-host\n",
    "2. **Cost tracking** - Built-in token cost calculation\n",
    "3. **Framework agnostic** - Works with any LLM setup\n",
    "4. **Privacy-friendly** - Self-host for sensitive data\n",
    "5. **LangGraph compatible** - Easy CallbackHandler integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c8ab3f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Langfuse Setup & Demo\n",
    "\n",
    "### Getting Started with Langfuse\n",
    "\n",
    "#### Option 1: Langfuse Cloud (Recommended for Workshop)\n",
    "\n",
    "1. Sign up at [langfuse.com](https://cloud.langfuse.com)\n",
    "2. Create a new project\n",
    "3. Copy your API keys\n",
    "\n",
    "#### Option 2: Self-Hosted (Production)\n",
    "\n",
    "```bash\n",
    "# Docker Compose\n",
    "git clone https://github.com/langfuse/langfuse.git\n",
    "cd langfuse\n",
    "docker-compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0dc681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse Installation & Setup\n",
      "============================================================\n",
      "\n",
      "To install Langfuse:\n",
      "\n",
      "    pip install langfuse langchain-openai\n",
      "\n",
      "Set environment variables:\n",
      "\n",
      "    export LANGFUSE_SECRET_KEY=\"sk-lf-...\"\n",
      "    export LANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\n",
      "    export LANGFUSE_HOST=\"https://cloud.langfuse.com\"  # or self-hosted URL\n",
      "\n",
      "Or set in Python:\n",
      "\n",
      "    import os\n",
      "    os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
      "    os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install Langfuse\n",
    "# !pip install langfuse langchain-openai\n",
    "\n",
    "print(\"Langfuse Installation & Setup\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "To install Langfuse:\n",
    "\n",
    "    pip install langfuse langchain-openai\n",
    "\n",
    "Set environment variables:\n",
    "\n",
    "    export LANGFUSE_SECRET_KEY=\"sk-lf-...\"\n",
    "    export LANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\n",
    "    export LANGFUSE_HOST=\"https://cloud.langfuse.com\"  # or self-hosted URL\n",
    "\n",
    "Or set in Python:\n",
    "\n",
    "    import os\n",
    "    os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
    "    os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd4ee3",
   "metadata": {},
   "source": [
    "# Basic Langfuse Integration with LangChain/LangGraph\n",
    "\n",
    "```python\n",
    "\n",
    "from langfuse.callback import CallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create Langfuse callback handler\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# Use with LangChain\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "response = llm.invoke(\n",
    "    \"What is the capital of France?\",\n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    ")\n",
    "\n",
    "# Traces automatically appear in Langfuse dashboard!\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbc576d",
   "metadata": {},
   "source": [
    "# Langfuse with LangGraph Integration\n",
    "\n",
    "```python\n",
    "from langfuse.callback import CallbackHandler\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# Create handler with trace metadata\n",
    "langfuse_handler = CallbackHandler(\n",
    "    user_id=\"user-123\",           # Track by user\n",
    "    session_id=\"session-abc\",     # Group related traces\n",
    "    tags=[\"production\", \"v1.2\"]   # Custom tags for filtering\n",
    ")\n",
    "\n",
    "# Pass to LangGraph invoke\n",
    "graph = create_postmortem_graph()  # Your LangGraph workflow\n",
    "result = graph.invoke(\n",
    "    initial_state,\n",
    "    config={\n",
    "        \"callbacks\": [langfuse_handler],\n",
    "        \"run_name\": \"incident-postmortem\"  # Name in dashboard\n",
    "    }\n",
    ")\n",
    "\n",
    "# All nodes, LLM calls, and tool invocations are traced!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a860626c",
   "metadata": {},
   "source": [
    "# Integrating Langfuse with Incident-PostMortem Project\n",
    "\n",
    "```python\n",
    "\"\"\"Main entry point with Langfuse observability.\"\"\"\n",
    "\n",
    "import os\n",
    "from langfuse.callback import CallbackHandler\n",
    "from graph.workflow import create_postmortem_graph\n",
    "from graph.state import IncidentState\n",
    "\n",
    "\n",
    "def create_langfuse_handler(incident_id: str, severity: str) -> CallbackHandler:\n",
    "    \"\"\"Create a configured Langfuse handler for incident analysis.\"\"\"\n",
    "    return CallbackHandler(\n",
    "        session_id=f\"incident-{incident_id}\",\n",
    "        user_id=os.getenv(\"ONCALL_USER\", \"system\"),\n",
    "        tags=[\n",
    "            f\"severity:{severity}\",\n",
    "            \"incident-postmortem\",\n",
    "            os.getenv(\"ENVIRONMENT\", \"development\")\n",
    "        ],\n",
    "        metadata={\n",
    "            \"incident_id\": incident_id,\n",
    "            \"severity\": severity,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def analyze_incident(incident_data: dict) -> dict:\n",
    "    \"\"\"Run the incident postmortem workflow with tracing.\"\"\"\n",
    "    \n",
    "    # Create Langfuse handler\n",
    "    langfuse_handler = create_langfuse_handler(\n",
    "        incident_id=incident_data.get(\"incident_id\", \"unknown\"),\n",
    "        severity=incident_data.get(\"severity\", \"unknown\")\n",
    "    )\n",
    "    \n",
    "    # Create graph\n",
    "    graph = create_postmortem_graph()\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = IncidentState(\n",
    "        incident_id=incident_data[\"incident_id\"],\n",
    "        severity=incident_data[\"severity\"],\n",
    "        logs=incident_data[\"logs\"],\n",
    "        timeline=[]\n",
    "    )\n",
    "    \n",
    "    # Run with Langfuse tracing\n",
    "    result = graph.invoke(\n",
    "        initial_state,\n",
    "        config={\n",
    "            \"callbacks\": [langfuse_handler],\n",
    "            \"run_name\": f\"postmortem-{incident_data['incident_id']}\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Flush traces before returning\n",
    "    langfuse_handler.flush()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    incident = {\n",
    "        \"incident_id\": \"INC-2024-001\",\n",
    "        \"severity\": \"SEV2\",\n",
    "        \"logs\": \"... log data ...\"\n",
    "    }\n",
    "    result = analyze_incident(incident)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddacd29",
   "metadata": {},
   "source": [
    "# Advanced: Custom Spans with @observe Decorator\n",
    "\n",
    "```python\n",
    "from langfuse.decorators import observe, langfuse_context\n",
    "\n",
    "@observe()\n",
    "def analyze_logs(logs: str) -> dict:\n",
    "    \"\"\"Custom function with automatic tracing.\"\"\"\n",
    "    \n",
    "    # Add custom metadata to the span\n",
    "    langfuse_context.update_current_observation(\n",
    "        metadata={\"log_lines\": len(logs.split(\"\\\\n\"))},\n",
    "        tags=[\"log-analysis\"]\n",
    "    )\n",
    "    \n",
    "    # Your analysis logic here\n",
    "    patterns = extract_patterns(logs)\n",
    "    \n",
    "    # Score the span (for evaluation)\n",
    "    langfuse_context.score_current_observation(\n",
    "        name=\"pattern_coverage\",\n",
    "        value=len(patterns) / 10,  # Normalized score\n",
    "        comment=\"Number of patterns found\"\n",
    "    )\n",
    "    \n",
    "    return {\"patterns\": patterns}\n",
    "\n",
    "\n",
    "@observe(name=\"root-cause-analysis\")\n",
    "def identify_root_cause(patterns: list, context: str) -> str:\n",
    "    \"\"\"Root cause analysis with custom span name.\"\"\"\n",
    "    \n",
    "    # Nested function calls create child spans\n",
    "    root_cause = llm_analyze(patterns, context)\n",
    "    \n",
    "    return root_cause\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92039a58",
   "metadata": {},
   "source": [
    "### What You'll See in Langfuse Dashboard\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     LANGFUSE TRACE VIEW                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  Trace: postmortem-INC-2024-001                                         â”‚\n",
    "â”‚  â”œâ”€â”€ Session: incident-INC-2024-001                                     â”‚\n",
    "â”‚  â”œâ”€â”€ User: oncall@company.com                                           â”‚\n",
    "â”‚  â”œâ”€â”€ Tags: severity:SEV2, incident-postmortem, production               â”‚\n",
    "â”‚  â”‚                                                                      â”‚\n",
    "â”‚  â”‚  Timeline:                                                           â”‚\n",
    "â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚\n",
    "â”‚  â”‚                                                                      â”‚\n",
    "â”‚  â”‚  [0ms]    â”Œâ”€ log_analyzer_node â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚  â”‚           â”‚  â”œâ”€ ChatOpenAI (gpt-4o)              â”‚                   â”‚\n",
    "â”‚  â”‚           â”‚  â”‚  Input: \"Analyze these logs...\"   â”‚                   â”‚\n",
    "â”‚  â”‚           â”‚  â”‚  Output: \"Pattern: DB timeout...\" â”‚                   â”‚\n",
    "â”‚  â”‚           â”‚  â”‚  Tokens: 450 in, 230 out          â”‚                   â”‚\n",
    "â”‚  â”‚           â”‚  â”‚  Cost: $0.0068                    â”‚                   â”‚\n",
    "â”‚  â”‚  [2.1s]   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚  â”‚                                                                      â”‚\n",
    "â”‚  â”‚  [2.1s]   â”Œâ”€ root_cause_node â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚  â”‚           â”‚  â”œâ”€ ChatOpenAI (gpt-4o)              â”‚                   â”‚\n",
    "â”‚  â”‚           â”‚  â”‚  Tokens: 380 in, 180 out          â”‚                   â”‚\n",
    "â”‚  â”‚           â”‚  â”‚  Cost: $0.0054                    â”‚                   â”‚\n",
    "â”‚  â”‚  [3.8s]   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚  â”‚                                                                      â”‚\n",
    "â”‚  â”‚  [3.8s]   â”Œâ”€ writer_node â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚  â”‚           â”‚  Cost: $0.0089                       â”‚                   â”‚\n",
    "â”‚  â”‚  [5.2s]   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚  â”‚                                                                      â”‚\n",
    "â”‚  â”‚  [5.2s]   â”Œâ”€ reviewer_node â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚  â”‚           â”‚  Result: REVISION_NEEDED             â”‚                   â”‚\n",
    "â”‚  â”‚  [6.8s]   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚  â”‚                                                                      â”‚\n",
    "â”‚  â”‚  [6.8s]   â”Œâ”€ writer_node (retry) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚  â”‚           â”‚  Cost: $0.0102                       â”‚                   â”‚\n",
    "â”‚  â”‚  [8.5s]   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚  â”‚                                                                      â”‚\n",
    "â”‚  â”‚  Total: 8.5s | Cost: $0.0423 | Tokens: 2,340                         â”‚\n",
    "â”‚  â”‚                                                                      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”˜\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7735f1f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Cost Optimization Strategies\n",
    "\n",
    "### The Cost Challenge: Why This Matters\n",
    "\n",
    "LLM costs can spiral quickly, especially with agentic workflows. Let's do the math:\n",
    "\n",
    "**Example: Incident Postmortem System**\n",
    "```\n",
    "Single postmortem run:\n",
    "â”œâ”€â”€ Log Analyzer:     ~800 input + 400 output tokens\n",
    "â”œâ”€â”€ Root Cause:       ~600 input + 300 output tokens  \n",
    "â”œâ”€â”€ Writer:           ~1000 input + 1500 output tokens\n",
    "â”œâ”€â”€ Reviewer:         ~1800 input + 400 output tokens\n",
    "â”œâ”€â”€ Writer (retry):   ~2200 input + 1600 output tokens  â† Self-reflection loop!\n",
    "â””â”€â”€ Total:            ~6,400 input + 4,200 output tokens\n",
    "\n",
    "With GPT-4o ($2.50/1M input, $10/1M output):\n",
    "â”œâ”€â”€ Input cost:  6,400 Ã— $2.50/1M = $0.016\n",
    "â”œâ”€â”€ Output cost: 4,200 Ã— $10/1M  = $0.042\n",
    "â””â”€â”€ Total per run: ~$0.06\n",
    "\n",
    "At scale:\n",
    "â”œâ”€â”€ 100 incidents/day = $6/day\n",
    "â”œâ”€â”€ 1000 incidents/day = $60/day = $1,800/month\n",
    "â””â”€â”€ With 3 retries avg = $5,400/month ğŸ˜±\n",
    "```\n",
    "\n",
    "| Factor | Impact | Real Example |\n",
    "|--------|--------|--------------|\n",
    "| **Multi-step reasoning** | Each agent step = LLM call | 5 nodes Ã— $0.01 = $0.05/request |\n",
    "| **Self-reflection loops** | Writer â†” Reviewer iterates | 3 iterations = 3Ã— cost |\n",
    "| **Tool retry logic** | Failed tools get retried | Network errors â†’ 2Ã— calls |\n",
    "| **Long context** | Full conversation history | 10K tokens â†’ $0.025 input alone |\n",
    "| **Embedding costs** | RAG retrieval | $0.0001/1K tokens (often overlooked) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf8f9c",
   "metadata": {},
   "source": [
    "### 2025 LLM Pricing Reference\n",
    "\n",
    "## OpenAI Models\n",
    "\n",
    "| Model             | Input (per 1M tokens) | Output (per 1M tokens) | Best For |\n",
    "|-------------------|------------------------|--------------------------|----------|\n",
    "| **GPT-4o**        | $2.50                  | $10.00                   | Complex reasoning, multimodal tasks |\n",
    "| **GPT-4o-mini**   | $0.15                  | $0.60                    | Lightweight tasks, classification |\n",
    "| **GPT-4.1 mini**  | $0.40                  | $1.60                    | Lower-cost balanced model |\n",
    "| **GPT-4.1**       | $2.00                  | $8.00                    | Balanced long-context model |\n",
    "\n",
    "---\n",
    "\n",
    "## Anthropic Claude Models\n",
    "\n",
    "| Model                 | Input (per 1M tokens) | Output (per 1M tokens) | Best For |\n",
    "|-----------------------|------------------------|--------------------------|----------|\n",
    "| **Claude Haiku 4.5**  | $1.00                  | $5.00                    | Fast, cost-efficient workloads |\n",
    "| **Claude Sonnet 4.5** | $3.00                  | $15.00                   | Balanced reasoning + coding |\n",
    "| **Claude Opus 4**     | $15.00                 | $75.00                   | Deep reasoning, premium workloads |\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Comparison (1M Input + 1M Output Tokens)\n",
    "\n",
    "GPT-4o-mini: $0.75\n",
    "Claude Haiku 4.5: $6.00\n",
    "GPT-4o: $12.50\n",
    "Claude Sonnet 4.5: $18.00\n",
    "Claude Opus 4: $90.00\n",
    "\n",
    "**Note:** Based on publicly available pricing from OpenAI and Anthropic (2025).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43b23c",
   "metadata": {},
   "source": [
    "### Cost Optimization Hierarchy\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     COST OPTIMIZATION PYRAMID                          â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  Start from the BOTTOM for biggest impact!                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚                            â”Œâ”€â”€â”€â”€â”€â”                                      â”‚\n",
    "â”‚                           /â”‚ 10% â”‚\\    4. Model Selection               â”‚\n",
    "â”‚                          / â”‚     â”‚ \\   (gpt-4o â†’ gpt-4o-mini)          â”‚\n",
    "â”‚                         /  â””â”€â”€â”€â”€â”€â”˜  \\  Easy but limited impact         â”‚\n",
    "â”‚                        /      â–²      \\                                  â”‚\n",
    "â”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
    "â”‚                      /â”‚      20%      â”‚\\   3. Prompt Engineering        â”‚\n",
    "â”‚                     / â”‚               â”‚ \\  (shorter, focused prompts)  â”‚\n",
    "â”‚                    /  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  \\                              â”‚\n",
    "â”‚                   /          â–²            \\                             â”‚\n",
    "â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚\n",
    "â”‚                 /â”‚          30%            â”‚\\  2. Caching               â”‚\n",
    "â”‚                / â”‚                         â”‚ \\ (semantic + exact)       â”‚\n",
    "â”‚               /  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  \\                         â”‚\n",
    "â”‚              /              â–²                  \\                        â”‚\n",
    "â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\n",
    "â”‚            â”‚               40%                   â”‚  1. Architecture     â”‚\n",
    "â”‚            â”‚                                     â”‚  (fewer LLM calls)   â”‚\n",
    "â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  âš¡ Pro tip: Combining all strategies can reduce costs by 60-80%       â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1876bf",
   "metadata": {},
   "source": [
    "### Strategy 1: Architecture Optimization (40% savings)\n",
    "\n",
    "**The Problem**: Developers often use LLMs for tasks that don't need them.\n",
    "\n",
    "**Before** - Every step uses an LLM:\n",
    "```\n",
    "User Query â†’ LLM (intent) â†’ LLM (extract) â†’ LLM (plan) â†’ LLM (execute) â†’ LLM (format)\n",
    "             â†‘               â†‘               â†‘            â†‘               â†‘\n",
    "           $0.01           $0.01           $0.02        $0.03           $0.01\n",
    "           \n",
    "Total: 5 LLM calls = $0.08 per query\n",
    "```\n",
    "\n",
    "**After** - Hybrid approach:\n",
    "```\n",
    "User Query â†’ Regex/Rules â†’ LLM (plan + execute) â†’ Template\n",
    "             â†‘              â†‘                      â†‘\n",
    "           FREE           $0.03                  FREE\n",
    "           \n",
    "Total: 1 LLM call = $0.03 per query (62% savings!)\n",
    "```\n",
    "\n",
    "#### Practical Examples\n",
    "\n",
    "| Step | LLM Approach | Optimized Approach | Savings |\n",
    "|------|--------------|-------------------|---------|\n",
    "| **Intent Detection** | \"Classify this query...\" | Keyword matching + rules | 100% |\n",
    "| **Entity Extraction** | \"Extract the date from...\" | Regex patterns | 100% |\n",
    "| **Data Validation** | \"Is this JSON valid?\" | JSON schema validator | 100% |\n",
    "| **Response Formatting** | \"Format this as markdown...\" | Jinja2 templates | 100% |\n",
    "| **Simple Routing** | \"Which agent handles this?\" | Decision tree | 100% |\n",
    "\n",
    "#### Real-World Pattern: Tiered Processing\n",
    "\n",
    "```python\n",
    "def process_query(query: str) -> str:\n",
    "    # Tier 1: Rule-based (FREE)\n",
    "    if is_greeting(query):\n",
    "        return random.choice(GREETING_RESPONSES)\n",
    "    \n",
    "    if is_faq(query):\n",
    "        return FAQ_DATABASE.get(query)\n",
    "    \n",
    "    # Tier 2: Simple LLM (CHEAP - gpt-4o-mini)\n",
    "    if is_simple_question(query):\n",
    "        return llm_mini.invoke(query)\n",
    "    \n",
    "    # Tier 3: Complex LLM (EXPENSIVE - gpt-4o) \n",
    "    return llm_full.invoke(query)\n",
    "```\n",
    "\n",
    "**Impact on Incident Postmortem**:\n",
    "- Log parsing â†’ Regex for timestamps/error codes (save $0.01)\n",
    "- Severity detection â†’ Keyword rules (save $0.01)\n",
    "- Report templating â†’ Jinja2 instead of LLM (save $0.02)\n",
    "- **Estimated savings: 35% per run**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b590299f",
   "metadata": {},
   "source": [
    "### Strategy 2: Caching (30% savings)\n",
    "\n",
    "Caching is your **best friend** for repeated or similar queries.\n",
    "\n",
    "- Anthropic: https://platform.claude.com/docs/en/build-with-claude/prompt-caching\n",
    "- OpenAI: https://platform.openai.com/docs/guides/prompt-caching\n",
    "\n",
    "#### Types of LLM Caching\n",
    "\n",
    "| Cache Type | How It Works | Hit Rate | Best For |\n",
    "|------------|--------------|----------|----------|\n",
    "| **Exact Match** | Hash prompt â†’ lookup | High (90%+) for FAQs | Repeated identical queries |\n",
    "| **Semantic Cache** | Embed prompt â†’ similarity search | Medium (60-80%) | Similar questions |\n",
    "| **Prompt Prefix Cache** | Cache long system prompts | Very High (95%+) | Same instructions |\n",
    "| **Response Cache** | Cache LLM outputs | High | Deterministic outputs |\n",
    "\n",
    "#### OpenAI's Automatic Prompt Caching\n",
    "\n",
    "OpenAI automatically caches prompt prefixes for **50% discount**:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    OPENAI PROMPT CACHING                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  System Prompt (1000 tokens):                                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚ \"You are an expert incident analyst. Your job is to analyze     â”‚    â”‚\n",
    "â”‚  â”‚  logs, identify patterns, determine root cause, and write       â”‚    â”‚\n",
    "â”‚  â”‚  comprehensive postmortem reports following our template...\"    â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  First Call:    1000 tokens Ã— $2.50/1M = $0.0025 (full price)           â”‚\n",
    "â”‚  Second Call:   1000 tokens Ã— $1.25/1M = $0.00125 (50% cached!)         â”‚\n",
    "â”‚  Third Call:    1000 tokens Ã— $1.25/1M = $0.00125 (50% cached!)         â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  Requirements:                                                          â”‚\n",
    "â”‚  â€¢ Prompts must be >1024 tokens                                         â”‚\n",
    "â”‚  â€¢ Identical prefix (character-by-character)                            â”‚\n",
    "â”‚  â€¢ Cache expires after 5-10 minutes of inactivity                       â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "#### Caching Strategy by Use Case\n",
    "\n",
    "| Use Case | Cache Strategy | Expected Savings |\n",
    "|----------|---------------|------------------|\n",
    "| **FAQ Bot** | Exact match + semantic | 70-90% |\n",
    "| **Code Review** | Semantic (similar code patterns) | 40-60% |\n",
    "| **Incident Analysis** | Prompt prefix (system prompt) | 20-30% |\n",
    "| **Report Generation** | Limited (unique inputs) | 10-20% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da5c93d",
   "metadata": {},
   "source": [
    "### Strategy 3: Prompt Engineering for Cost (20% savings)\n",
    "\n",
    "Every token counts! Optimizing prompts reduces both input AND output costs.\n",
    "\n",
    "#### Token Reduction Techniques\n",
    "\n",
    "| Technique | Before | After | Token Savings |\n",
    "|-----------|--------|-------|---------------|\n",
    "| **Remove fluff** | \"Please kindly provide a detailed summary of the following text, making sure to include all important points...\" | \"Summarize:\" | ~80% |\n",
    "| **Use abbreviations** | \"incident_identification_number\" | \"inc_id\" | ~60% |\n",
    "| **JSON over prose** | \"The error type is connection timeout and it occurred at 14:30\" | `{\"type\":\"timeout\",\"time\":\"14:30\"}` | ~40% |\n",
    "| **Structured output** | Free-form explanation | JSON schema response | ~30% |\n",
    "\n",
    "#### Real Example: Postmortem System Prompt\n",
    "\n",
    "**Before** (847 tokens):\n",
    "```\n",
    "You are an expert incident management analyst with years of experience in \n",
    "identifying root causes of system failures. Your task is to carefully analyze \n",
    "the provided incident logs and produce a comprehensive postmortem report. \n",
    "\n",
    "Please make sure to include the following sections in your report:\n",
    "1. Executive Summary - A brief overview of the incident\n",
    "2. Timeline - A chronological sequence of events\n",
    "3. Root Cause Analysis - Detailed investigation of what went wrong\n",
    "4. Impact Assessment - How the incident affected users and systems\n",
    "5. Action Items - Specific steps to prevent future occurrences\n",
    "\n",
    "Remember to maintain a blameless tone throughout your analysis...\n",
    "```\n",
    "\n",
    "**After** (312 tokens - 63% reduction!):\n",
    "```\n",
    "Role: Incident analyst\n",
    "Task: Analyze logs, write postmortem\n",
    "\n",
    "Output JSON:\n",
    "{\n",
    "  \"summary\": \"1-2 sentences\",\n",
    "  \"timeline\": [{\"time\": \"HH:MM\", \"event\": \"...\"}],\n",
    "  \"root_cause\": \"primary cause\",\n",
    "  \"impact\": {\"users\": N, \"duration\": \"Xh\"},\n",
    "  \"actions\": [{\"priority\": \"P0-P2\", \"action\": \"...\", \"owner\": \"...\"}]\n",
    "}\n",
    "\n",
    "Tone: Blameless, factual\n",
    "```\n",
    "\n",
    "#### Output Length Control\n",
    "\n",
    "```python\n",
    "# Technique 1: Max tokens parameter\n",
    "response = llm.invoke(\n",
    "    prompt,\n",
    "    max_tokens=500  # Hard limit on output\n",
    ")\n",
    "\n",
    "# Technique 2: Explicit length instructions\n",
    "prompt = \"\"\"\n",
    "Summarize in EXACTLY 3 bullet points (max 20 words each):\n",
    "{content}\n",
    "\"\"\"\n",
    "\n",
    "# Technique 3: Structured output with schema\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    main_point: str = Field(max_length=100)\n",
    "    details: list[str] = Field(max_items=3)\n",
    "```\n",
    "\n",
    "#### The 80/20 Rule for Prompts\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    PROMPT TOKEN DISTRIBUTION                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  Typical prompt breakdown:                                             â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  System instructions (60%)     â”‚\n",
    "â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  Few-shot examples (20%)       â”‚\n",
    "â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  Actual user input (10%)       â”‚\n",
    "â”‚  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  Output formatting (10%)       â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  Optimization priority:                                                 â”‚\n",
    "â”‚  1. System instructions â†’ Move to cached prefix                        â”‚\n",
    "â”‚  2. Few-shot examples â†’ Reduce to 1-2, or use fine-tuning              â”‚\n",
    "â”‚  3. Output formatting â†’ Use JSON schema instead of descriptions         â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68f629",
   "metadata": {},
   "source": [
    "### Strategy 4: Model Selection & Routing (10-50% savings)\n",
    "\n",
    "**The key insight**: Not all tasks need GPT-4o!\n",
    "\n",
    "#### Model Capability vs Cost Matrix\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    MODEL SELECTION GUIDE                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  Task Complexity                                                        â”‚\n",
    "â”‚       â–²                                                                 â”‚\n",
    "â”‚       â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
    "â”‚  High â”‚                        â”‚   GPT-4o    â”‚  Complex reasoning       â”‚\n",
    "â”‚       â”‚                        â”‚   Claude    â”‚  Multi-step analysis     â”‚\n",
    "â”‚       â”‚                        â”‚   Opus      â”‚  Novel problems          â”‚\n",
    "â”‚       â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
    "â”‚       â”‚         â”‚      GPT-4o-mini                                      â”‚\n",
    "â”‚       â”‚         â”‚      Claude Sonnet        â”‚  Structured tasks         â”‚\n",
    "â”‚  Med  â”‚         â”‚      Claude Haiku         â”‚  Standard Q&A             â”‚\n",
    "â”‚       â”‚         â”‚                           â”‚  Code completion          â”‚\n",
    "â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚\n",
    "â”‚       â”‚  â”‚   Rules / Templates / Regex                                  â”‚\n",
    "â”‚  Low  â”‚  â”‚   No LLM needed!                  â”‚  Classification          â”‚\n",
    "â”‚       â”‚  â”‚                                   â”‚  Formatting              â”‚\n",
    "â”‚       â””â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶   â”‚\n",
    "â”‚          Low              Medium              High                      â”‚\n",
    "â”‚                         Cost â†’                                          â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "#### Implementing a Model Router\n",
    "\n",
    "```python\n",
    "from enum import Enum\n",
    "from typing import Callable\n",
    "\n",
    "class TaskComplexity(Enum):\n",
    "    SIMPLE = \"simple\"      # Classification, extraction\n",
    "    MEDIUM = \"medium\"      # Summarization, Q&A\n",
    "    COMPLEX = \"complex\"    # Reasoning, analysis, creativity\n",
    "\n",
    "class ModelRouter:\n",
    "    \"\"\"Route tasks to appropriate models based on complexity.\"\"\"\n",
    "    \n",
    "    MODEL_MAP = {\n",
    "        TaskComplexity.SIMPLE: \"gpt-4o-mini\",   # $0.15/1M in\n",
    "        TaskComplexity.MEDIUM: \"gpt-4o-mini\",   # $0.15/1M in\n",
    "        TaskComplexity.COMPLEX: \"gpt-4o\",       # $2.50/1M in\n",
    "    }\n",
    "    \n",
    "    COST_MAP = {\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        \"gpt-4o\": {\"input\": 2.50, \"output\": 10.00},\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def classify_task(cls, task: str) -> TaskComplexity:\n",
    "        \"\"\"Classify task complexity using heuristics.\"\"\"\n",
    "        \n",
    "        # Simple tasks: Classification, yes/no, extraction\n",
    "        simple_keywords = [\"classify\", \"extract\", \"is this\", \"yes or no\", \"true or false\"]\n",
    "        if any(kw in task.lower() for kw in simple_keywords):\n",
    "            return TaskComplexity.SIMPLE\n",
    "        \n",
    "        # Complex tasks: Analysis, reasoning, multi-step\n",
    "        complex_keywords = [\"analyze\", \"why\", \"explain\", \"compare\", \"reason\", \"strategy\"]\n",
    "        if any(kw in task.lower() for kw in complex_keywords):\n",
    "            return TaskComplexity.COMPLEX\n",
    "        \n",
    "        return TaskComplexity.MEDIUM\n",
    "    \n",
    "    @classmethod\n",
    "    def get_model(cls, task: str) -> str:\n",
    "        complexity = cls.classify_task(task)\n",
    "        return cls.MODEL_MAP[complexity]\n",
    "\n",
    "\n",
    "# Usage in Incident Postmortem\n",
    "class OptimizedPostmortemAgents:\n",
    "    \n",
    "    def log_analyzer(self, logs: str) -> dict:\n",
    "        # Simple extraction task â†’ cheap model\n",
    "        model = ModelRouter.get_model(\"extract patterns from logs\")\n",
    "        return ChatOpenAI(model=model).invoke(...)\n",
    "    \n",
    "    def root_cause_analyzer(self, patterns: list) -> str:\n",
    "        # Complex reasoning â†’ expensive model\n",
    "        model = ModelRouter.get_model(\"analyze why the system failed\")\n",
    "        return ChatOpenAI(model=model).invoke(...)\n",
    "    \n",
    "    def reviewer(self, report: str) -> dict:\n",
    "        # Medium task (scoring) â†’ cheap model\n",
    "        model = ModelRouter.get_model(\"score this report quality\")\n",
    "        return ChatOpenAI(model=model).invoke(...)\n",
    "```\n",
    "\n",
    "#### Cost Impact of Model Routing\n",
    "\n",
    "| Agent | Before (all GPT-4o) | After (routed) | Savings |\n",
    "|-------|---------------------|----------------|---------|\n",
    "| Log Analyzer | $0.015 | $0.001 (mini) | 93% |\n",
    "| Root Cause | $0.020 | $0.020 (4o) | 0% |\n",
    "| Writer | $0.025 | $0.025 (4o) | 0% |\n",
    "| Reviewer | $0.015 | $0.001 (mini) | 93% |\n",
    "| **Total** | **$0.075** | **$0.047** | **37%** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c44c1",
   "metadata": {},
   "source": [
    "### Strategy 5: Batch Processing & Async (OpenAI Batch API)\n",
    "\n",
    "OpenAI offers **50% discount** for batch requests processed within 24 hours.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    OPENAI BATCH API                                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  Standard API:                                                          â”‚\n",
    "â”‚  Request â†’ Process â†’ Response (immediate, full price)                   â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  Batch API:                                                             â”‚\n",
    "â”‚  Request 1 â”€â”                                                           â”‚\n",
    "â”‚  Request 2 â”€â”¼â”€â†’ Queue â†’ Process within 24h â†’ Results (50% off!)         â”‚\n",
    "â”‚  Request 3 â”€â”˜                                                           â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  Perfect for:                                                           â”‚\n",
    "â”‚  â€¢ Nightly report generation                                            â”‚\n",
    "â”‚  â€¢ Bulk data processing                                                 â”‚\n",
    "â”‚  â€¢ Non-urgent analysis                                                  â”‚\n",
    "â”‚  â€¢ Evaluation/testing pipelines                                         â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "#### Batch API Example\n",
    "\n",
    "```python\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Prepare batch requests\n",
    "requests = [\n",
    "    {\n",
    "        \"custom_id\": f\"incident-{i}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"Analyze this incident log...\"},\n",
    "                {\"role\": \"user\", \"content\": log_data}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    for i, log_data in enumerate(all_incidents)\n",
    "]\n",
    "\n",
    "# Write to JSONL file\n",
    "with open(\"batch_input.jsonl\", \"w\") as f:\n",
    "    for req in requests:\n",
    "        f.write(json.dumps(req) + \"\\n\")\n",
    "\n",
    "# Upload and create batch\n",
    "batch_file = openai.files.create(file=open(\"batch_input.jsonl\", \"rb\"), purpose=\"batch\")\n",
    "batch = openai.batches.create(input_file_id=batch_file.id, endpoint=\"/v1/chat/completions\")\n",
    "\n",
    "# Check status later\n",
    "status = openai.batches.retrieve(batch.id)\n",
    "# status.status: \"validating\" â†’ \"in_progress\" â†’ \"completed\"\n",
    "```\n",
    "\n",
    "#### When to Use Batch API\n",
    "\n",
    "| Scenario | Use Batch? | Reason |\n",
    "|----------|------------|--------|\n",
    "| Real-time chat | âŒ No | Users expect immediate response |\n",
    "| Incident postmortem | âŒ No | On-call needs results now |\n",
    "| Nightly report generation | âœ… Yes | Can wait until morning |\n",
    "| Bulk evaluation runs | âœ… Yes | Running 1000s of test cases |\n",
    "| Historical analysis | âœ… Yes | Processing old incidents |\n",
    "| Training data generation | âœ… Yes | Creating synthetic datasets |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a6a6f",
   "metadata": {},
   "source": [
    "### Cost Monitoring with Langfuse\n",
    "\n",
    "Langfuse automatically tracks costs when you use their callback handler. This is **essential** for:\n",
    "- Budget tracking & alerts\n",
    "- Cost attribution by feature/user\n",
    "- ROI analysis per use case\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     LANGFUSE COST DASHBOARD                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  ğŸ“Š Daily Cost Breakdown                     ğŸ“… Dec 9, 2024            â”‚\n",
    "â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  Today: $45.23 (â†‘12% vs yesterday)                                     â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  By Model:                                                              â”‚\n",
    "â”‚  â”œâ”€â”€ gpt-4o:        $32.15 (71%)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘       â”‚\n",
    "â”‚  â”œâ”€â”€ gpt-4o-mini:   $8.45 (19%)   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘       â”‚\n",
    "â”‚  â””â”€â”€ embeddings:    $4.63 (10%)   â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘       â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  By Feature (using tags):                                               â”‚\n",
    "â”‚  â”œâ”€â”€ incident-postmortem: $18.50  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘        â”‚\n",
    "â”‚  â”œâ”€â”€ chatbot:             $15.30  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘        â”‚\n",
    "â”‚  â””â”€â”€ code-review:         $11.43  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘        â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  ğŸ“ˆ Cost Trends (7 days):                                               â”‚\n",
    "â”‚       $50 â”¤        â•­â”€â•®                                                  â”‚\n",
    "â”‚           â”‚      â•­â”€â•¯ â•°â”€â”€â•®                                               â”‚\n",
    "â”‚       $40 â”¤   â•­â”€â”€â•¯      â•°â”€â•®                                             â”‚\n",
    "â”‚           â”‚ â•­â”€â•¯           â”‚                                             â”‚\n",
    "â”‚       $30 â”¼â”€â•¯             â•°â”€â”€                                           â”‚\n",
    "â”‚           Mon  Tue  Wed  Thu  Fri  Sat  Sun                             â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  âš ï¸  ALERT: Cost up 45% from yesterday                                 â”‚\n",
    "â”‚  â””â”€â”€ Root cause: 3 SEV1 incidents triggered extended analysis          â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "#### Setting Up Cost Alerts\n",
    "\n",
    "```python\n",
    "# In your observability.py or monitoring setup\n",
    "def check_daily_costs():\n",
    "    \"\"\"Alert if daily costs exceed threshold.\"\"\"\n",
    "    from langfuse import Langfuse\n",
    "    \n",
    "    langfuse = Langfuse()\n",
    "    \n",
    "    # Get today's traces\n",
    "    traces = langfuse.fetch_traces(\n",
    "        from_timestamp=datetime.now().replace(hour=0, minute=0),\n",
    "        to_timestamp=datetime.now()\n",
    "    )\n",
    "    \n",
    "    total_cost = sum(t.total_cost or 0 for t in traces.data)\n",
    "    \n",
    "    if total_cost > DAILY_BUDGET:\n",
    "        send_slack_alert(f\"ğŸš¨ Daily LLM cost ${total_cost:.2f} exceeds budget ${DAILY_BUDGET}\")\n",
    "        \n",
    "    return total_cost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6d77e",
   "metadata": {},
   "source": [
    "### Complete Cost Optimization Checklist\n",
    "\n",
    "Use this checklist when building or optimizing LLM applications:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  COST OPTIMIZATION CHECKLIST                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  ARCHITECTURE (40% potential savings)                                   â”‚\n",
    "â”‚  â–¡ Identify steps that don't need LLM (use regex, rules, templates)     â”‚\n",
    "â”‚  â–¡ Combine multiple LLM calls into one where possible                   â”‚\n",
    "â”‚  â–¡ Implement tiered processing (rules â†’ cheap LLM â†’ expensive LLM)      â”‚\n",
    "â”‚  â–¡ Limit self-reflection loops (max 2-3 iterations)                     â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  CACHING (30% potential savings)                                        â”‚\n",
    "â”‚  â–¡ Enable LangChain caching for development                             â”‚\n",
    "â”‚  â–¡ Use long system prompts to benefit from OpenAI prefix caching        â”‚\n",
    "â”‚  â–¡ Cache embeddings for RAG applications                                â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  PROMPTS (20% potential savings)                                        â”‚\n",
    "â”‚  â–¡ Remove unnecessary words and pleasantries                            â”‚\n",
    "â”‚  â–¡ Use JSON/structured output instead of prose                          â”‚\n",
    "â”‚  â–¡ Limit few-shot examples (1-2 instead of 5)                           â”‚\n",
    "â”‚  â–¡ Set max_tokens to prevent runaway outputs                            â”‚\n",
    "â”‚  â–¡ Use abbreviations for field names                                    â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  MODEL SELECTION (10-50% potential savings)                             â”‚\n",
    "â”‚  â–¡ Audit each LLM call - does it need GPT-4o?                           â”‚\n",
    "â”‚  â–¡ Implement model routing based on task complexity                     â”‚\n",
    "â”‚  â–¡ Use GPT-4o-mini for classification, extraction, scoring              â”‚\n",
    "â”‚  â–¡ Reserve GPT-4o for reasoning, analysis, creative tasks               â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  MONITORING (prevent cost surprises)                                    â”‚\n",
    "â”‚  â–¡ Set up Langfuse/observability for cost tracking                      â”‚\n",
    "â”‚  â–¡ Create daily/weekly cost alerts                                      â”‚\n",
    "â”‚  â–¡ Track cost per feature using tags                                    â”‚\n",
    "â”‚  â–¡ Review cost anomalies weekly                                         â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  ADVANCED                                                               â”‚\n",
    "â”‚  â–¡ Use Batch API for non-urgent bulk processing (50% off)               â”‚\n",
    "â”‚  â–¡ Consider fine-tuning to replace long prompts                         â”‚\n",
    "â”‚  â–¡ Evaluate open-source models for specific tasks                       â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314241b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### Observability Tools Comparison\n",
    "\n",
    "| Criteria | LangSmith | Langfuse |\n",
    "|----------|-----------|----------|\n",
    "| **Best for** | LangChain native | Framework-agnostic |\n",
    "| **Deployment** | Cloud only | Cloud + self-hosted |\n",
    "| **Cost tracking** | Limited | Built-in |\n",
    "| **Open source** | No | Yes |\n",
    "| **Integration** | Automatic | Callback handler |\n",
    "\n",
    "### Langfuse Quick Reference\n",
    "\n",
    "```python\n",
    "# 1. Install\n",
    "pip install langfuse\n",
    "\n",
    "# 2. Configure\n",
    "export LANGFUSE_SECRET_KEY=\"sk-lf-...\"\n",
    "export LANGFUSE_PUBLIC_KEY=\"pk-lf-...\"\n",
    "\n",
    "# 3. Use with LangGraph\n",
    "from langfuse.callback import CallbackHandler\n",
    "handler = CallbackHandler(user_id=\"...\", session_id=\"...\", tags=[...])\n",
    "graph.invoke(state, config={\"callbacks\": [handler]})\n",
    "```\n",
    "\n",
    "### Cost Optimization Summary\n",
    "\n",
    "| Strategy | Savings | Effort |\n",
    "|----------|---------|--------|\n",
    "| Architecture (fewer LLM calls) | 40% | High |\n",
    "| Caching (semantic + exact) | 30% | Medium |\n",
    "| Prompt optimization | 20% | Low |\n",
    "| Model selection | 10% | Low |\n",
    "\n",
    "---\n",
    "\n",
    "## Next: Module 3 - Security & Guardrails\n",
    "\n",
    "In the next module, we'll cover:\n",
    "- Security concepts for LLM applications\n",
    "- Prompt injection defenses\n",
    "- Output validation techniques\n",
    "- Guardrails frameworks overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c0bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Complete!\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  ğŸ‰ MODULE 2 COMPLETE: Observability & Cost Optimization             â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  âœ… What You Learned:                                                â•‘\n",
    "â•‘     â€¢ LangSmith vs Langfuse comparison                               â•‘\n",
    "â•‘     â€¢ Langfuse setup and integration with LangGraph                  â•‘\n",
    "â•‘     â€¢ Custom spans with @observe decorator                           â•‘\n",
    "â•‘     â€¢ Cost optimization strategies (architecture, caching, prompts)  â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  ğŸ“š Resources:                                                        â•‘\n",
    "â•‘     â€¢ Langfuse: https://langfuse.com/docs                            â•‘\n",
    "â•‘     â€¢ LangSmith: https://docs.smith.langchain.com                    â•‘\n",
    "â•‘     â€¢ OpenAI Pricing: https://openai.com/pricing                     â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  ğŸ› ï¸  Hands-on:                                                        â•‘\n",
    "â•‘     â€¢ Langfuse integration added to incident-postmortem project      â•‘\n",
    "â•‘     â€¢ See: session-2/projects/incident-postmortem/main.py            â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  â¡ï¸  Next: Module 3 - Security & Guardrails                          â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
