{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be3b5142",
   "metadata": {},
   "source": [
    "# Module 4: Model Context Protocol (MCP) Integration\n",
    "\n",
    "## Connecting LLM Agents to the World\n",
    "\n",
    "**Updated for LangChain v1 and LangGraph v1 (December 2024)**\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "1. Understand **what MCP is** and why it matters\n",
    "2. Know the **architecture** of MCP (servers, clients, transports)\n",
    "3. **Build an MCP server** from scratch using FastMCP\n",
    "4. **Consume MCP tools** from a LangChain v1 agent\n",
    "5. Understand **real-world use cases** for MCP\n",
    "\n",
    "---\n",
    "\n",
    "### The Problem MCP Solves\n",
    "\n",
    "> \"Every AI company is building tool integrations from scratch. It's like the web before HTTP.\"\n",
    "\n",
    "#### Before MCP: The N Ã— M Problem\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Claude    â”‚â”€â”€â”€â”€â–¶â”‚  Custom     â”‚â”€â”€â”€â”€â–¶â”‚   Slack     â”‚\n",
    "â”‚             â”‚     â”‚  Connector  â”‚     â”‚             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   GPT-4     â”‚â”€â”€â”€â”€â–¶â”‚  Different  â”‚â”€â”€â”€â”€â–¶â”‚   Slack     â”‚\n",
    "â”‚             â”‚     â”‚  Connector  â”‚     â”‚             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Every LLM Ã— Every Tool = Explosion of custom integrations\n",
    "\n",
    "#### After MCP: Standardized Protocol\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Claude    â”‚â—€â”€â”€â”€â”                â”Œâ”€â”€â–¶â”‚   Slack     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”Œâ”€â”€â”€â”€â”€â”    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚    â”‚     â”‚    â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”œâ”€â”€â”€â”€â”‚ MCP â”‚â”€â”€â”€â”€â”¤   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   GPT-4     â”‚â—€â”€â”€â”€â”¤    â”‚     â”‚    â”œâ”€â”€â–¶â”‚   GitHub    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â””â”€â”€â”€â”€â”€â”˜    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚               â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚               â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Llama     â”‚â—€â”€â”€â”€â”˜               â””â”€â”€â–¶â”‚   Database  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "One protocol to rule them all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1373a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking LLM Configuration...\n",
      "==================================================\n",
      "ğŸ“¡ Provider: DIAL (Azure OpenAI via EPAM AI Proxy)\n",
      "âœ… DIAL_API_KEY is set\n",
      "\n",
      "ğŸ“‹ Configuration:\n",
      "   AZURE_OPENAI_ENDPOINT: https://ai-proxy.lab.epam.com\n",
      "   AZURE_OPENAI_API_VERSION: 2024-08-01-preview\n",
      "   AZURE_OPENAI_DEPLOYMENT_NAME: gpt-4\n",
      "\n",
      "âœ… DIAL setup verified successfully!\n",
      "âœ… Environment ready for MCP exploration\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from setup_llm import verify_setup, get_chat_model\n",
    "\n",
    "verify_setup()\n",
    "print(\"âœ… Environment ready for MCP exploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39850c92",
   "metadata": {},
   "source": [
    "## Part 1: MCP Fundamentals\n",
    "\n",
    "### What is MCP?\n",
    "\n",
    "**Model Context Protocol (MCP)** is an open protocol developed by Anthropic that standardizes how AI applications connect to external data sources and tools.\n",
    "\n",
    "Think of it like:\n",
    "- **HTTP** for web communication\n",
    "- **SQL** for database queries  \n",
    "- **MCP** for AI â†” Tool communication\n",
    "\n",
    "### Core Components\n",
    "\n",
    "| Component | Role | Example |\n",
    "|-----------|------|---------|\n",
    "| **MCP Server** | Exposes tools/resources | A server that provides GitHub tools |\n",
    "| **MCP Client** | Connects to servers | Claude Desktop, your LangGraph agent |\n",
    "| **Transport** | How they communicate | HTTP (network), stdio (subprocess) |\n",
    "| **Tools** | Actions the LLM can take | `create_issue()`, `search_code()` |\n",
    "| **Resources** | Data the LLM can read | File contents, database records |\n",
    "| **Prompts** | Reusable prompt templates | Standard operating procedures |\n",
    "\n",
    "### Transport Types\n",
    "\n",
    "| Transport | How it Works | Best For |\n",
    "|-----------|--------------|----------|\n",
    "| **HTTP** (streamable-http) | Client connects to server via HTTP URL | **Production**, remote servers, multiple clients |\n",
    "| **stdio** | Client spawns server as subprocess | Local debugging, CLI tools |\n",
    "\n",
    "```\n",
    "HTTP Transport (Production)\n",
    "============================\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  MCP Client  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  MCP Server  â”‚\n",
    "â”‚  (Terminal 2)â”‚             â”‚  (Terminal 1)â”‚\n",
    "â”‚              â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚              â”‚\n",
    "â”‚              â”‚  JSON/SSE   â”‚ :8000/mcp    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â€¢ Server runs independently (python server.py)\n",
    "â€¢ Client connects via URL (http://localhost:8000/mcp)\n",
    "â€¢ Multiple clients can connect simultaneously\n",
    "â€¢ Easy to deploy and scale\n",
    "\n",
    "stdio Transport (Local)\n",
    "========================\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   spawn    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  MCP Client  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  MCP Server  â”‚\n",
    "â”‚              â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  (subprocess)â”‚\n",
    "â”‚              â”‚  stdin/out â”‚              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â€¢ Client spawns server as a child process\n",
    "â€¢ Communication via stdin/stdout pipes\n",
    "â€¢ Server lifecycle tied to client\n",
    "â€¢ Good for local CLI integrations\n",
    "```\n",
    "\n",
    "**Our project uses HTTP transport** because it better demonstrates real-world architecture where servers are deployed independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4b489",
   "metadata": {},
   "source": [
    "## Part 2: Building an MCP Server ğŸ”§\n",
    "\n",
    "Let's build a DevOps MCP server that provides tools for:\n",
    "- Checking service health\n",
    "- Fetching logs  \n",
    "- Running diagnostics\n",
    "- Executing safe commands\n",
    "\n",
    "### Step 1: Project Setup\n",
    "\n",
    "First, install the required packages:\n",
    "\n",
    "```bash\n",
    "pip install mcp langchain-mcp-adapters\n",
    "```\n",
    "\n",
    "### Step 2: Server Implementation\n",
    "\n",
    "Here's how to create an MCP server using FastMCP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ccf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ MCP Server Structure:\n",
      "==================================================\n",
      "\n",
      "\"\"\"DevOps MCP Server - Exposes tools for service monitoring.\"\"\"\n",
      "\n",
      "from mcp.server.fastmcp import FastMCP\n",
      "import json\n",
      "import random\n",
      "from datetime import datetime\n",
      "\n",
      "# Create the MCP server\n",
      "mcp = FastMCP(\"devops-tools\")\n",
      "\n",
      "# ============================================================\n",
      "# Tool 1: Get Service Health\n",
      "# ============================================================\n",
      "@mcp.tool()\n",
      "async def get_service_health(service_name: str) -> dict:\n",
      "    \"\"\"\n",
      "    Get the health status of a service.\n",
      "\n",
      "    Args:\n",
      "        service_name: Name of the service (e.g., 'api-gateway', 'auth-service')\n",
      "\n",
      "    Returns:\n",
      "        Health status including uptime, latency, and error rate\n",
      "    \"\"\"\n",
      "    # Simulated health data (in production, call real monitoring APIs)\n",
      "    statuses = [\"healthy\", \"healthy\", \"healthy\", \"degraded\", \"unhealthy\"]\n",
      "\n",
      "    return {\n",
      "        \"service\": service_name,\n",
      "        \"status\": random.choice(statuses),\n",
      "        \"uptime\": f\"{random.uniform(99.0, 99.99):.2f}%\",\n",
      "        \"latency_ms\": random.randint(10, 200),\n",
      "        \"error_rate\": f\"{random.uniform(0, 2):.2f}%\",\n",
      "        \"last_checked\": datetime.now().isoformat()\n",
      "    }\n",
      "\n",
      "\n",
      "# ============================================================\n",
      "# Tool 2: Get Logs\n",
      "# ============================================================\n",
      "@mcp.tool()\n",
      "async def get_logs(\n",
      "    service_name: str, \n",
      "    lines: int = 50, \n",
      "    level: str = \"all\"\n",
      ") -> dict:\n",
      "    \"\"\"\n",
      "    Fetch recent logs for a service.\n",
      "\n",
      "    Args:\n",
      "        service_name: Name of the service\n",
      "        lines: Number of log lines to fetch (default: 50)\n",
      "        level: Filter by log level: 'all', 'error', 'warn', 'info'\n",
      "    \"\"\"\n",
      "    levels = [\"INFO\", \"WARN\", \"ERROR\", \"DEBUG\"]\n",
      "    messages = [\n",
      "        \"Request processed successfully\",\n",
      "        \"Cache miss, fetching from database\",\n",
      "        \"Connection timeout, retrying...\",\n",
      "        \"Rate limit exceeded for client\",\n",
      "        \"Health check passed\",\n",
      "        \"Memory usage above threshold\",\n",
      "    ]\n",
      "\n",
      "    logs = []\n",
      "    for i in range(min(lines, 20)):\n",
      "        log_level = random.choice(levels)\n",
      "        if level != \"all\" and log_level.lower() != level.lower():\n",
      "            continue\n",
      "        logs.append({\n",
      "            \"timestamp\": datetime.now().isoformat(),\n",
      "            \"level\": log_level,\n",
      "            \"message\": random.choice(messages),\n",
      "            \"service\": service_name\n",
      "        })\n",
      "\n",
      "    return {\"service\": service_name, \"logs\": logs, \"total\": len(logs)}\n",
      "\n",
      "\n",
      "# ============================================================\n",
      "# Tool 3: Run Diagnostics\n",
      "# ============================================================\n",
      "@mcp.tool()\n",
      "async def run_diagnostics(service_name: str) -> dict:\n",
      "    \"\"\"\n",
      "    Run diagnostic checks on a service.\n",
      "\n",
      "    Args:\n",
      "        service_name: Name of the service to diagnose\n",
      "    \"\"\"\n",
      "    checks = {\n",
      "        \"database_connection\": random.choice([True, True, True, False]),\n",
      "        \"cache_available\": random.choice([True, True, False]),\n",
      "        \"dependencies_healthy\": random.choice([True, True, True, False]),\n",
      "        \"disk_space_ok\": True,\n",
      "        \"memory_ok\": random.choice([True, True, False]),\n",
      "        \"cpu_ok\": True,\n",
      "    }\n",
      "\n",
      "    failed = [k for k, v in checks.items() if not v]\n",
      "\n",
      "    return {\n",
      "        \"service\": service_name,\n",
      "        \"status\": \"pass\" if not failed else \"fail\",\n",
      "        \"checks\": checks,\n",
      "        \"failed_checks\": failed,\n",
      "        \"recommendations\": [\n",
      "            f\"Investigate {check}\" for check in failed\n",
      "        ] if failed else [\"All checks passed!\"]\n",
      "    }\n",
      "\n",
      "\n",
      "# ============================================================\n",
      "# Main Entry Point\n",
      "# ============================================================\n",
      "if __name__ == \"__main__\":\n",
      "    mcp.run()  # Starts the server (stdio transport by default)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the MCP server we've built\n",
    "# (This is server.py from projects/mcp-devops-tools/)\n",
    "\n",
    "server_code = '''\n",
    "\"\"\"DevOps MCP Server - Exposes tools for service monitoring.\n",
    "\n",
    "Run with:\n",
    "    python server.py          # HTTP mode (production-like) at :8000/mcp\n",
    "    python server.py --stdio  # stdio mode (local subprocess)\n",
    "\"\"\"\n",
    "\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Server configuration\n",
    "MCP_HOST = \"localhost\"\n",
    "MCP_PORT = 8000\n",
    "\n",
    "# Create the MCP server\n",
    "mcp = FastMCP(\"devops-tools\")\n",
    "\n",
    "# ============================================================\n",
    "# Tool 1: Get Service Health\n",
    "# ============================================================\n",
    "@mcp.tool()\n",
    "async def get_service_health(service_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get the health status of a service.\n",
    "    \n",
    "    Args:\n",
    "        service_name: Name of the service (e.g., 'api-gateway', 'auth-service')\n",
    "    \n",
    "    Returns:\n",
    "        Health status including uptime, latency, and error rate\n",
    "    \"\"\"\n",
    "    # Simulated health data (in production, call real monitoring APIs)\n",
    "    statuses = [\"healthy\", \"healthy\", \"healthy\", \"degraded\", \"unhealthy\"]\n",
    "    \n",
    "    return {\n",
    "        \"service\": service_name,\n",
    "        \"status\": random.choice(statuses),\n",
    "        \"uptime\": f\"{random.uniform(99.0, 99.99):.2f}%\",\n",
    "        \"latency_ms\": random.randint(10, 200),\n",
    "        \"error_rate\": f\"{random.uniform(0, 2):.2f}%\",\n",
    "        \"last_checked\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Tool 2: Get Logs\n",
    "# ============================================================\n",
    "@mcp.tool()\n",
    "async def get_logs(\n",
    "    service_name: str, \n",
    "    lines: int = 50, \n",
    "    level: str = \"all\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Fetch recent logs for a service.\n",
    "    \n",
    "    Args:\n",
    "        service_name: Name of the service\n",
    "        lines: Number of log lines to fetch (default: 50)\n",
    "        level: Filter by log level: 'all', 'error', 'warn', 'info'\n",
    "    \"\"\"\n",
    "    levels = [\"INFO\", \"WARN\", \"ERROR\", \"DEBUG\"]\n",
    "    messages = [\n",
    "        \"Request processed successfully\",\n",
    "        \"Cache miss, fetching from database\",\n",
    "        \"Connection timeout, retrying...\",\n",
    "        \"Rate limit exceeded for client\",\n",
    "        \"Health check passed\",\n",
    "        \"Memory usage above threshold\",\n",
    "    ]\n",
    "    \n",
    "    logs = []\n",
    "    for i in range(min(lines, 20)):\n",
    "        log_level = random.choice(levels)\n",
    "        if level != \"all\" and log_level.lower() != level.lower():\n",
    "            continue\n",
    "        logs.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"level\": log_level,\n",
    "            \"message\": random.choice(messages),\n",
    "            \"service\": service_name\n",
    "        })\n",
    "    \n",
    "    return {\"service\": service_name, \"logs\": logs, \"total\": len(logs)}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Tool 3: Run Diagnostics\n",
    "# ============================================================\n",
    "@mcp.tool()\n",
    "async def run_diagnostics(service_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Run diagnostic checks on a service.\n",
    "    \n",
    "    Args:\n",
    "        service_name: Name of the service to diagnose\n",
    "    \"\"\"\n",
    "    checks = {\n",
    "        \"database_connection\": random.choice([True, True, True, False]),\n",
    "        \"cache_available\": random.choice([True, True, False]),\n",
    "        \"dependencies_healthy\": random.choice([True, True, True, False]),\n",
    "        \"disk_space_ok\": True,\n",
    "        \"memory_ok\": random.choice([True, True, False]),\n",
    "        \"cpu_ok\": True,\n",
    "    }\n",
    "    \n",
    "    failed = [k for k, v in checks.items() if not v]\n",
    "    \n",
    "    return {\n",
    "        \"service\": service_name,\n",
    "        \"status\": \"pass\" if not failed else \"fail\",\n",
    "        \"checks\": checks,\n",
    "        \"failed_checks\": failed,\n",
    "        \"recommendations\": [\n",
    "            f\"Investigate {check}\" for check in failed\n",
    "        ] if failed else [\"All checks passed!\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main Entry Point - HTTP Transport (default)\n",
    "# ============================================================\n",
    "\n",
    "# Create Starlette app for HTTP transport\n",
    "app = mcp.streamable_http_app()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    import uvicorn\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--stdio\", action=\"store_true\")\n",
    "    parser.add_argument(\"--host\", default=MCP_HOST)\n",
    "    parser.add_argument(\"--port\", type=int, default=MCP_PORT)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.stdio:\n",
    "        mcp.run(transport=\"stdio\")  # For subprocess spawning\n",
    "    else:\n",
    "        # HTTP transport - production-like setup using uvicorn\n",
    "        print(f\"ğŸš€ Starting server at http://{args.host}:{args.port}/mcp\")\n",
    "        uvicorn.run(app, host=args.host, port=args.port)\n",
    "'''\n",
    "\n",
    "print(\"ğŸ“„ MCP Server Structure:\")\n",
    "print(\"=\" * 50)\n",
    "print(server_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa8080d",
   "metadata": {},
   "source": [
    "### Key Concepts in the Server Code\n",
    "\n",
    "1. **`FastMCP(\"devops-tools\")`** - Creates a named MCP server\n",
    "2. **`@mcp.tool()`** - Decorator that exposes a function as an MCP tool\n",
    "3. **Type hints** - MCP uses these for parameter/return schemas\n",
    "4. **Docstrings** - Become the tool description shown to the LLM\n",
    "5. **`mcp.streamable_http_app()`** - Returns a Starlette app for HTTP transport (run with uvicorn)\n",
    "\n",
    "### Transport Options\n",
    "\n",
    "| Transport | Command | URL |\n",
    "|-----------|---------|-----|\n",
    "| **HTTP** (default) | `python server.py` | `http://localhost:8000/mcp` |\n",
    "| **stdio** | `python server.py --stdio` | N/A (subprocess) |\n",
    "\n",
    "### Tool Design Best Practices\n",
    "\n",
    "| Practice | Why It Matters |\n",
    "|----------|---------------|\n",
    "| **Clear names** | LLM selects tools by name |\n",
    "| **Detailed docstrings** | LLM reads this to understand when to use |\n",
    "| **Type annotations** | Enables parameter validation |\n",
    "| **Structured returns** | JSON makes parsing reliable |\n",
    "| **Error handling** | Graceful failures help debugging |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e1547a",
   "metadata": {},
   "source": [
    "## Part 3: Connecting to MCP from Python ğŸ”Œ\n",
    "\n",
    "### Direct MCP Client Usage (HTTP Transport)\n",
    "\n",
    "Before integrating with LangGraph, let's understand how to connect to an MCP server directly.\n",
    "\n",
    "**Two-Terminal Workflow:**\n",
    "1. **Terminal 1**: `python server.py` (starts HTTP server at :8000)\n",
    "2. **Terminal 2**: `python simple_client.py` (connects via HTTP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041b1ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Simple MCP Client:\n",
      "==================================================\n",
      "\n",
      "\"\"\"Basic MCP Client - Connect and call tools directly.\"\"\"\n",
      "\n",
      "import asyncio\n",
      "import json\n",
      "from mcp import ClientSession, StdioServerParameters\n",
      "from mcp.client.stdio import stdio_client\n",
      "\n",
      "async def main():\n",
      "    # Configure connection to server\n",
      "    server_params = StdioServerParameters(\n",
      "        command=\"python\",\n",
      "        args=[\"server.py\"],\n",
      "    )\n",
      "\n",
      "    # Connect using stdio transport\n",
      "    async with stdio_client(server_params) as (read, write):\n",
      "        async with ClientSession(read, write) as session:\n",
      "            # Initialize the MCP connection\n",
      "            await session.initialize()\n",
      "\n",
      "            # List available tools\n",
      "            tools_result = await session.list_tools()\n",
      "            print(f\"Available tools: {len(tools_result.tools)}\")\n",
      "            for tool in tools_result.tools:\n",
      "                print(f\"  - {tool.name}: {tool.description[:50]}...\")\n",
      "\n",
      "            # Call a tool\n",
      "            result = await session.call_tool(\n",
      "                \"get_service_health\",\n",
      "                arguments={\"service_name\": \"api-gateway\"}\n",
      "            )\n",
      "\n",
      "            # Parse the result\n",
      "            content = result.content[0].text\n",
      "            data = json.loads(content)\n",
      "            print(f\"\\nHealth check result:\")\n",
      "            print(json.dumps(data, indent=2))\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    asyncio.run(main())\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Direct MCP Client Usage over HTTP (from simple_client.py)\n",
    "\n",
    "client_code = '''\n",
    "\"\"\"Basic MCP Client - Connect via HTTP and call tools directly.\n",
    "\n",
    "Prerequisites:\n",
    "    1. Start the server: python server.py\n",
    "    2. Then run this:   python simple_client.py\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "# Server URL - must match server.py\n",
    "MCP_SERVER_URL = \"http://localhost:8000/mcp\"\n",
    "\n",
    "async def main():\n",
    "    print(f\"Connecting to {MCP_SERVER_URL}...\")\n",
    "    \n",
    "    # Connect using HTTP transport\n",
    "    async with streamablehttp_client(MCP_SERVER_URL) as (read, write, _):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize the MCP connection\n",
    "            await session.initialize()\n",
    "            \n",
    "            # List available tools\n",
    "            tools_result = await session.list_tools()\n",
    "            print(f\"Available tools: {len(tools_result.tools)}\")\n",
    "            for tool in tools_result.tools:\n",
    "                print(f\"  - {tool.name}: {tool.description[:50]}...\")\n",
    "            \n",
    "            # Call a tool\n",
    "            result = await session.call_tool(\n",
    "                \"get_service_health\",\n",
    "                arguments={\"service_name\": \"api-gateway\"}\n",
    "            )\n",
    "            \n",
    "            # Parse the result\n",
    "            content = result.content[0].text\n",
    "            data = json.loads(content)\n",
    "            print(f\"\\\\nHealth check result:\")\n",
    "            print(json.dumps(data, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "'''\n",
    "\n",
    "print(\"ğŸ“„ Simple MCP Client (HTTP Transport):\")\n",
    "print(\"=\" * 50)\n",
    "print(client_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab178d39",
   "metadata": {},
   "source": [
    "### Connection Flow (HTTP Transport)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  MCP HTTP Client Flow                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Terminal 1: python server.py                               â”‚\n",
    "â”‚              ğŸš€ Server running at http://localhost:8000/mcp â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Terminal 2: python simple_client.py                        â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  1. streamablehttp_client(url)  â”€â”€â–¶  Connect via HTTP       â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  2. ClientSession(read, write)  â”€â”€â–¶  Create session         â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  3. session.initialize()  â”€â”€â–¶  Handshake with server        â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  4. session.list_tools()  â”€â”€â–¶  Discover available tools     â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  5. session.call_tool(name, args)  â”€â”€â–¶  Execute tool        â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key difference from stdio**: Server runs independently - you start it once, and multiple clients can connect to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba8278c",
   "metadata": {},
   "source": [
    "## Part 4: LangChain v1 + MCP Integration ğŸš€\n",
    "\n",
    "This is where the magic happens! We'll use `langchain-mcp-adapters` to:\n",
    "1. Connect to our MCP server over HTTP\n",
    "2. Convert MCP tools to LangChain tools\n",
    "3. Use them with LangChain v1's `create_agent`\n",
    "\n",
    "### The Bridge: langchain-mcp-adapters\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   LangChain v1  â”‚â—€â”€â”€â–¶â”‚ langchain-mcp-adapters  â”‚â—€â”€â”€â–¶â”‚ MCP Server  â”‚\n",
    "â”‚   create_agent  â”‚    â”‚                         â”‚    â”‚ (HTTP)      â”‚\n",
    "â”‚                 â”‚    â”‚ â€¢ Converts MCPâ†’LangChainâ”‚    â”‚             â”‚\n",
    "â”‚                 â”‚    â”‚ â€¢ Manages connections   â”‚    â”‚ :8000/mcp   â”‚\n",
    "â”‚                 â”‚    â”‚ â€¢ HTTP/stdio transport  â”‚    â”‚             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### What's New in LangChain v1\n",
    "\n",
    "| Old API (Deprecated) | New API (LangChain v1) |\n",
    "|---------------------|------------------------|\n",
    "| `from langgraph.prebuilt import create_react_agent` | `from langchain.agents import create_agent` |\n",
    "| `create_react_agent(llm, tools, state_modifier=...)` | `create_agent(model=llm, tools=tools, system_prompt=...)` |\n",
    "| stdio spawn subprocess | HTTP connect to running server |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754dfb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ LangGraph + MCP Integration:\n",
      "==================================================\n",
      "\n",
      "\"\"\"LangGraph Consumer for MCP DevOps Tools.\"\"\"\n",
      "\n",
      "import asyncio\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langgraph.prebuilt import create_react_agent\n",
      "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
      "\n",
      "async def main():\n",
      "    # Configure MCP server connection\n",
      "    # The client will spawn the server as a subprocess\n",
      "    mcp_config = {\n",
      "        \"devops-tools\": {\n",
      "            \"command\": \"python\",\n",
      "            \"args\": [\"server.py\"],\n",
      "            \"transport\": \"stdio\",\n",
      "        }\n",
      "    }\n",
      "\n",
      "    # Connect to MCP server\n",
      "    async with MultiServerMCPClient(mcp_config) as mcp_client:\n",
      "        # Get tools from MCP server (converted to LangChain format)\n",
      "        tools = mcp_client.get_tools()\n",
      "\n",
      "        print(f\"Loaded {len(tools)} tools from MCP server:\")\n",
      "        for tool in tools:\n",
      "            print(f\"  â€¢ {tool.name}\")\n",
      "\n",
      "        # Create LLM\n",
      "        llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
      "\n",
      "        # Create ReAct agent with MCP tools\n",
      "        agent = create_react_agent(\n",
      "            llm,\n",
      "            tools,\n",
      "            state_modifier=\"\"\"You are a DevOps assistant. \n",
      "            Use the available tools to investigate issues and \n",
      "            provide actionable recommendations.\"\"\"\n",
      "        )\n",
      "\n",
      "        # Run an investigation\n",
      "        result = await agent.ainvoke({\n",
      "            \"messages\": [(\"user\", \n",
      "                \"Check the health of the api-gateway service. \"\n",
      "                \"If there are issues, run diagnostics and get logs.\"\n",
      "            )]\n",
      "        })\n",
      "\n",
      "        # Print the agent's response\n",
      "        for msg in result[\"messages\"]:\n",
      "            if hasattr(msg, \"content\") and msg.content:\n",
      "                print(f\"\\n{msg.type}: {msg.content[:500]}\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    asyncio.run(main())\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LangChain v1 + MCP Integration Code (HTTP Transport)\n",
    "# Updated for LangChain v1 and LangGraph v1 (December 2024)\n",
    "\n",
    "langgraph_code = '''\n",
    "\"\"\"LangChain v1 Consumer for MCP DevOps Tools (HTTP Transport).\n",
    "\n",
    "Prerequisites:\n",
    "    1. Start the server: python server.py\n",
    "    2. Then run this:   python langgraph_consumer.py\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from langchain.agents import create_agent  # LangChain v1 API\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Server URL - must match server.py\n",
    "MCP_SERVER_URL = \"http://localhost:8000/mcp\"\n",
    "\n",
    "async def main():\n",
    "    # Configure MCP server connection using HTTP transport\n",
    "    # Server must be running: python server.py\n",
    "    mcp_config = {\n",
    "        \"devops-tools\": {\n",
    "            \"transport\": \"streamable_http\",\n",
    "            \"url\": MCP_SERVER_URL,\n",
    "            # Optional: \"headers\": {\"Authorization\": \"Bearer <token>\"}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Connecting to MCP server at {MCP_SERVER_URL}...\")\n",
    "    \n",
    "    # Connect to MCP server\n",
    "    mcp_client = MultiServerMCPClient(mcp_config)\n",
    "    \n",
    "    # Get tools from MCP server (converted to LangChain format)\n",
    "    tools = await mcp_client.get_tools()\n",
    "    \n",
    "    print(f\"Loaded {len(tools)} tools from MCP server:\")\n",
    "    for tool in tools:\n",
    "        print(f\"  â€¢ {tool.name}\")\n",
    "    \n",
    "    # Create LLM\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    \n",
    "    # LangChain v1: create_agent replaces langgraph.prebuilt.create_react_agent\n",
    "    agent = create_agent(\n",
    "        model=llm,\n",
    "        tools=tools,\n",
    "        system_prompt=\"\"\"You are a DevOps assistant. \n",
    "        Use the available tools to investigate issues and \n",
    "        provide actionable recommendations.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Run an investigation\n",
    "    result = await agent.ainvoke({\n",
    "        \"messages\": [(\"user\", \n",
    "            \"Check the health of the api-gateway service. \"\n",
    "            \"If there are issues, run diagnostics and get logs.\"\n",
    "        )]\n",
    "    })\n",
    "    \n",
    "    # Print the agent's response\n",
    "    for msg in result[\"messages\"]:\n",
    "        if hasattr(msg, \"content\") and msg.content:\n",
    "            print(f\"\\\\n{msg.type}: {msg.content[:500]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "'''\n",
    "\n",
    "print(\"ğŸ“„ LangChain v1 + MCP Integration (HTTP Transport):\")\n",
    "print(\"=\" * 50)\n",
    "print(langgraph_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d9a2d",
   "metadata": {},
   "source": [
    "### Key Points in the Integration\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| `MultiServerMCPClient` | Connects to one or more MCP servers |\n",
    "| `mcp_config` | Defines transport type, URL, and optional headers |\n",
    "| `client.get_tools()` | Returns LangChain-compatible tools |\n",
    "| `create_agent` | LangChain v1 API to create agent with tools |\n",
    "\n",
    "### Running the Demo\n",
    "\n",
    "**Two-Terminal Workflow:**\n",
    "\n",
    "```bash\n",
    "cd projects/mcp-devops-tools\n",
    "\n",
    "# Terminal 1: Start the MCP server\n",
    "python server.py\n",
    "# ğŸš€ Server running at http://localhost:8000/mcp\n",
    "\n",
    "# Terminal 2: Run the client or agent\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Simple client (no LLM needed)\n",
    "python simple_client.py\n",
    "\n",
    "# LangChain agent (needs API key)\n",
    "export OPENAI_API_KEY='your-key'\n",
    "python langgraph_consumer.py\n",
    "```\n",
    "\n",
    "### Or use Make commands:\n",
    "\n",
    "```bash\n",
    "make server   # Terminal 1\n",
    "make client   # Terminal 2 (or make agent)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9769436c",
   "metadata": {},
   "source": [
    "## Part 5: MCP Ecosystem & Real-World Servers ğŸŒ\n",
    "\n",
    "### Popular MCP Servers\n",
    "\n",
    "The MCP ecosystem is growing rapidly. Here are some useful servers:\n",
    "\n",
    "| Server | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| **@modelcontextprotocol/server-filesystem** | File system access | Read/write files |\n",
    "| **@modelcontextprotocol/server-github** | GitHub API | Issues, PRs, code search |\n",
    "| **@modelcontextprotocol/server-postgres** | PostgreSQL queries | Database operations |\n",
    "| **@modelcontextprotocol/server-slack** | Slack integration | Messages, channels |\n",
    "| **@modelcontextprotocol/server-puppeteer** | Browser automation | Web scraping, testing |\n",
    "\n",
    "### Using Pre-built Servers\n",
    "\n",
    "You can connect to multiple MCP servers simultaneously:\n",
    "\n",
    "```python\n",
    "mcp_config = {\n",
    "    # Your custom server\n",
    "    \"devops-tools\": {\n",
    "        \"command\": \"python\",\n",
    "        \"args\": [\"server.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "    # Official GitHub server\n",
    "    \"github\": {\n",
    "        \"command\": \"npx\",\n",
    "        \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n",
    "        \"transport\": \"stdio\",\n",
    "        \"env\": {\n",
    "            \"GITHUB_TOKEN\": os.getenv(\"GITHUB_TOKEN\")\n",
    "        }\n",
    "    },\n",
    "    # File system server\n",
    "    \"filesystem\": {\n",
    "        \"command\": \"npx\",\n",
    "        \"args\": [\n",
    "            \"-y\", \n",
    "            \"@modelcontextprotocol/server-filesystem\",\n",
    "            \"/path/to/allowed/directory\"\n",
    "        ],\n",
    "        \"transport\": \"stdio\",\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Claude Desktop Integration\n",
    "\n",
    "MCP servers can also be used with Claude Desktop. Add to `claude_desktop_config.json`:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"devops-tools\": {\n",
    "      \"command\": \"python\",\n",
    "      \"args\": [\"/full/path/to/server.py\"]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c880c",
   "metadata": {},
   "source": [
    "## Part 6: Beyond Tools - Resources and Prompts ğŸ“š\n",
    "\n",
    "MCP isn't just about tools. It also supports:\n",
    "\n",
    "### Resources\n",
    "\n",
    "Resources let the LLM read data (like files, database records):\n",
    "\n",
    "```python\n",
    "@mcp.resource(\"config://services\")\n",
    "async def get_services_config() -> str:\n",
    "    \"\"\"Returns the list of monitored services.\"\"\"\n",
    "    return json.dumps({\n",
    "        \"services\": [\"api-gateway\", \"auth-service\", \"database\"],\n",
    "        \"environment\": \"production\"\n",
    "    })\n",
    "```\n",
    "\n",
    "### Prompts\n",
    "\n",
    "Prompts are reusable templates the LLM can use:\n",
    "\n",
    "```python\n",
    "@mcp.prompt(\"incident_template\")\n",
    "async def incident_template(severity: str, service: str) -> str:\n",
    "    \"\"\"Standard incident report template.\"\"\"\n",
    "    return f\"\"\"\n",
    "    # Incident Report\n",
    "    \n",
    "    **Severity**: {severity}\n",
    "    **Service**: {service}\n",
    "    **Time**: {{current_time}}\n",
    "    \n",
    "    ## Summary\n",
    "    {{summary}}\n",
    "    \n",
    "    ## Root Cause\n",
    "    {{root_cause}}\n",
    "    \n",
    "    ## Resolution\n",
    "    {{resolution}}\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "### Complete MCP Server Structure\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           MCP Server                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                        â”‚\n",
    "â”‚  Tools (@mcp.tool)                     â”‚\n",
    "â”‚  â”œâ”€â”€ get_service_health()              â”‚\n",
    "â”‚  â”œâ”€â”€ get_logs()                        â”‚\n",
    "â”‚  â””â”€â”€ run_diagnostics()                 â”‚\n",
    "â”‚                                        â”‚\n",
    "â”‚  Resources (@mcp.resource)             â”‚\n",
    "â”‚  â”œâ”€â”€ config://services                 â”‚\n",
    "â”‚  â””â”€â”€ docs://runbooks                   â”‚\n",
    "â”‚                                        â”‚\n",
    "â”‚  Prompts (@mcp.prompt)                 â”‚\n",
    "â”‚  â”œâ”€â”€ incident_template                 â”‚\n",
    "â”‚  â””â”€â”€ escalation_guide                  â”‚\n",
    "â”‚                                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124d3a40",
   "metadata": {},
   "source": [
    "## Part 7: MCP Best Practices âœ…\n",
    "\n",
    "### Security Considerations\n",
    "\n",
    "| Risk | Mitigation |\n",
    "|------|------------|\n",
    "| **Arbitrary code execution** | Whitelist allowed commands |\n",
    "| **Data exposure** | Implement authentication |\n",
    "| **Resource exhaustion** | Add rate limiting |\n",
    "| **Prompt injection** | Validate all inputs |\n",
    "\n",
    "### Design Principles\n",
    "\n",
    "1. **Single Responsibility**: Each tool does one thing well\n",
    "2. **Clear Naming**: `get_service_logs` > `fetch_data`\n",
    "3. **Detailed Descriptions**: Help the LLM choose correctly\n",
    "4. **Graceful Errors**: Return error info, don't crash\n",
    "5. **Idempotency**: Safe to call multiple times\n",
    "\n",
    "### When to Use MCP vs Direct Integration\n",
    "\n",
    "| Use MCP When | Use Direct Integration When |\n",
    "|--------------|----------------------------|\n",
    "| Building reusable tools | One-off integration |\n",
    "| Multi-LLM support needed | Single LLM application |\n",
    "| Team sharing tools | Personal project |\n",
    "| Complex tool ecosystem | Simple tool needs |\n",
    "| Production deployment | Quick prototype |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b861c8",
   "metadata": {},
   "source": [
    "## ğŸ¯ Hands-On Exercise\n",
    "\n",
    "### Task: Extend the MCP DevOps Server\n",
    "\n",
    "Add a new tool to the server that creates incident tickets:\n",
    "\n",
    "```python\n",
    "@mcp.tool()\n",
    "async def create_incident(\n",
    "    title: str,\n",
    "    severity: str,\n",
    "    service: str,\n",
    "    description: str\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Create a new incident ticket.\n",
    "    \n",
    "    Args:\n",
    "        title: Brief title of the incident\n",
    "        severity: 'critical', 'high', 'medium', or 'low'\n",
    "        service: Affected service name\n",
    "        description: Detailed description\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Requirements**:\n",
    "1. Validate severity is one of the allowed values\n",
    "2. Return a ticket ID and creation timestamp\n",
    "3. Include a link to the (mock) ticket\n",
    "\n",
    "**Bonus**:\n",
    "- Add a `@mcp.resource()` for listing open incidents\n",
    "- Add a `@mcp.prompt()` for incident templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2426340",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **MCP solves the NÃ—M problem** - One protocol for all LLM-tool connections\n",
    "2. **FastMCP makes servers easy** - Just decorate your functions\n",
    "3. **Two transports**: HTTP (production, remote) and stdio (local, subprocess)\n",
    "4. **langchain-mcp-adapters** bridges MCP and LangChain v1\n",
    "5. **MCP has 3 primitives**: Tools, Resources, and Prompts\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     MCP Mental Model                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  MCP Server = Tool provider (like an API server)            â”‚\n",
    "â”‚  MCP Client = Tool consumer (your agent)                    â”‚\n",
    "â”‚  Transport  = How they talk (HTTP, stdio)                   â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Tools     = Actions (functions the LLM can call)           â”‚\n",
    "â”‚  Resources = Data (things the LLM can read)                 â”‚\n",
    "â”‚  Prompts   = Templates (reusable prompt patterns)           â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **MCP Specification**: https://spec.modelcontextprotocol.io\n",
    "- **MCP GitHub**: https://github.com/modelcontextprotocol\n",
    "- **Server Examples**: https://github.com/modelcontextprotocol/servers\n",
    "- **langchain-mcp-adapters**: https://github.com/langchain-ai/langchain-mcp-adapters\n",
    "- **Workshop Project**: `projects/mcp-devops-tools/`\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ‰ Congratulations!\n",
    "\n",
    "You've completed Session 3 of the Agentic AI Workshop!\n",
    "\n",
    "**What you've mastered:**\n",
    "- âœ… Evaluation frameworks for LLM agents (DeepEval)\n",
    "- âœ… Observability and cost optimization (Langfuse)\n",
    "- âœ… Security guardrails and best practices\n",
    "- âœ… Common anti-patterns and how to avoid them\n",
    "- âœ… MCP for tool interoperability\n",
    "\n",
    "**Next Steps:**\n",
    "- Integrate Langfuse into your incident-postmortem project\n",
    "- Add DeepEval tests for quality assurance\n",
    "- Build your own MCP servers for your tools\n",
    "- Apply these patterns to production systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
